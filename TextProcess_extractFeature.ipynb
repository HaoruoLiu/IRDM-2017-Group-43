{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import copy\n",
    "import xgboost\n",
    "from google_dict import *\n",
    "from nltk.corpus import wordnet as wn\n",
    "import difflib\n",
    "import time\n",
    "import os\n",
    "from IRDM_Functions import *\n",
    "from replace_brand_dict import *\n",
    "stoplist_wo_can=stoplist[:]\n",
    "stoplist_wo_can.remove('can')\n",
    "from config import *\n",
    "from time import time\n",
    "import csv\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "stoplist.append('till')  # add 'till' to stoplist\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/home/ubuntu/web-datamining/IRDM/Data/train.csv',encoding = \"ISO-8859-1\")\n",
    "test_df = pd.read_csv('/home/ubuntu/web-datamining/IRDM/Data/test.csv',encoding = \"ISO-8859-1\")\n",
    "attributes = pd.read_csv('/home/ubuntu/web-datamining/IRDM/Data/attributes.csv', encoding=\"ISO-8859-1\")\n",
    "pro_descr = pd.read_csv('/home/ubuntu/web-datamining/IRDM/Data/product_descriptions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attributes = attributes.dropna(axis=0,how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data from Description and Attribute dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df = pd.concat((train_df,test_df),axis=0,ignore_index=True)\n",
    "All_df = pd.merge(All_df,pro_descr,how='left',on='product_uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brand_df = attributes[attributes['name']=='MFG Brand Name']\n",
    "brand_df = brand_df.rename(columns={'value':'Brand'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df = pd.merge(All_df, brand_df, how='left', on='product_uid')\n",
    "All_df = All_df.drop('name',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df.Brand.fillna('none',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### need to exploration \n",
    "np.any(All_df['relevance'].isnull()==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### extract Materials from attributes\n",
    "material_df = attributes[attributes['name'].str.contains('Material')==True]\n",
    "\n",
    "dup_material = material_df[material_df.duplicated('product_uid')==True]\n",
    "nodup_material = material_df[material_df.duplicated('product_uid')==False]\n",
    "dup_material.fillna('none',inplace=True)\n",
    "nodup_material.fillna('none',inplace=True)\n",
    "\n",
    "dm = dup_material.index\n",
    "material_df = material_df.drop(dm)\n",
    "for i in dm:\n",
    "    material_df['value'][material_df['product_uid']==dup_material.ix[i,'product_uid']]=\\\n",
    "    material_df['value'][material_df['product_uid']==dup_material.ix[i,'product_uid']]+','+\\\n",
    "    dup_material.ix[i,'value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# material_df.to_csv('Material_cloud_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### extract Color from attribute dataset\n",
    "color_df = attributes[attributes['name'].str.contains('Color')==True]\n",
    "color_df.fillna('none',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dup_color = color_df[color_df.duplicated('product_uid')==True]\n",
    "nodup_color = color_df[color_df.duplicated('product_uid')==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dc = dup_color.index\n",
    "color_df = color_df.drop(dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in dc:\n",
    "    color_df['value'][color_df['product_uid']==dup_color.ix[j,'product_uid']]=\\\n",
    "    color_df['value'][color_df['product_uid']==dup_color.ix[j,'product_uid']]+','+\\\n",
    "    dup_color.ix[j,'value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# color_df.to_csv('Color_cloud.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### extract bullet from attributes\n",
    "bullet_df = attributes[attributes['name'].str.contains('Bullet')==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dup_bullet = bullet_df[bullet_df.duplicated('product_uid')==True]\n",
    "db = dup_bullet.index\n",
    "bullet_df_copy = copy.deepcopy(bullet_df)\n",
    "bullet_df_copy = bullet_df_copy.drop(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bullet_df = bullet_df.dropna()\n",
    "for i in list(set(list(attributes['product_uid']))):\n",
    "    bullet_df_copy['value'][bullet_df_copy['product_uid']==i] ='.'.join(list(bullet_df['value'][bullet_df['product_uid']==i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bullet_df_copy.to_csv('Bullet_cloud.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bdi = pd.Series(list(brand_df.index))\n",
    "mdd = pd.Series(list(attributes[attributes['name'].str.contains('Material')==True].index))\n",
    "cdd = pd.Series(list(attributes[attributes['name'].str.contains('Color')==True].index))\n",
    "bdd = pd.Series(list(attributes[attributes['name'].str.contains('Bullet')==True].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_to_drop = pd.concat((bdi,mdd,cdd,bdd),axis=0)\n",
    "attributes = attributes.drop(set(list(index_to_drop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### extract product height\n",
    "Height_family = attributes[attributes['name'].str.contains('Product Height')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product Height (in.)            54698\n",
       "Actual Product Height (in.)       265\n",
       "Nominal Product Height (In.)      179\n",
       "Product Height (ft.)               83\n",
       "Maximum Product Height (in.)       66\n",
       "Minimum Product Height (In.)       48\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Height_family['name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i in range(len(list(Height_family['name'].groupby(Height_family['product_uid'])))):\n",
    "#     if len(list(Height_family['name'].groupby(Height_family['product_uid']))[i])>2:\n",
    "#         print(list(Height_family['name'].groupby(Height_family['product_uid']))[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dup_Height_family = Height_family[Height_family.duplicated('product_uid')==True]\n",
    "Height_family = Height_family.drop(dup_Height_family.index)\n",
    "collect_id = []\n",
    "for i in dup_Height_family['product_uid']:\n",
    "    j = dup_Height_family[dup_Height_family['product_uid']==i].index\n",
    "    for n in np.arange(0,len(j)):\n",
    "        m = j.tolist()[n]\n",
    "        if dup_Height_family.ix[m,'name']=='Product Height (in.)':\n",
    "            Height_family['value'][Height_family['product_uid']==i] = dup_Height_family.ix[m,'value']\n",
    "            if n==len(j)-1:\n",
    "                dup_Height_family = dup_Height_family.drop(j)\n",
    "            else:\n",
    "                collect_id.append(j)\n",
    "            \n",
    "\n",
    "Height_family['value'][Height_family['name']=='Product Height (ft.)']=\\\n",
    "Height_family['value'][Height_family['name']=='Product Height (ft.)']*12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product Height (in.)            54380\n",
       "Actual Product Height (in.)       265\n",
       "Nominal Product Height (In.)      179\n",
       "Product Height (ft.)               83\n",
       "Maximum Product Height (in.)       66\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Height_family['name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### extract product width\n",
    "Width_family = attributes[attributes['name'].str.contains('Product Width')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product Width (in.)           61137\n",
       "Actual Product Width (in.)      265\n",
       "Product Width (ft.)             156\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Width_family['name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dup_Width_family = Width_family[Width_family.duplicated('product_uid')==True]\n",
    "Width_family = Width_family.drop(dup_Width_family.index)\n",
    "for i in dup_Width_family['product_uid']:\n",
    "    j = dup_Width_family[dup_Width_family['product_uid']==i].index\n",
    "    for n in np.arange(0,len(j)):\n",
    "        m = j.tolist()[n]\n",
    "        if dup_Width_family.ix[m,'name']=='Product Width (in.)':\n",
    "            Width_family['value'][Width_family['product_uid']==i] = dup_Width_family.ix[m,'value']\n",
    "            if n==len(j)-1:\n",
    "                dup_Width_family = dup_Width_family.drop(j)\n",
    "            else:\n",
    "                collect_id.append(j)\n",
    "            \n",
    "        \n",
    "\n",
    "Width_family['value'][Width_family['name']=='Product Width (ft.)']=\\\n",
    "Width_family['value'][Width_family['name']=='Product Width (ft.)']*12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### extract product depth\n",
    "Depth_family = attributes[attributes['name'].str.contains('Product Depth')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product Depth (in.)           53652\n",
       "Actual Product Depth (in.)      265\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Depth_family['name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### extract product weight\n",
    "Weight_family = attributes[attributes['name'].str.contains('Product Weight')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product Weight (lb.)          45175\n",
       "Product Weight (oz.)            704\n",
       "Total Product Weight (lb.)       35\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Weight_family['name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dup_Weight_family = Weight_family[Weight_family.duplicated('product_uid')==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Weight_family = Weight_family.drop(dup_Weight_family.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### extract product Length\n",
    "Length_family = attributes[attributes['name'].str.contains('Product Length')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product Length (in.)            16705\n",
       "Product Length (ft.)             2241\n",
       "Nominal Product Length (ft.)      296\n",
       "Nominal Product Length (in.)      259\n",
       "Maximum Product Length (in.)      202\n",
       "Minimum Product Length (in.)       32\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Length_family['name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dup_Length_family = Length_family[Length_family.duplicated('product_uid')==True]\n",
    "Length_family = Length_family.drop(dup_Length_family.index)\n",
    "for i in dup_Length_family['product_uid']:\n",
    "    j = dup_Length_family[dup_Length_family['product_uid']==i].index\n",
    "    for n in np.arange(0,len(j)):\n",
    "        m = j.tolist()[n]\n",
    "        if dup_Length_family.ix[m,'name']=='Product Length (in.)':\n",
    "            Length_family['value'][Length_family['product_uid']==i] = dup_Length_family.ix[m,'value']\n",
    "            Length_family['name'][Length_family['product_uid']==i] = 'Product Length (in.)'\n",
    "            if n==len(j)-1:\n",
    "                dup_Length_family = dup_Length_family.drop(j)\n",
    "            else:\n",
    "                collect_id.append(j)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product Length (ft.)            54\n",
       "Nominal Product Length (in.)    25\n",
       "Minimum Product Length (in.)    20\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_Length_family['name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Length_ft = dup_Length_family[dup_Length_family['name']=='Product Length (ft.)']\n",
    "for i in Length_ft['product_uid']:\n",
    "    j = Length_ft[Length_ft['product_uid']==i].index.tolist()[0]\n",
    "    k = Length_family[Length_family['product_uid']==i].index.tolist()[0]\n",
    "    if Length_family.ix[k,'name']=='Product Length (in.)':\n",
    "        dup_Length_family = dup_Length_family.drop(dup_Length_family[dup_Length_family['product_uid']==i].index)   \n",
    "    if Length_family.ix[k,'name']!='Product Length (in.)':\n",
    "        Length_family.ix[k,'name']='Product Length (in.)'\n",
    "        Length_family.ix[k,'value']=Length_ft.ix[j,'value']*12\n",
    "        dup_Length_family = dup_Length_family.drop(dup_Length_family[dup_Length_family['product_uid']==i].index)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Length_family['value'][Length_family['name']=='Product Length (ft.)']=Length_family['value'][Length_family['name']=='Product Length (ft.)']*12\n",
    "Length_family['name'][Length_family['name']=='Product Length (ft.)']='Product Length (in.)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### extract other attributes\n",
    "# CER_LIS = attributes[attributes['name'].str.contains('Certifications and Listings')]\n",
    "I_O = attributes[attributes['name'].str.contains('Indoor/Outdoor')]\n",
    "C_R = attributes[attributes['name'].str.contains('Commercial / Residential')]\n",
    "ESC = attributes[attributes['name'].str.contains('ENERGY STAR Certified')]\n",
    "P_Q = attributes[attributes['name'].str.contains('Package Quantity')]\n",
    "H_I = attributes[attributes['name'].str.contains('Hardware Included')]\n",
    "dup_HI_family = H_I[H_I.duplicated('product_uid')==True]\n",
    "H_I = H_I.drop(dup_HI_family.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_drop = ['Unnamed: 0','name']\n",
    "material_df = material_df.drop(features_to_drop,axis=1)\n",
    "material_df = material_df.rename(columns={'value':'Material'})\n",
    "color_df = color_df.drop(features_to_drop,axis=1)\n",
    "color_df = color_df.rename(columns={'value':'Color'})\n",
    "bullet_df_1 = bullet_df_copy.drop(features_to_drop,axis=1)\n",
    "bullet_df_1 = bullet_df_1.rename(columns={'value':'Bullet'})\n",
    "Height_family = Height_family.drop('name',axis=1)\n",
    "Height_family = Height_family.rename(columns={'value':'Product Height (in.)'})\n",
    "Width_family = Width_family.drop('name',axis=1)\n",
    "Width_family = Width_family.rename(columns={'value':'Product Width (in.)'})\n",
    "Depth_family = Depth_family.drop('name',axis=1)\n",
    "Depth_family = Depth_family.rename(columns={'value':'Product Depth (in.)'})\n",
    "Weight_family = Weight_family.drop('name',axis=1)\n",
    "Weight_family = Weight_family.rename(columns={'value':'Product Weight (lb.)'})\n",
    "Length_family = Length_family.drop('name',axis=1)\n",
    "Length_family = Length_family.rename(columns={'value':'Product Length (in.)'})\n",
    "I_O = I_O.drop('name',axis=1)\n",
    "I_O = I_O.rename(columns={'value':'Indoor/Outdoor'})\n",
    "C_R = C_R.drop('name',axis=1)\n",
    "C_R = C_R.rename(columns={'value':'Commercial / Residential'})\n",
    "ESC = ESC.drop('name',axis=1)\n",
    "ESC = ESC.rename(columns={'value':'ENERGY STAR Certified'})\n",
    "P_Q = P_Q.drop('name',axis=1)\n",
    "P_Q = P_Q.rename(columns={'value':'Package Quantity'})\n",
    "H_I = H_I.drop('name',axis=1)\n",
    "H_I = H_I.rename(columns={'value':'Hardware Included'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "List_att = attributes[attributes['name'].str.contains('List')]\n",
    "List_att = List_att[~List_att['value'].str.contains('No')]\n",
    "L = List_att['product_uid'].unique()\n",
    "Certi_att = attributes[attributes['name'].str.contains('Certifi')]\n",
    "Certi_att = Certi_att[~attributes['name'].str.contains('No')]\n",
    "C = Certi_att['product_uid'].unique()\n",
    "CER_LIS = np.concatenate((C,L), axis=0)\n",
    "CER_LIS = list(set(CER_LIS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['Certification'] = 0\n",
    "for i in CER_LIS:\n",
    "    All_df['Certification'][All_df['product_uid']==i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df = pd.merge(All_df, material_df, how='left', on='product_uid')\n",
    "All_df = pd.merge(All_df, color_df, how='left', on='product_uid')\n",
    "All_df = pd.merge(All_df, bullet_df_1, how='left', on='product_uid')\n",
    "All_df = pd.merge(All_df, Height_family, how='left', on='product_uid')\n",
    "All_df = pd.merge(All_df, Width_family, how='left', on='product_uid')\n",
    "All_df = pd.merge(All_df, Depth_family, how='left', on='product_uid')\n",
    "All_df = pd.merge(All_df, Weight_family, how='left', on='product_uid')\n",
    "All_df = pd.merge(All_df, Length_family, how='left', on='product_uid')\n",
    "All_df = pd.merge(All_df, I_O, how='left', on='product_uid')\n",
    "All_df = pd.merge(All_df, C_R, how='left', on='product_uid')\n",
    "All_df = pd.merge(All_df, ESC, how='left', on='product_uid')\n",
    "All_df = pd.merge(All_df, P_Q, how='left', on='product_uid')\n",
    "All_df = pd.merge(All_df, H_I, how='left', on='product_uid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_parser(s):\n",
    "    s = re.sub('&amp;', '&', s)\n",
    "    s = re.sub('&nbsp;', '', s)\n",
    "    s = re.sub('&#39;', '', s)\n",
    "    s = s.replace(\"-\",\" \")\n",
    "    s = s.replace(\"+\",\" \")\n",
    "    s = re.sub(r'(?<=[a-zA-Z])\\/(?=[a-zA-Z])', ' ', s)\n",
    "    s = re.sub(r'(?<=\\))(?=[a-zA-Z0-9])', ' ', s) # add space between parentheses and letters\n",
    "    s = re.sub(r'(?<=[a-zA-Z0-9])(?=\\()', ' ', s) # add space between parentheses and letters\n",
    "    s = re.sub(r'(?<=[a-zA-Z][\\.\\,])(?=[a-zA-Z])', ' ', s) # add space after dot or colon between letters\n",
    "    s = re.sub('[^a-zA-Z0-9\\n\\ ]', '', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_word_in_string(word,s):\n",
    "    return word in s.split() \n",
    "    \n",
    "def create_bigrams(s):\n",
    "    lst=[word for word in s.split() if len(re.sub('[^0-9]', '', word))==0 and len(word)>2]\n",
    "    output=\"\"\n",
    "    i=0\n",
    "    if len(lst)>=2:\n",
    "        while i<len(lst)-1:\n",
    "            output+= \" \"+lst[i]+\"_\"+lst[i+1]\n",
    "            i+=1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq_matcher(s1,s2):\n",
    "    seq=difflib.SequenceMatcher(None, s1,s2)\n",
    "    rt=round(seq.ratio(),7)\n",
    "    l1=len(s1)\n",
    "    l2=len(s2)\n",
    "    if len(s1)==0 or len(s2)==0:\n",
    "        rt=0\n",
    "        rt_scaled=0\n",
    "    else:\n",
    "        rt_scaled=round(rt*max(l1,l2)/min(l1,l2),7)\n",
    "    return rt, rt_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def col_parser(clmn, automatic_spell_check_dict={}, remove_from_brackets=False,parse_material=False,add_space_stop_list=[]):\n",
    "#     t0 = time()\n",
    "    aa=list(set(list(clmn)))\n",
    "    my_dict={}\n",
    "    for i in range(0,len(aa)):\n",
    "        my_dict[aa[i]]=str_parser(aa[i],automatic_spell_check_dict=automatic_spell_check_dict, remove_from_brackets=remove_from_brackets,\\\n",
    "                                    parse_material=parse_material,add_space_stop_list=add_space_stop_list)\n",
    "        if (i % 10000)==0:\n",
    "            print(\"parsed \"+str(i)+\" out of \"+str(len(aa))+\" unique values; \"+str(round((time()-t0)/60,1))+\" minutes\")\n",
    "    return clmn.map(lambda x: my_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create spell checker dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "458  words from brands in add_space_stop_list\n"
     ]
    }
   ],
   "source": [
    "add_space_stop_list=[]\n",
    "uniq_brands=list(set(list(All_df['Brand'])))\n",
    "for i in range(0,len(uniq_brands)):\n",
    "    uniq_brands[i]=simple_parser(uniq_brands[i])\n",
    "    if re.search(r'[a-z][A-Z][a-z]',uniq_brands[i])!=None:\n",
    "        for word in uniq_brands[i].split():\n",
    "            if re.search(r'[a-z][A-Z][a-z]',word)!=None:\n",
    "                add_space_stop_list.append(word.lower())\n",
    "add_space_stop_list=list(set(add_space_stop_list))      \n",
    "print(len(add_space_stop_list),\" words from brands in add_space_stop_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1722  total words from brands and product titles in add_space_stop_list\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uniq_titles=list(set(list(All_df['product_title'])))\n",
    "for i in range(0,len(uniq_titles)):\n",
    "    uniq_titles[i]=simple_parser(uniq_titles[i])\n",
    "    if re.search(r'[a-z][A-Z][a-z]',uniq_titles[i])!=None:\n",
    "        for word in uniq_titles[i].split():\n",
    "            if re.search(r'[a-z][A-Z][a-z]',word)!=None:\n",
    "                add_space_stop_list.append(word.lower())    \n",
    "add_space_stop_list=list(set(add_space_stop_list))      \n",
    "print(len(add_space_stop_list) ,\" total words from brands and product titles in add_space_stop_list\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['search_term'] = All_df['search_term'].map(lambda x: google_dict[x] if x in google_dict.keys() else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['product_title_simpleparsed'] = All_df['product_title'].map(lambda x: simple_parser(x).lower())\n",
    "All_df['search_term_simpleparsed'] = All_df['search_term'].map(lambda x: simple_parser(x).lower())\n",
    "\n",
    "str_title = \" \".join(list(All_df['product_title'].map(lambda x: simple_parser(x).lower())))\n",
    "str_query = \" \".join(list(All_df['search_term'].map(lambda x: simple_parser(x).lower())))\n",
    "\n",
    "# create bigrams\n",
    "bigrams_str_title = \" \".join(list(All_df['product_title'].map(lambda x: create_bigrams(simple_parser(x).lower()))))\n",
    "bigrams_set = set(bigrams_str_title.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_dict={}\n",
    "str1= str_title+\" \"+str_query\n",
    "for word in list(set(list(str1.split()))):\n",
    "    my_dict[word]={\"title\":0, \"query\":0, 'word':word}\n",
    "for word in str_title.split():\n",
    "    my_dict[word][\"title\"]+=1    \n",
    "for word in str_query.split():\n",
    "    my_dict[word][\"query\"]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errors_dict={}\n",
    "correct_dict={}\n",
    "for word in my_dict.keys():\n",
    "    if len(word)>=3 and len(re.sub('[^0-9]', '', word))==0:\n",
    "        if my_dict[word][\"title\"]==0:\n",
    "            if len(wn.synsets(word))>0 \\\n",
    "            or (word.endswith('s') and  (word[:-1] in my_dict.keys()) and my_dict[word[:-1]][\"title\"]>0)\\\n",
    "            or (word[-1]!='s' and (word+'s' in my_dict.keys()) and my_dict[word+'s'][\"title\"]>0):\n",
    "                1\n",
    "            else:\n",
    "                errors_dict[word]=my_dict[word]\n",
    "        elif my_dict[word][\"title\"]>=5:\n",
    "            correct_dict[word]=my_dict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnt=0\n",
    "NN=len(errors_dict.keys())\n",
    "# t0=time()\n",
    "for i in range(0,len(errors_dict.keys())):\n",
    "    word=sorted(errors_dict.keys())[i]\n",
    "    cnt+=1\n",
    "    lst=[]\n",
    "    lst_tuple=[]\n",
    "    suggested=False\n",
    "    suggested_word=\"\"\n",
    "    rt_max=0\n",
    "    \n",
    "    # if only one word in query, use be more selective in choosing a correction\n",
    "    min_query_len=min(All_df['search_term_simpleparsed'][All_df['search_term_simpleparsed'].map(lambda x: is_word_in_string(word,x))].map(lambda x: len(x.split())))\n",
    "    delta=0.05*int(min_query_len<2)\n",
    "    \n",
    "    words_from_matched_titles=[item for item in \\\n",
    "        \" \".join(list(set(All_df['product_title_simpleparsed'][All_df['search_term_simpleparsed'].map(lambda x: is_word_in_string(word,x))]))).split() \\\n",
    "        if len(item)>2 and len(re.sub('[^0-9]', '', item))==0]\n",
    "    words_from_matched_titles=list(set(words_from_matched_titles))\n",
    "    words_from_matched_titles.sort()\n",
    "    \n",
    "    source=\"\"\n",
    "    for bigram in bigrams_set:\n",
    "        if bigram.replace(\"_\",\"\")==word:\n",
    "            suggested=True\n",
    "            suggested_word=bigram.replace(\"_\",\" \")\n",
    "            source=\"from bigrams\"\n",
    "    if source==\"\":\n",
    "        for correct_word in words_from_matched_titles: \n",
    "            rt, rt_scaled = seq_matcher(word,correct_word)\n",
    "            #print correct_word, rt,rt_scaled\n",
    "            \n",
    "            if rt>0.75+delta or (len(word)<6 and rt>0.68+delta):\n",
    "                lst.append(correct_word)\n",
    "                lst_tuple.append((correct_word,my_dict[correct_word][\"title\"]))\n",
    "                if rt>rt_max:\n",
    "                    rt_max=rt\n",
    "                    suggested=True\n",
    "                    source=\"from matched products\"\n",
    "                    suggested_word=correct_word\n",
    "                elif rt==rt_max and seq_matcher(\"\".join(sorted(word)),\"\".join(sorted(correct_word)))[0]>seq_matcher(\"\".join(sorted(word)),\"\".join(sorted(suggested_word)))[0]:\n",
    "                    suggested_word=correct_word\n",
    "                elif rt==rt_max:\n",
    "                    suggested=False\n",
    "                    source=\"\"\n",
    "        \n",
    "    if source==\"\" and len(lst)==0:\n",
    "        source=\"from all products\"\n",
    "        for correct_word in correct_dict.keys():\n",
    "            rt, rt_scaled = seq_matcher(word,correct_word)\n",
    "            #print correct_word, rt,rt_scaled\n",
    "            if correct_dict[correct_word][\"title\"]>10 and (rt>0.8+delta or (len(word)<6 and rt>0.73+delta)):\n",
    "                #print correct_word, rt,rt_scaled\n",
    "                lst.append(correct_word)\n",
    "                lst_tuple.append((correct_word,correct_dict[correct_word][\"title\"]))\n",
    "                if rt>rt_max:\n",
    "                    rt_max=rt\n",
    "                    suggested=True\n",
    "                    suggested_word=correct_word\n",
    "                elif rt==rt_max and seq_matcher(\"\".join(sorted(word)),\"\".join(sorted(correct_word)))[0]>seq_matcher(\"\".join(sorted(word)),\"\".join(sorted(suggested_word)))[0]:\n",
    "                    suggested_word=correct_word\n",
    "                elif rt==rt_max: \n",
    "                    suggested=False\n",
    "                    \n",
    "    if suggested==True:\n",
    "        errors_dict[word][\"suggestion\"]=suggested_word\n",
    "        errors_dict[word][\"others\"]=lst_tuple\n",
    "        errors_dict[word][\"source\"]=source\n",
    "    else:\n",
    "        errors_dict[word][\"suggestion\"]=\"\"\n",
    "        errors_dict[word][\"others\"]=lst_tuple\n",
    "        errors_dict[word][\"source\"]=source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in my_dict.keys():\n",
    "    if my_dict[word]['query']>0 and my_dict[word]['title']==0 \\\n",
    "    and len(re.sub('[^0-9]', '', word))!=0 and len(re.sub('[^a-z]', '', word))!=0:\n",
    "        srch=re.search(r'(?<=^)[a-z][a-z][a-z]+(?=[0-9])',word)\n",
    "        if srch!=None and len(wn.synsets(srch.group(0)))>0 \\\n",
    "        and len(re.sub('[^aeiou]', '', word))>0 and word[-1] in '0123456789': \n",
    "            errors_dict[word]=my_dict[word]\n",
    "            errors_dict[word][\"source\"]=\"added space before digit\"\n",
    "            errors_dict[word][\"suggestion\"]=re.sub(r'(?<=^)'+srch.group(0)+r'(?=[a-zA-Z0-9])',srch.group(0)+' ',word)\n",
    "            #print word, re.sub(r'(?<=^)'+srch.group(0)+r'(?=[a-zA-Z0-9])',srch.group(0)+' ',word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### load words for spell checker\n",
    "spell_check_dict={}\n",
    "for word in errors_dict.keys():\n",
    "    if errors_dict[word]['suggestion']!=\"\":\n",
    "        spell_check_dict[word]=errors_dict[word]['suggestion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore search_term and product title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed 0 out of 24469 unique values; 0.0 minutes\n",
      "parsed 10000 out of 24469 unique values; 7.8 minutes\n",
      "parsed 20000 out of 24469 unique values; 15.7 minutes\n",
      "parsed 0 out of 24469 unique values; 19.2 minutes\n",
      "parsed 10000 out of 24469 unique values; 19.2 minutes\n",
      "parsed 20000 out of 24469 unique values; 19.3 minutes\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "All_df['search_term_parsed']=col_parser(All_df['search_term'],automatic_spell_check_dict=spell_check_dict,\\\n",
    "            add_space_stop_list=[])#.map(lambda x: x.encode('utf-8'))\n",
    "All_df['search_term_parsed_wospellcheck']=col_parser(All_df['search_term'],automatic_spell_check_dict={},\\\n",
    "            add_space_stop_list=[])#.map(lambda x: x.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def match_queries(q1,q2):\n",
    "#     q1=re.sub('[^a-z\\ ]', '', map(str,q1))\n",
    "#     q2=re.sub('[^a-z\\ ]', '', map(str,q2))\n",
    "#     q1= \" \".join([word[0:(len(word)-int(word[-1]=='s'))] for word in q1.split()])\n",
    "#     q2= \" \".join([word[0:(len(word)-int(word[-1]=='s'))] for word in q2.split()])\n",
    "#     return difflib.SequenceMatcher(None, q1,q2).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All_df['is_query_misspelled'] = All_df.apply(lambda x: \\\n",
    "#             match_queries(x['search_term_parsed'],x['search_term_parsed_wospellcheck']),axis=1)\n",
    "# All_df = All_df.drop(['search_term_parsed_wospellcheck'],axis=1)    \n",
    "# print('create dummy \"is_query_misspelled\" time:',round((time()-t0)/60,1) ,'minutes\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed 0 out of 120453 unique values; 0.0 minutes\n",
      "parsed 10000 out of 120453 unique values; 0.1 minutes\n",
      "parsed 20000 out of 120453 unique values; 0.1 minutes\n",
      "parsed 30000 out of 120453 unique values; 0.2 minutes\n",
      "parsed 40000 out of 120453 unique values; 0.3 minutes\n",
      "parsed 50000 out of 120453 unique values; 0.3 minutes\n",
      "parsed 60000 out of 120453 unique values; 0.4 minutes\n",
      "parsed 70000 out of 120453 unique values; 0.5 minutes\n",
      "parsed 80000 out of 120453 unique values; 0.6 minutes\n",
      "parsed 90000 out of 120453 unique values; 0.6 minutes\n",
      "parsed 100000 out of 120453 unique values; 0.7 minutes\n",
      "parsed 110000 out of 120453 unique values; 0.8 minutes\n",
      "parsed 120000 out of 120453 unique values; 0.8 minutes\n",
      "product_title parsing time: 0.8 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "All_df['product_title_parsed']=col_parser(All_df['product_title'],add_space_stop_list=[],\\\n",
    "                remove_from_brackets=True)#.map(lambda x: x.encode('utf-8'))\n",
    "print('product_title parsing time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed 0 out of 4285 unique values; 0.0 minutes\n"
     ]
    }
   ],
   "source": [
    "### parse brand\n",
    "All_df['brand_parsed'] = col_parser(All_df['Brand'].map(lambda x: re.sub('^[t|T]he ', '', x.replace(\".N/A\",\"\").replace(\"N.A.\",\"\").replace(\"n/a\",\"\").replace(\"Generic Unbranded\",\"\").replace(\"Unbranded\",\"\").replace(\"Generic\",\"\"))),add_space_stop_list=add_space_stop_list)\n",
    "list_brands = list(All_df['brand_parsed'])\n",
    "\n",
    "All_df['brand_parsed'] = All_df['brand_parsed'].map(lambda x: replace_brand_dict[x] if x in replace_brand_dict.keys() else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating brand dict: How many times each brand appears in the dataset?\n",
      "500 out of 4266 unique attributes 1.1 minutes\n",
      "1000 out of 4266 unique attributes 2.2 minutes\n",
      "1500 out of 4266 unique attributes 3.4 minutes\n",
      "2000 out of 4266 unique attributes 4.5 minutes\n",
      "2500 out of 4266 unique attributes 5.6 minutes\n",
      "3000 out of 4266 unique attributes 6.7 minutes\n",
      "3500 out of 4266 unique attributes 7.9 minutes\n",
      "4000 out of 4266 unique attributes 9.0 minutes\n"
     ]
    }
   ],
   "source": [
    "### count frequencies of brands in query and product_title\n",
    "str_query=\" : \".join(list(map(str,All_df['search_term_parsed']))).lower()\n",
    "print(\"\\nGenerating brand dict: How many times each brand appears in the dataset?\")\n",
    "brand_dict=get_attribute_dict(list_brands,str_query=str_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del_list=['aaa','off','impact','square','shelves','finish','ring','flood','dual','ball','cutter',\\\n",
    "'max','off','mat','allure','diamond','drive', 'edge','anchor','walls','universal','cat', 'dawn','ion','daylight',\\\n",
    "'roman', 'weed eater', 'restore', 'design', 'caddy', 'pole caddy', 'jet', 'classic', 'element', 'aqua',\\\n",
    "'terra', 'decora', 'ez', 'briggs', 'wedge', 'sunbrella',  'adorne', 'santa', 'bella', 'duck', 'hotpoint',\\\n",
    "'duck', 'tech', 'titan', 'powerwasher', 'cooper lighting', 'heritage', 'imperial', 'monster', 'peak', \n",
    "'bell', 'drive', 'trademark', 'toto', 'champion', 'shop vac', 'lava', 'jet', 'flood', \\\n",
    "'roman', 'duck', 'magic', 'allen', 'bunn', 'element', 'international', 'larson', 'tiki', 'titan', \\\n",
    " 'space saver', 'cutter', 'scotch', 'adorne', 'ball', 'sunbeam', 'fatmax', 'poulan', 'ring', 'sparkle', 'bissell', \\\n",
    " 'universal', 'paw', 'wedge', 'restore', 'daylight', 'edge', 'americana', 'wacker', 'cat', 'allure', 'bonnie plants', \\\n",
    " 'troy', 'impact', 'buffalo', 'adams', 'jasco', 'rapid dry', 'aaa', 'pole caddy', 'pac', 'seymour', 'mobil', \\\n",
    " 'mastercool', 'coca cola', 'timberline', 'classic', 'caddy', 'sentry', 'terrain', 'nautilus', 'precision', \\\n",
    " 'artisan', 'mural', 'game', 'royal', 'use', 'dawn', 'task', 'american line', 'sawtrax', 'solo', 'elements', \\\n",
    " 'summit', 'anchor', 'off', 'spruce', 'medina', 'shoulder dolly', 'brentwood', 'alex', 'wilkins', 'natural magic', \\\n",
    " 'kodiak', 'metro', 'shelter', 'centipede', 'imperial', 'cooper lighting', 'exide', 'bella', 'ez', 'decora', \\\n",
    " 'terra', 'design', 'diamond', 'mat', 'finish', 'tilex', 'rhino', 'crock pot', 'legend', 'leatherman', 'remove', \\\n",
    " 'architect series', 'greased lightning', 'castle', 'spirit', 'corian', 'peak', 'monster', 'heritage', 'powerwasher',\\\n",
    " 'reese', 'tech', 'santa', 'briggs', 'aqua', 'weed eater', 'ion', 'walls', 'max', 'dual', 'shelves', 'square',\\\n",
    " 'hickory', \"vikrell\", \"e3\", \"pro series\", \"keeper\", \"coastal shower doors\", 'cadet','church','gerber','glidden',\\\n",
    " 'cooper wiring devices', 'border blocks', 'commercial electric', 'pri','exteria','extreme', 'veranda',\\\n",
    " 'gorilla glue','gorilla','shark','wen']\n",
    "del_list=list(set(list(del_list)))\n",
    "\n",
    "for key in del_list:\n",
    "    if key in brand_dict.keys():\n",
    "        del(brand_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# brand_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for item in brand_dict.keys():\n",
    "#     if (brand_dict[item]['cnt_attribute']>=3 and brand_dict[item]['cnt_query']>=1) \\\n",
    "#     or (brand_dict[item]['cnt_attribute'])>=8:\n",
    "#         1\n",
    "#     else:\n",
    "#         del(brand_dict[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brand_df=pd.DataFrame(brand_dict).transpose().sort(['cnt_query'], ascending=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore Material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def change_material(s):\n",
    "    replace_dict={'Medium Density Fiberboard (MDF)':'mdf', 'High Density Fiberboard (HDF)':'hdf',\\\n",
    "    'Fibre Reinforced Polymer (FRP)': 'frp', 'Acrylonitrile Butadiene Styrene (ABS)': 'abs',\\\n",
    "    'Cross-Linked Polyethylene (PEX)':'pex', 'Chlorinated Poly Vinyl Chloride (CPVC)': 'cpvc',\\\n",
    "    'PVC (vinyl)': 'pvc','Thermoplastic rubber (TPR)':'tpr','Poly Lactic Acid (PLA)': 'pla',\\\n",
    "    '100% Polyester':'polyester','100% UV Olefin':'olefin', '100% BCF Polypropylene': 'polypropylene',\\\n",
    "    '100% PVC':'pvc'}\n",
    "        \n",
    "    if s in replace_dict.keys():\n",
    "        s=replace_dict[s]\n",
    "    return s\n",
    "All_df['Material'] = All_df['Material'].map(lambda x: change_material(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "# do_copy = True\n",
    "# if do_copy:\n",
    "dict_materials = {}\n",
    "key_list = material_df['product_uid'].keys()\n",
    "for i in range(0,len(key_list)):\n",
    "    if material_df['product_uid'][key_list[i]] not in dict_materials.keys():\n",
    "        dict_materials[material_df['product_uid'][key_list[i]]]={}\n",
    "        dict_materials[material_df['product_uid'][key_list[i]]]['product_uid']=material_df['product_uid'][key_list[i]]\n",
    "        dict_materials[material_df['product_uid'][key_list[i]]]['cnt']=1\n",
    "        dict_materials[material_df['product_uid'][key_list[i]]]['material']=material_df['Material'][key_list[i]]\n",
    "    else:\n",
    "        ##print key_list[i]\n",
    "        dict_materials[material_df['product_uid'][key_list[i]]]['material']=dict_materials[material_df['product_uid'][key_list[i]]]['material']+' '+material_df['Material'][key_list[i]]\n",
    "        dict_materials[material_df['product_uid'][key_list[i]]]['cnt']+=1\n",
    "    if (i % 10000)==0:\n",
    "        print (i)\n",
    "                       \n",
    "df_materials=pd.DataFrame(dict_materials).transpose()\n",
    "else:\n",
    "    All_df['Material'] = All_df['Material'].map(lambda x: change_material(x))\n",
    "    material_df['value'] = material_df['Material'].map(lambda x: change_material(x))\n",
    "    key_list=material_df['product_uid'].keys()\n",
    "    dict_materials = {}\n",
    "    key_list=material_df['product_uid'].keys()\n",
    "    for i in range(0,len(key_list)):\n",
    "        if material_df['product_uid'][key_list[i]] not in dict_materials.keys():\n",
    "            dict_materials[material_df['product_uid'][key_list[i]]]={}\n",
    "            dict_materials[material_df['product_uid'][key_list[i]]]['product_uid']=material_df['product_uid'][key_list[i]]\n",
    "            dict_materials[material_df['product_uid'][key_list[i]]]['cnt']=1\n",
    "            dict_materials[material_df['product_uid'][key_list[i]]]['material']=material_df['value'][key_list[i]]\n",
    "        else:\n",
    "            ##print key_list[i]\n",
    "            dict_materials[material_df['product_uid'][key_list[i]]]['material']=dict_materials[material_df['product_uid'][key_list[i]]]['material']+' '+material_df['value'][key_list[i]]\n",
    "            dict_materials[material_df['product_uid'][key_list[i]]]['cnt']+=1\n",
    "        if (i % 10000)==0:\n",
    "            print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed 0 out of 1851 unique values; 9.9 minutes\n"
     ]
    }
   ],
   "source": [
    "All_df = All_df.drop('Material',axis=1)\n",
    "All_df = pd.merge(All_df, material_df, how='left', on='product_uid')\n",
    "All_df['Material']=All_df['Material'].fillna(\"\")#.map(lambda x: x.encode('utf-8'))\n",
    "All_df['material_parsed']=col_parser(All_df['Material'].map(lambda x: x.replace(\"Other\",\"\").replace(\"*\",\"\")), parse_material=True,add_space_stop_list=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating material dict: How many times each material appears in the dataset?\n",
      "500 out of 1668 unique attributes 1.1 minutes\n",
      "1000 out of 1668 unique attributes 2.3 minutes\n",
      "1500 out of 1668 unique attributes 3.4 minutes\n"
     ]
    }
   ],
   "source": [
    "### list of all materials\n",
    "list_materials=list(All_df['material_parsed'].map(lambda x: x.lower())) \n",
    "### count frequencies of materials in query and product_title\n",
    "print(\"\\nGenerating material dict: How many times each material appears in the dataset?\")\n",
    "material_dict=get_attribute_dict(list_materials,str_query=str_query)\n",
    "### create dataframe and save to file\n",
    "material_df_1=pd.DataFrame(material_dict).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate whether material and brand in query and title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getremove_brand_or_material_from_str(s,df, replace_brand_dict={}):\n",
    "    items_found=[]\n",
    "    df=df.sort_values(['nwords'],ascending=[0])\n",
    "    key_list=df['nwords'].keys()\n",
    "    #start with several-word brands or materials\n",
    "    #assert df['nwords'][key_list[0]]>1\n",
    "    for i in range(0,len(key_list)):\n",
    "        item=df['name'][key_list[i]]\n",
    "        if item in s:\n",
    "            if re.search(r'\\b'+item+r'\\b',s)!=None:\n",
    "                s=re.sub(r'\\b'+item+r'\\b', '', s)\n",
    "                if item in replace_brand_dict.keys():\n",
    "                    items_found.append(replace_brand_dict[item])\n",
    "                else:\n",
    "                    items_found.append(item)\n",
    "\n",
    "    return \" \".join(s.split()), \";\".join(items_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aa=list(set(list(All_df['search_term_parsed'])))\n",
    "my_dict={}\n",
    "for i in range(0,len(aa)):\n",
    "    my_dict[aa[i]]=getremove_brand_or_material_from_str(aa[i],brand_df)\n",
    "    if (i % 5000)==0:\n",
    "        print(\"Extracted brands from\",i,\"out of\",len(aa),\"unique search terms; \", str(round((time()-t0)/60,1)),\"minutes\")\n",
    "All_df['search_term_tuple']= All_df['search_term_parsed'].map(lambda x: my_dict[x])\n",
    "All_df['search_term_parsed_woBrand']= All_df['search_term_tuple'].map(lambda x: x[0])\n",
    "All_df['brands_in_search_term']= All_df['search_term_tuple'].map(lambda x: x[1])\n",
    "print('extract brands from query time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()\n",
    "\n",
    "All_df['search_term_tuple']= All_df['search_term_parsed_woBrand'].map(lambda x: getremove_brand_or_material_from_str(x,material_df_1))\n",
    "All_df['search_term_parsed_woBM']= All_df['search_term_tuple'].map(lambda x: x[0])\n",
    "All_df['materials_in_search_term']= All_df['search_term_tuple'].map(lambda x: x[1])\n",
    "All_df = All_df.drop('search_term_tuple',axis=1)\n",
    "print('extract materials from query time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['search_term_parsed_woBrand']= All_df['search_term_parsed_woBrand'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract materials from query time: 351.5 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "All_df['search_term_tuple']= All_df['search_term_parsed_woBrand'].map(lambda x: getremove_brand_or_material_from_str(x,material_df_1))\n",
    "All_df['search_term_parsed_woBM']= All_df['search_term_tuple'].map(lambda x: x[0])\n",
    "All_df['materials_in_search_term']= All_df['search_term_tuple'].map(lambda x: x[1])\n",
    "All_df = All_df.drop('search_term_tuple',axis=1)\n",
    "print('extract materials from query time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted brands from 0 out of 118401 unique product titles;  0.0 minutes\n",
      "Extracted brands from 5000 out of 118401 unique product titles;  7.3 minutes\n",
      "Extracted brands from 10000 out of 118401 unique product titles;  14.6 minutes\n",
      "Extracted brands from 15000 out of 118401 unique product titles;  21.9 minutes\n",
      "Extracted brands from 20000 out of 118401 unique product titles;  29.1 minutes\n",
      "Extracted brands from 25000 out of 118401 unique product titles;  36.4 minutes\n",
      "Extracted brands from 30000 out of 118401 unique product titles;  43.6 minutes\n",
      "Extracted brands from 35000 out of 118401 unique product titles;  50.9 minutes\n",
      "Extracted brands from 40000 out of 118401 unique product titles;  58.1 minutes\n",
      "Extracted brands from 45000 out of 118401 unique product titles;  65.4 minutes\n",
      "Extracted brands from 50000 out of 118401 unique product titles;  72.6 minutes\n",
      "Extracted brands from 55000 out of 118401 unique product titles;  79.9 minutes\n",
      "Extracted brands from 60000 out of 118401 unique product titles;  87.2 minutes\n",
      "Extracted brands from 65000 out of 118401 unique product titles;  94.4 minutes\n",
      "Extracted brands from 70000 out of 118401 unique product titles;  101.7 minutes\n",
      "Extracted brands from 75000 out of 118401 unique product titles;  109.0 minutes\n",
      "Extracted brands from 80000 out of 118401 unique product titles;  116.3 minutes\n",
      "Extracted brands from 85000 out of 118401 unique product titles;  123.6 minutes\n",
      "Extracted brands from 90000 out of 118401 unique product titles;  130.9 minutes\n",
      "Extracted brands from 95000 out of 118401 unique product titles;  138.1 minutes\n",
      "Extracted brands from 100000 out of 118401 unique product titles;  145.4 minutes\n",
      "Extracted brands from 105000 out of 118401 unique product titles;  152.7 minutes\n",
      "Extracted brands from 110000 out of 118401 unique product titles;  160.0 minutes\n",
      "Extracted brands from 115000 out of 118401 unique product titles;  167.3 minutes\n",
      "extract brands from product title time: 172.2 minutes\n",
      "\n",
      "extract materials from product titles time: 141.0 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aa=list(set(list(All_df['product_title_parsed'])))\n",
    "my_dict={}\n",
    "for i in range(0,len(aa)):\n",
    "    my_dict[aa[i]]=getremove_brand_or_material_from_str(aa[i],brand_df)\n",
    "    if (i % 5000)==0:\n",
    "        print(\"Extracted brands from\",i,\"out of\",len(aa),\"unique product titles; \", str(round((time()-t0)/60,1)),\"minutes\")\n",
    "\n",
    "All_df['product_title_tuple']= All_df['product_title_parsed'].map(lambda x: my_dict[x])\n",
    "All_df['product_title_parsed_woBrand']= All_df['product_title_tuple'].map(lambda x: x[0])\n",
    "All_df['brands_in_product_title']= All_df['product_title_tuple'].map(lambda x: x[1])\n",
    "print('extract brands from product title time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()\n",
    "\n",
    "\n",
    "All_df['product_title_tuple']= All_df['product_title_parsed_woBrand'].map(lambda x: getremove_brand_or_material_from_str(x,material_df_1))\n",
    "All_df['product_title_parsed_woBM']= All_df['product_title_tuple'].map(lambda x: x[0])\n",
    "All_df['materials_in_product_title']= All_df['product_title_tuple'].map(lambda x: x[1])\n",
    "All_df = All_df.drop('product_title_tuple',axis=1)\n",
    "print('extract materials from product titles time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged 20000 out of 240761 total rows; 0.1 minutes\n",
      "tagged 40000 out of 240761 total rows; 0.2 minutes\n",
      "tagged 60000 out of 240761 total rows; 0.3 minutes\n",
      "tagged 80000 out of 240761 total rows; 0.4 minutes\n",
      "tagged 100000 out of 240761 total rows; 0.6 minutes\n",
      "tagged 120000 out of 240761 total rows; 0.7 minutes\n",
      "tagged 140000 out of 240761 total rows; 0.8 minutes\n",
      "tagged 160000 out of 240761 total rows; 0.9 minutes\n",
      "tagged 180000 out of 240761 total rows; 1.0 minutes\n",
      "tagged 200000 out of 240761 total rows; 1.1 minutes\n",
      "tagged 220000 out of 240761 total rows; 1.2 minutes\n",
      "tagged 240000 out of 240761 total rows; 1.3 minutes\n",
      "tagged 240761 out of 240761 total rows; 1.3 minutes\n",
      "search term tagging time: 1.3 minutes\n",
      "\n",
      "tagged 20000 out of 240761 total rows; 0.3 minutes\n",
      "tagged 40000 out of 240761 total rows; 0.6 minutes\n",
      "tagged 60000 out of 240761 total rows; 0.9 minutes\n",
      "tagged 80000 out of 240761 total rows; 1.2 minutes\n",
      "tagged 100000 out of 240761 total rows; 1.5 minutes\n",
      "tagged 120000 out of 240761 total rows; 1.8 minutes\n",
      "tagged 140000 out of 240761 total rows; 2.0 minutes\n",
      "tagged 160000 out of 240761 total rows; 2.3 minutes\n",
      "tagged 180000 out of 240761 total rows; 2.6 minutes\n",
      "tagged 200000 out of 240761 total rows; 2.9 minutes\n",
      "tagged 220000 out of 240761 total rows; 3.2 minutes\n",
      "tagged 240000 out of 240761 total rows; 3.5 minutes\n",
      "tagged 240761 out of 240761 total rows; 3.5 minutes\n",
      "product title tagging time: 3.5 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### tagging\n",
    "All_df['search_term_tokens'] =col_tagger(All_df['search_term_parsed_woBM'])\n",
    "print('search term tagging time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()\n",
    "All_df['product_title_tokens'] =col_tagger(All_df['product_title_parsed_woBM'])\n",
    "print('product title tagging time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore Bullets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['Bullet'] = All_df['Bullet'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Parsing\n",
    "All_df['attribute_bullets_parsed'] = All_df['Bullet'].map(lambda x:str_parser(x,add_space_stop_list=[]))\n",
    "print('attribute bullets parsing time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()\n",
    "\n",
    "### Extracting brands...\n",
    "All_df['attribute_bullets_tuple']= All_df['attribute_bullets_parsed'].map(lambda x: getremove_brand_or_material_from_str(x,brand_df))\n",
    "All_df['attribute_bullets_parsed_woBrand']= All_df['attribute_bullets_tuple'].map(lambda x: x[0])\n",
    "All_df['brands_in_attribute_bullets']= All_df['attribute_bullets_tuple'].map(lambda x: x[1])\n",
    "print('extract brands from attribute_bullets time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()\n",
    "\n",
    "## ... and materials from text...\n",
    "All_df['attribute_bullets_tuple']= All_df['attribute_bullets_parsed_woBrand'].map(lambda x: getremove_brand_or_material_from_str(x,material_df_1))\n",
    "All_df['attribute_bullets_parsed_woBM']= All_df['attribute_bullets_tuple'].map(lambda x: x[0])\n",
    "All_df['materials_in_attribute_bullets']= All_df['attribute_bullets_tuple'].map(lambda x: x[1])\n",
    "All_df = All_df.drop(['attribute_bullets_tuple'],axis=1)\n",
    "print('extract materials from attribute_bullets time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()\n",
    "\n",
    "### ... and tagging text using NLTK\n",
    "All_df['attribute_bullets_tokens'] =col_tagger(All_df['attribute_bullets_parsed_woBM'])\n",
    "print('attribute bullets tagging time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['attribute_bullets_parsed'] = All_df['attribute_bullets_parsed'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### explore Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product description parsing time: 100.6 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Parsing\n",
    "pro_descr['product_description_parsed'] = pro_descr['product_description'].map(lambda x:str_parser(x,add_space_stop_list=add_space_stop_list))#.encode('utf-8'))\n",
    "print('product description parsing time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract brands from product_description time: 189.8 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "pro_descr['product_description_tuple']= pro_descr['product_description_parsed'].map(lambda x: getremove_brand_or_material_from_str(x,brand_df))\n",
    "pro_descr['product_description_parsed_woBrand']= pro_descr['product_description_tuple'].map(lambda x: x[0])\n",
    "pro_descr['brands_in_product_description']= pro_descr['product_description_tuple'].map(lambda x: x[1])\n",
    "print('extract brands from product_description time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract materials from product_description time: 77.5 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### ... and materials from text...\n",
    "pro_descr['product_description_tuple']= pro_descr['product_description_parsed_woBrand'].map(lambda x: getremove_brand_or_material_from_str(x,material_df_1))\n",
    "pro_descr['product_description_parsed_woBM']= pro_descr['product_description_tuple'].map(lambda x: x[0])\n",
    "pro_descr['materials_in_product_description']= pro_descr['product_description_tuple'].map(lambda x: x[1])\n",
    "pro_descr=pro_descr.drop(['product_description_tuple'],axis=1)\n",
    "print('extract materials from product_description time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged 20000 out of 124428 total rows; 1.8 minutes\n",
      "tagged 40000 out of 124428 total rows; 3.6 minutes\n",
      "tagged 60000 out of 124428 total rows; 5.4 minutes\n",
      "tagged 80000 out of 124428 total rows; 7.2 minutes\n",
      "tagged 100000 out of 124428 total rows; 9.0 minutes\n",
      "tagged 120000 out of 124428 total rows; 10.7 minutes\n",
      "tagged 124428 out of 124428 total rows; 11.1 minutes\n",
      "product decription tagging time: 11.1 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### ... and tagging text using NLTK\n",
    "pro_descr['product_description_tokens'] = col_tagger(pro_descr['product_description_parsed_woBM'])\n",
    "print('product decription tagging time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()\n",
    "\n",
    "# pro_descr['product_description']= pro_descr['product_description']#.map(lambda x: x.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get importand words from query and title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cut_product_title(s):\n",
    "    s=s.lower()\n",
    "    s = re.sub('&amp;', '&', s)\n",
    "    s = re.sub('&nbsp;', '', s)\n",
    "    s = re.sub('&#39;', '', s)\n",
    "    s = re.sub(r'(?<=[0-9]),[\\ ]*(?=[0-9])', '', s)\n",
    "    s = re.sub(r'(?<=\\))(?=[a-zA-Z0-9])', ' ', s) # add space between parentheses and letters\n",
    "    s = re.sub(r'(?<=[a-zA-Z0-9])(?=\\()', ' ', s) # add space between parentheses and letters\n",
    "    s = s.replace(\";\",\". \")\n",
    "    s = s.replace(\":\",\" . \")\n",
    "    s=s.replace(\"&\",\" and \") \n",
    "    s = re.sub('[^a-zA-Z0-9\\n\\ \\%\\$\\-\\#\\@\\&\\/\\.\\'\\*\\(\\)\\,\\+]', ' ', s)\n",
    "    s= \" \".join(s.split())\n",
    "    s = re.sub(r'(?<=[0-9])\\.\\ ', ' ', s)\n",
    "    s = re.sub(r'(?<=\\ in)\\.(?=[a-zA-Z])', '. ', s)\n",
    "    \n",
    "    s=replace_in_parser(s)\n",
    "    \n",
    "    s = re.sub(r'\\-discontinued', '', s)\n",
    "    s = re.sub(r' \\+ free app(?=$)', '', s)\n",
    "    s = s.replace(\"+\",\" \")    \n",
    "    s = re.sub('\\([a-zA-Z0-9\\n\\ \\%\\$\\-\\#\\@\\&\\/\\.\\'\\*\\(\\)\\,]+?\\)', '', s)\n",
    "    #s= re.sub('[\\(\\)]', '', s)\n",
    "    if \" - \" in s:\n",
    "        #srch=re.search(r'(?<= - )[a-zA-Z0-9\\n\\ \\%\\$\\-\\#\\@\\&\\/\\.\\'\\*\\(\\)\\,]+',s) \n",
    "        if re.search(r'(\\d|\\.|mm|cm|in|ft|mhz|volt|\\bhp|\\bl|oz|lb|gal) \\- \\d',s)==None \\\n",
    "            and re.search(r' (sign|carpet|decal[s]*|figure[s]*)(?=$)',s)==None and re.search(r'\\d \\- (way\\b|day\\b)',s)==None: \n",
    "        #if ' - ' is found and the string doesnt end with word 'sign' or 'carpet' or 'decal' and not string '[0-9] - way' found\n",
    "            s = re.sub(r' - [a-zA-Z0-9\\n\\ \\%\\$\\-\\#\\@\\&\\/\\.\\'\\*\\(\\)\\,]*', '', s) #greedy regular expression\n",
    "    if \"uilt in\" not in s and \"uilt In\" not in s:\n",
    "        s = re.sub(r'(?<=[a-zA-Z\\%\\$\\#\\@\\&\\/\\.\\*])[\\ ]+[I|i]n [a-zA-Z0-9\\n\\ \\%\\$\\-\\#\\@\\&\\/\\.\\'\\*\\(\\)\\,]*', '', s)\n",
    "    s = s.replace(\" - \",\" \")\n",
    "    if re.search(r' (sign|decal[s]*|figure[s]*)(?=$)',s)==None:    \n",
    "        s = re.sub(r'(?<=[a-zA-Z0-9\\%\\$\\#\\@\\&\\/\\.\\*])[\\ ]+[W|w]ith [a-zA-Z0-9\\n\\ \\%\\$\\-\\#\\@\\&\\/\\.\\'\\*\\(\\)\\,]*', '', s)\n",
    "        s = re.sub(r'(?<=[a-zA-Z0-9\\%\\$\\#\\@\\&\\/\\.\\*])[\\ ]+[W|w]ithout [a-zA-Z0-9\\n\\ \\%\\$\\-\\#\\@\\&\\/\\.\\'\\*\\(\\)\\,]*', '', s)\n",
    "    s = re.sub(r'(?<=[a-zA-Z\\%\\$\\#\\@\\&\\/\\.\\*])[\\ ]+[w]/[\\ a-z0-9][a-z0-9][a-z0-9\\.][a-zA-Z0-9\\n\\ \\%\\$\\-\\#\\@\\&\\/\\.\\'\\*\\(\\)\\,]*', '', s)\n",
    "    \n",
    "    if \" fits for \" not in s and \" for fits \" not in s:    \n",
    "        s = re.sub(r'(?<=[a-zA-Z0-9\\%\\$\\#\\@\\&\\/\\.\\*])[\\ ]+fits [a-zA-Z0-9\\n\\ \\%\\$\\-\\#\\@\\&\\/\\.\\'\\*\\(\\)\\,]*', '', s)    \n",
    "    if \" for lease \" not in s and re.search(r' (sign|decal[s]*|figure[s]*)(?=$)',s)==None:\n",
    "        s = re.sub(r'(?<=[a-zA-Z0-9\\%\\$\\#\\@\\&\\/\\.\\*])[\\ ]+[F|f]or [a-zA-Z0-9\\n\\ \\%\\$\\-\\#\\@\\&\\/\\.\\'\\*\\(\\)\\,]*', '', s)\n",
    "    s = re.sub(r'(?<=[a-zA-Z0-9\\%\\$\\#\\@\\&\\/\\.\\*])[\\ ]+[T|t]hat [a-zA-Z0-9\\n\\ \\%\\$\\-\\#\\@\\&\\/\\.\\'\\*\\(\\)\\,]*', '', s)\n",
    "\n",
    "\n",
    "    s = re.sub(r' on (wheels|a pallet|spool|bracket|3 in|blue post|360|track|spike|rock|lamp|11 in|2 in|pedestal|square base|tub|steel work)[a-zA-Z0-9\\n\\ \\%\\$\\-\\#\\@\\&\\/\\.\\'\\*\\(\\)]*', '', s)\n",
    "    s = re.sub(r' on (plinth|insulator|casters|pier base|reel|fireplace|moon|bracket|24p ebk|zinc spike|mailbox|cream chand|blue post)[a-zA-Z0-9\\n\\ \\%\\$\\-\\#\\@\\&\\/\\.\\'\\*\\(\\)]*', '', s)\n",
    " \n",
    "    s = re.sub(r'(?<= on white stiker)[a-zA-Z0-9\\n\\ \\%\\$\\-\\#\\@\\&\\/\\.\\'\\*\\(\\)\\,]*', '', s)\n",
    "    s = re.sub(r' on [installing][a-zA-Z0-9\\n\\ \\%\\$\\-\\#\\@\\&\\/\\.\\'\\*\\(\\)\\,]*', '', s)\n",
    "    if \",\" in s:\n",
    "        srch=re.search(r'(?<=, )[a-zA-Z0-9\\n\\ \\%\\$\\-\\#\\@\\&\\/\\.\\'\\*\\(\\)\\,]*',s)  #greedy regular expression\n",
    "        if srch!=None:\n",
    "            if len(re.sub('[^a-zA-Z\\ ]', '', srch.group(0)))<25:\n",
    "                s = re.sub(r', [a-zA-Z0-9\\n\\ \\%\\$\\-\\#\\@\\&\\/\\.\\'\\*\\(\\)\\,]*', '', s)\n",
    "    s = re.sub(r'(?<=recessed door reinforcer), [a-zA-Z0-9\\n\\ \\%\\$\\-\\#\\@\\&\\/\\.\\'\\*\\(\\)\\,]*', '', s)\n",
    "    #s = re.sub(r'(?<=[a-zA-Z0-9]),\\ [a-zA-Z0-9\\n\\ \\%\\$\\-\\#\\@\\&\\/\\.\\'\\*\\(\\)]*', '', s)\n",
    "    s = re.sub(r'(?<=[a-zA-Z\\%\\$\\#\\@\\&\\/\\.\\*]) [F|f]eaturing [a-zA-Z0-9\\n\\ \\%\\$\\-\\#\\@\\&\\/\\.\\'\\*\\(\\)]*', '', s)\n",
    "    s = re.sub(r'(?<=[a-zA-Z\\%\\$\\#\\@\\&\\/\\.\\*]) [I|i]ncludes [a-zA-Z0-9\\n\\ \\%\\$\\-\\#\\@\\&\\/\\.\\'\\*\\(\\)]*', '', s)\n",
    "    s = re.sub(' [\\#]\\d+[\\-\\d]*[\\,]*', '', s)  \n",
    "    s = re.sub(r'(?<=[a-zA-Z\\ ])\\/(?=[a-zA-Z])', ' ', s)\n",
    "    s = re.sub(r'(?<=[a-zA-Z\\ ])\\-(?=[a-zA-Z])', ' ', s)\n",
    "    s = s.replace(\",\",\". \")\n",
    "    s = s.replace(\"..\",\".\")\n",
    "    s = s.replace(\"..\",\".\")\n",
    "    s = s.replace(\"*\",\"\")\n",
    "    return \" \".join([word.replace(\"-\",\"\") for word in s.split() if re.search(r'\\d\\-\\d',word)==None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "not_keyword_list=list(brand_df['name'][brand_df['nwords']==1])\n",
    "for item in add_space_stop_list:\n",
    "    if len(wn.synsets(item,pos=wn.NOUN))==0:\n",
    "        not_keyword_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_key_words(tag_list,wordtag_list, string_output=False,out_list=not_keyword_list[:]):\n",
    "    i=len(tag_list)\n",
    "    in_list=['tv','downrod', 'sillcock', 'shelving', 'luminaire', 'paracord', 'ducting', \\\n",
    "    'recyclamat', 'rebar', 'spackling', 'hoodie', 'placemat', 'innoculant', 'protectant', \\\n",
    "    'colorant', 'penetrant', 'attractant', 'bibb', 'nosing', 'subflooring', 'torchiere', 'thhn',\\\n",
    "    'lantern','epoxy','cloth','trim','adhesive','light','lights','saw','pad','polish','nose','stove',\\\n",
    "    'elbow','elbows','lamp','door','doors','pipe','bulb','wood','woods','wire','sink','hose','tile','bath','table','duct',\\\n",
    "    'windows','mesh','rug','rugs','shower','showers','wheels','fan','lock','rod','mirror','cabinet','shelves','paint',\\\n",
    "    'plier','pliers','set','screw','lever','bathtub','vacuum','nut', 'nipple','straw','saddle','pouch','underlayment',\\\n",
    "    'shade','top', 'bulb', 'bulbs', 'paint', 'oven', 'ranges', 'sharpie', 'shed', 'faucet',\\\n",
    "    'finish','microwave', 'can', 'nozzle', 'grabber', 'tub', 'angles','showerhead', 'dehumidifier', 'shelving', 'urinal', 'mdf']\n",
    "\n",
    "    out_list= out_list +['free','height', 'width', 'depth', 'model','pcs', 'thick','pack','adhesive','steel','cordless', 'aaa' 'b', 'nm', 'hc', 'insulated','gll', 'nutmeg',\\\n",
    "    'pnl', 'sotc','withe','stainless','chrome','beige','max','acrylic', 'cognac', 'cherry', 'ivory','electric','fluorescent', 'recessed', 'matte',\\\n",
    "    'propane','sku','brushless','quartz','gfci','shut','sds','value','brown','white','black','red','green','yellow','blue','silver','pink',\\\n",
    "    'gray','gold','thw','medium','type','flush',\"metaliks\", 'metallic', 'amp','btu','gpf','pvc','mil','gcfi','plastic', 'vinyl','aaa',\\\n",
    "    'aluminum','brass','antique', 'brass','copper','nickel','satin','rubber','porcelain','hickory','marble','polyacrylic','golden','fiberglass',\\\n",
    "    'nylon','lmapy','maple','polyurethane','mahogany','enamel', 'enameled', 'linen','redwood', 'sku','oak','quart','abs','travertine', 'resin',\\\n",
    "    'birch','birchwood','zinc','pointe','polycarbonate', 'ash', 'wool', 'rockwool', 'teak','alder','frp','cellulose','abz', 'male', 'female', 'used',\\\n",
    "    'hepa','acc','keyless','aqg','arabesque','polyurethane', 'polyurethanes','ardex','armorguard','asb', 'motion','adorne','fatpack',\\\n",
    "    'fatmax','feet','ffgf','fgryblkg', 'douglas', 'fir', 'fleece','abba', 'nutri', 'thermal','thermoclear', 'heat', 'water', 'systemic',\\\n",
    "    'heatgasget', 'cool', 'fusion', 'awg', 'par', 'parabolic', 'tpi', 'pint', 'draining', 'rain', 'cost', 'costs', 'costa','ecostorage',\n",
    "    'mtd', 'pass', 'emt', 'jeld', 'npt', 'sch', 'pvc', 'dusk', 'dawn', 'lathe','lows','pressure', 'round', 'series','impact', 'resistant','outdoor',\\\n",
    "    'off', 'sawall', 'elephant', 'ear', 'abb', 'baby', 'feedback', 'fastback','jumbo', 'flexlock', 'instant', 'natol', 'naples','florcant',\\\n",
    "    'canna','hammock', 'jrc', 'honeysuckle', 'honey', 'serrano','sequoia', 'amass', 'ashford', 'gal','gas', 'gasoline', 'compane','occupancy',\\\n",
    "    'home','bakeware', 'lite', 'lithium', 'golith','gxwh',  'wht', 'heirloom', 'marine', 'marietta', 'cambria', 'campane','birmingham',\\\n",
    "    'bellingham','chamois', 'chamomile', 'chaosaw', 'chanpayne', 'thats', 'urethane', 'champion', 'chann', 'mocha', 'bay', 'rough',\\\n",
    "    'undermount', 'price', 'prices', 'way', 'air', 'bazaar', 'broadway', 'driveway', 'sprayway', 'subway', 'flood', 'slate', 'wet',\\\n",
    "    'clean', 'tweed', 'weed', 'cub', 'barb', 'salem', 'sale', 'sales', 'slip', 'slim', 'gang', 'office', 'allure', 'bronze', 'banbury',\\\n",
    "    'tuscan','tuscany', 'refinishing', 'fleam','schedule', 'doeskin','destiny', 'mean', 'hide', 'bobbex', 'pdi', 'dpdt', 'tri', 'order',\\\n",
    "    'kamado','seahawks','weymouth', 'summit','tel','riddex', 'alick','alvin', 'ano', 'assy', 'grade', 'barranco', 'batte','banbury',\\\n",
    "    'mcmaster', 'carr', 'ccl', 'china', 'choc', 'colle', 'cothom', 'cucbi', 'cuv', 'cwg', 'cylander', 'cylinoid', 'dcf', 'number', 'ultra',\\\n",
    "    'diat','discon', 'disconnect', 'plantation', 'dpt', 'duomo', 'dupioni', 'eglimgton', 'egnighter','ert','euroloft', 'everready',\\\n",
    "    'felxfx', 'financing', 'fitt', 'fosle', 'footage', 'gpf','fro', 'genis', 'giga', 'glu', 'gpxtpnrf', 'size', 'hacr', 'hardw',\\\n",
    "    'hexagon', 'hire', 'hoo','number','cosm', 'kelston', 'kind', 'all', 'semi', 'gloss', 'lmi', 'luana', 'gdak', 'natol', 'oatu',\\\n",
    "    'oval', 'olinol', 'pdi','penticlea', 'portalino', 'racc', 'rads', 'renat', 'roc', 'lon', 'sendero', 'adora', 'sleave', 'swu',\n",
    "    'tilde', 'cordoba', 'tuvpl','yel', 'acacia','mig','parties','alkaline','plexiglass', 'iii', 'watt']\n",
    "    \n",
    "    output_list=[]\n",
    "    if i>0:\n",
    "        finish=False\n",
    "        started = False\n",
    "        while not finish:\n",
    "            i-=1\n",
    "            \n",
    "            if started==False:\n",
    "                if (wordtag_list[i][0] not in out_list) \\\n",
    "                and (wordtag_list[i][0] in in_list \\\n",
    "                    or (re.search(r'(?=[e|o]r[s]*\\b)',wordtag_list[i][0])!=None and re.search(r'\\d+',wordtag_list[i][0])==None) \\\n",
    "                    or (len(wordtag_list[i][0])>2 and re.search(r'\\d+',wordtag_list[i][0])==None and len(wn.synsets(wordtag_list[i][0],pos=wn.NOUN))>0 \\\n",
    "                        and (wordtag_list[i][1] in ['NN', 'NNS','VBG'] or tag_list[i][1] in ['NN', 'NNS','VBG']) \\\n",
    "                            and len(re.sub('[^aeiouy]', '', wordtag_list[i][0]))>0 )): #exclude VBD\n",
    "                    started = True\n",
    "                    output_list.insert(0,wordtag_list[i])\n",
    "                        # handle exceptions below\n",
    "                        # 'iron' only with -ing is OK: soldering iron, seaming iron\n",
    "                    if i>1 and wordtag_list[i][0] in ['iron','irons'] and re.search(r'ing\\b',wordtag_list[i-1][0])==None:\n",
    "                        output_list=[]\n",
    "                        started = False    \n",
    "            else:\n",
    "\n",
    "                if tag_list[i][1] in ['NN','NNP', 'NNPS', 'NNS']:\n",
    "                    if len(re.sub('[^0-9]', '', tag_list[i][0]))==0 and \\\n",
    "                    len(re.sub('[^a-zA-Z0-9\\-]', '', tag_list[i][0]))>2 \\\n",
    "                    and tag_list[i][0] not in ['amp','btu','gpf','pvc','mil','watt','gcfi']\\\n",
    "                    and (len(wn.synsets(tag_list[i][0]))>0 or re.search(r'(?=[e|o]r[s]*\\b)',tag_list[i][0])!=None):\n",
    "                        output_list.insert(0,tag_list[i])\n",
    "                elif tag_list[i][0]=='and':\n",
    "                    output_list.insert(0,tag_list[i])\n",
    "                    started=False\n",
    "                else:\n",
    "                    if tag_list[max(0,i-1)][0]!=\"and\" and (tag_list[i][1] not in ['VBD', 'VBN']):\n",
    "                        finish=True\n",
    "                    if tag_list[i][1] in ['JJ','JJS', 'JJR', 'RB', 'RBS', 'RBR', 'VBG', 'VBD', 'VBN','VBP']:\n",
    "                        if len(re.sub('[^0-9]', '', tag_list[i][0]))==0 and \\\n",
    "                        len(re.sub('[^a-zA-Z0-9\\-]', '', tag_list[i][0]))>2 \\\n",
    "                        and tag_list[i][0] not in ['amp','btu','gpf','pvc','mil','watt','gcfi']\\\n",
    "                        and len(wn.synsets(tag_list[i][0]))>0:\n",
    "                            output_list.insert(0,tag_list[i])\n",
    "                \n",
    "            if i==0:\n",
    "                finish=True\n",
    "    if string_output==True:\n",
    "        return \" \".join([tag[0] for tag in output_list])\n",
    "    else:\n",
    "        return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['product_title_cut'] = All_df['product_title'].map(lambda x: cut_product_title(x))#.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0 out of 108307 unique cut product titles; 1.2 minutes\n",
      "processed 5000 out of 108307 unique cut product titles; 8.6 minutes\n",
      "processed 10000 out of 108307 unique cut product titles; 16.0 minutes\n",
      "processed 15000 out of 108307 unique cut product titles; 23.5 minutes\n",
      "processed 20000 out of 108307 unique cut product titles; 30.9 minutes\n",
      "processed 25000 out of 108307 unique cut product titles; 38.3 minutes\n",
      "processed 30000 out of 108307 unique cut product titles; 45.7 minutes\n",
      "processed 35000 out of 108307 unique cut product titles; 53.2 minutes\n",
      "processed 40000 out of 108307 unique cut product titles; 60.7 minutes\n",
      "processed 45000 out of 108307 unique cut product titles; 68.2 minutes\n",
      "processed 50000 out of 108307 unique cut product titles; 75.7 minutes\n",
      "processed 55000 out of 108307 unique cut product titles; 83.2 minutes\n",
      "processed 60000 out of 108307 unique cut product titles; 90.7 minutes\n",
      "processed 65000 out of 108307 unique cut product titles; 98.1 minutes\n",
      "processed 70000 out of 108307 unique cut product titles; 105.6 minutes\n",
      "processed 75000 out of 108307 unique cut product titles; 113.0 minutes\n",
      "processed 80000 out of 108307 unique cut product titles; 120.4 minutes\n",
      "processed 85000 out of 108307 unique cut product titles; 127.8 minutes\n",
      "processed 90000 out of 108307 unique cut product titles; 135.2 minutes\n",
      "processed 95000 out of 108307 unique cut product titles; 142.6 minutes\n",
      "processed 100000 out of 108307 unique cut product titles; 150.1 minutes\n",
      "processed 105000 out of 108307 unique cut product titles; 157.5 minutes\n",
      "extract brands from cut product title: 162.4 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### ... and that is why we have to remove the brand names again\n",
    "aa=list(set(list(All_df['product_title_cut'])))\n",
    "my_dict={}\n",
    "for i in range(0,len(aa)):\n",
    "    my_dict[aa[i]]=getremove_brand_or_material_from_str(aa[i],brand_df)\n",
    "    if (i % 5000)==0:\n",
    "        print(\"processed \"+str(i)+\" out of \"+str(len(aa))+\" unique cut product titles; \"+str(round((time()-t0)/60,1))+\" minutes\")\n",
    "All_df['product_title_cut_tuple'] = All_df['product_title_cut'].map(lambda x: my_dict[x])\n",
    "All_df['product_title_cut_woBrand'] = All_df['product_title_cut_tuple'].map(lambda x: x[0])\n",
    "All_df = All_df.drop(['product_title_cut_tuple'],axis=1)\n",
    "print('extract brands from cut product title:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagged 20000 out of 240761 total rows; 0.4 minutes\n",
      "tagged 40000 out of 240761 total rows; 0.7 minutes\n",
      "tagged 60000 out of 240761 total rows; 1.1 minutes\n",
      "tagged 80000 out of 240761 total rows; 1.4 minutes\n",
      "tagged 100000 out of 240761 total rows; 1.7 minutes\n",
      "tagged 120000 out of 240761 total rows; 2.1 minutes\n",
      "tagged 140000 out of 240761 total rows; 2.4 minutes\n",
      "tagged 160000 out of 240761 total rows; 2.8 minutes\n",
      "tagged 180000 out of 240761 total rows; 3.1 minutes\n",
      "tagged 200000 out of 240761 total rows; 3.5 minutes\n",
      "tagged 220000 out of 240761 total rows; 3.8 minutes\n",
      "tagged 240000 out of 240761 total rows; 4.2 minutes\n",
      "tagged 240761 out of 240761 total rows; 4.2 minutes\n",
      "wordtagged 20000 out of 240761 total rows; 0.4 minutes\n",
      "wordtagged 40000 out of 240761 total rows; 0.7 minutes\n",
      "wordtagged 60000 out of 240761 total rows; 1.1 minutes\n",
      "wordtagged 80000 out of 240761 total rows; 1.5 minutes\n",
      "wordtagged 100000 out of 240761 total rows; 1.8 minutes\n",
      "wordtagged 120000 out of 240761 total rows; 2.2 minutes\n",
      "wordtagged 140000 out of 240761 total rows; 2.6 minutes\n",
      "wordtagged 160000 out of 240761 total rows; 2.9 minutes\n",
      "wordtagged 180000 out of 240761 total rows; 3.3 minutes\n",
      "wordtagged 200000 out of 240761 total rows; 3.7 minutes\n",
      "wordtagged 220000 out of 240761 total rows; 4.0 minutes\n",
      "wordtagged 240000 out of 240761 total rows; 4.4 minutes\n",
      "wordtagged 240761 out of 240761 total rows; 4.4 minutes\n",
      "tagged 20000 out of 240761 total rows; 0.1 minutes\n",
      "tagged 40000 out of 240761 total rows; 0.2 minutes\n",
      "tagged 60000 out of 240761 total rows; 0.4 minutes\n",
      "tagged 80000 out of 240761 total rows; 0.5 minutes\n",
      "tagged 100000 out of 240761 total rows; 0.6 minutes\n",
      "tagged 120000 out of 240761 total rows; 0.7 minutes\n",
      "tagged 140000 out of 240761 total rows; 0.8 minutes\n",
      "tagged 160000 out of 240761 total rows; 0.9 minutes\n",
      "tagged 180000 out of 240761 total rows; 1.0 minutes\n",
      "tagged 200000 out of 240761 total rows; 1.2 minutes\n",
      "tagged 220000 out of 240761 total rows; 1.3 minutes\n",
      "tagged 240000 out of 240761 total rows; 1.4 minutes\n",
      "tagged 240761 out of 240761 total rows; 1.4 minutes\n",
      "wordtagged 20000 out of 240761 total rows; 0.1 minutes\n",
      "wordtagged 40000 out of 240761 total rows; 0.2 minutes\n",
      "wordtagged 60000 out of 240761 total rows; 0.4 minutes\n",
      "wordtagged 80000 out of 240761 total rows; 0.5 minutes\n",
      "wordtagged 100000 out of 240761 total rows; 0.6 minutes\n",
      "wordtagged 120000 out of 240761 total rows; 0.7 minutes\n",
      "wordtagged 140000 out of 240761 total rows; 0.8 minutes\n",
      "wordtagged 160000 out of 240761 total rows; 1.0 minutes\n",
      "wordtagged 180000 out of 240761 total rows; 1.1 minutes\n",
      "wordtagged 200000 out of 240761 total rows; 1.2 minutes\n",
      "wordtagged 220000 out of 240761 total rows; 1.3 minutes\n",
      "wordtagged 240000 out of 240761 total rows; 1.4 minutes\n",
      "wordtagged 240761 out of 240761 total rows; 1.4 minutes\n"
     ]
    }
   ],
   "source": [
    "### Tagging two times: full sentences and separate words\n",
    "All_df['product_title_cut_tokens'] =col_tagger(All_df['product_title_cut_woBrand'])\n",
    "All_df['product_title_cut_wordtokens'] =col_wordtagger(All_df['product_title_cut_woBrand'])\n",
    "\n",
    "### the same steps for search term, but we now we continue with the preprocessed resuts\n",
    "### since punctuation is not as important in query as it is in product title\n",
    "All_df['search_term_cut_woBrand']= All_df['search_term_parsed_woBrand'].map(lambda x: cut_product_title(x))#.encode('utf-8'))\n",
    "All_df['search_term_cut_tokens'] =col_tagger(All_df['search_term_cut_woBrand'])\n",
    "All_df['search_term_cut_wordtokens'] =col_wordtagger(All_df['search_term_cut_woBrand'])\n",
    "\n",
    "### Transform tags into text, it may look like unecessary step.\n",
    "### But in our work we have to frequently save processing results and recover tags from text.\n",
    "### Here this transformation is used to make the _tokens variables compatibe with \n",
    "### parser_mystr2tuple() function \n",
    "All_df['search_term_cut_tokens']=All_df['search_term_cut_tokens'].map(lambda x: str(x))\n",
    "All_df['search_term_cut_wordtokens']=All_df['search_term_cut_wordtokens'].map(lambda x: str(x))\n",
    "All_df['product_title_cut_tokens']=All_df['product_title_cut_tokens'].map(lambda x: str(x))\n",
    "All_df['product_title_cut_wordtokens']=All_df['product_title_cut_wordtokens'].map(lambda x: str(x))\n",
    "\n",
    "\n",
    "All_df['search_term_keys']=All_df.apply(lambda x: \\\n",
    "            get_key_words(parser_mystr2tuple(x['search_term_cut_tokens']),parser_mystr2tuple(x['search_term_cut_wordtokens']),string_output=True),axis=1)\n",
    "All_df['product_title_keys']=All_df.apply(lambda x: \\\n",
    "            get_key_words(parser_mystr2tuple(x['product_title_cut_tokens']),parser_mystr2tuple(x['product_title_cut_wordtokens']),string_output=True),axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_last_words_from_parsed_title(s):\n",
    "    words=s.split()\n",
    "    if len(words)==0:\n",
    "        last_word=\"\"\n",
    "        word_before_last=\"\"\n",
    "        word2_before_last=\"\"\n",
    "    else:\n",
    "        last_word=words[len(words)-1]\n",
    "        word_before_last=\"\"\n",
    "        word2_before_last=\"\"\n",
    "        if len(words)>1:\n",
    "            word_before_last=words[len(words)-2]\n",
    "            if word_before_last==\"and\":\n",
    "                word_before_last=\"\"\n",
    "            if len(words)>2 and word_before_last!=\"and\":\n",
    "                word2_before_last=words[len(words)-3]\n",
    "                if word2_before_last==\"and\":\n",
    "                    word2_before_last=\"\"\n",
    "    return last_word, word_before_last, word2_before_last\n",
    "\n",
    "def get_last_words_from_parsed_query(s,last_word_in_title):\n",
    "    words=s.split()\n",
    "    if len(words)==0:\n",
    "        last_word=\"\"\n",
    "        word_before_last=\"\"\n",
    "        word2_before_last=\"\"\n",
    "    else:\n",
    "        last_word=words[len(words)-1]\n",
    "        word_before_last=\"\"\n",
    "        word2_before_last=\"\"\n",
    "        if len(words)>1:\n",
    "            word_before_last=words[len(words)-2]\n",
    "            \n",
    "            if len(words)>2 and word_before_last!=\"and\":\n",
    "                word2_before_last=words[len(words)-3]\n",
    "                if word2_before_last==\"and\":\n",
    "                    word2_before_last=\"\"\n",
    "                    \n",
    "            if word_before_last==\"and\":\n",
    "                word_before_last=\"\"\n",
    "                if len(words)>2:\n",
    "                    cmp_word=words[len(words)-3]\n",
    "                    sm1=find_similarity(last_word,last_word_in_title)[0]\n",
    "                    sm2=find_similarity(cmp_word,last_word_in_title)[0]\n",
    "                    if sm1<sm2:\n",
    "                        last_word=cmp_word\n",
    "                        if len(words)>3:\n",
    "                            word_before_last=words[len(words)-4]\n",
    "               \n",
    "            \n",
    "    return last_word, word_before_last, word2_before_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting important words time: 53.7 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### get trigram from product title\n",
    "All_df['product_title_thekey_tuple']=All_df['product_title_keys'].map(lambda x: get_last_words_from_parsed_title(x))\n",
    "All_df['product_title_thekey']=All_df['product_title_thekey_tuple'].map(lambda x: x[0])\n",
    "All_df['product_title_beforethekey']=All_df['product_title_thekey_tuple'].map(lambda x: x[1])\n",
    "All_df['product_title_before2thekey']=All_df['product_title_thekey_tuple'].map(lambda x: x[2])\n",
    "All_df=All_df.drop(['product_title_thekey_tuple'],axis=1)\n",
    "\n",
    "\n",
    "### get trigram from query\n",
    "All_df['search_term_thekey_tuple']=All_df.apply(lambda x: \\\n",
    "            get_last_words_from_parsed_query(x['search_term_keys'],x['product_title_thekey']),axis=1)\n",
    "#All_df['thekey_info']=All_df['search_term_keys']+\"\\t\"+All_df['product_title_thekey']\n",
    "#All_df['search_term_thekey_tuple']=All_df['thekey_info'].map(lambda x: get_last_words_from_parsed_query(x.split(\"\\t\")[0],x.split(\"\\t\")[1]))\n",
    "All_df['search_term_thekey']=All_df['search_term_thekey_tuple'].map(lambda x: x[0])\n",
    "All_df['search_term_beforethekey']=All_df['search_term_thekey_tuple'].map(lambda x: x[1])\n",
    "All_df['search_term_before2thekey']=All_df['search_term_thekey_tuple'].map(lambda x: x[2])\n",
    "All_df=All_df.drop(['search_term_thekey_tuple'],axis=1)\n",
    "\n",
    "print('extracting important words time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['attribute_bullets_stemmed']=All_df['attribute_bullets_parsed'].map(lambda x:str_stemmer_wo_parser(x))\n",
    "All_df['attribute_bullets_stemmed_woBM']=All_df['attribute_bullets_parsed_woBM'].map(lambda x:str_stemmer_wo_parser(x))\n",
    "All_df['attribute_bullets_stemmed_woBrand']=All_df['attribute_bullets_parsed_woBrand'].map(lambda x:str_stemmer_wo_parser(x))\n",
    "pro_descr['product_description_stemmed']=pro_descr['product_description_parsed'].map(lambda x:str_stemmer_wo_parser(x))\n",
    "pro_descr['product_description_stefxmmed_woBM']=pro_descr['product_description_parsed_woBM'].map(lambda x:str_stemmer_wo_parser(x))\n",
    "pro_descr['product_description_stemmed_woBrand']=pro_descr['product_description_parsed_woBrand'].map(lambda x:str_stemmer_wo_parser(x))\n",
    "All_df['search_term_keys_stemmed']=All_df['search_term_keys'].map(lambda x: str_stemmer_wo_parser(x,stoplist=stoplist_wo_can))\n",
    "All_df['product_title_keys_stemmed']=All_df['product_title_keys'].map(lambda x: str_stemmer_wo_parser(x,stoplist=stoplist_wo_can))\n",
    "All_df['search_term_stemmed']=All_df['search_term_parsed'].map(lambda x:str_stemmer_wo_parser(x,stoplist=stoplist_wo_can))\n",
    "All_df['search_term_stemmed_woBM']=All_df['search_term_parsed_woBM'].map(lambda x:str_stemmer_wo_parser(x,stoplist=stoplist_wo_can))\n",
    "All_df['search_term_stemmed_woBrand']=All_df['search_term_parsed_woBrand'].map(lambda x:str_stemmer_wo_parser(x,stoplist=stoplist_wo_can))\n",
    "All_df['product_title_stemmed']=All_df['product_title_parsed'].map(lambda x:str_stemmer_wo_parser(x,stoplist=stoplist_wo_can))\n",
    "All_df['product_title_stemmed_woBM']=All_df['product_title_parsed_woBM'].map(lambda x:str_stemmer_wo_parser(x,stoplist=stoplist_wo_can))\n",
    "All_df['product_title_stemmed_woBrand']=All_df['product_title_parsed_woBrand'].map(lambda x:str_stemmer_wo_parser(x,stoplist=stoplist_wo_can))\n",
    "All_df['search_term_thekey_stemmed']=All_df['search_term_thekey'].map(lambda x: str_stemmer_wo_parser(x,stoplist=stoplist_wo_can))\n",
    "All_df['product_title_thekey_stemmed']=All_df['product_title_thekey'].map(lambda x: str_stemmer_wo_parser(x,stoplist=stoplist_wo_can))\n",
    "All_df['search_term_beforethekey_stemmed']=All_df['search_term_beforethekey'].map(lambda x: str_stemmer_wo_parser(x,stoplist=stoplist_wo_can))\n",
    "All_df['product_title_beforethekey_stemmed']=All_df['product_title_beforethekey'].map(lambda x: str_stemmer_wo_parser(x,stoplist=stoplist_wo_can))\n",
    "All_df['search_term_before2thekey_stemmed']=All_df['search_term_before2thekey'].map(lambda x: str_stemmer_wo_parser(x,stoplist=stoplist_wo_can))\n",
    "All_df['product_title_before2thekey_stemmed']=All_df['product_title_before2thekey'].map(lambda x: str_stemmer_wo_parser(x,stoplist=stoplist_wo_can))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pro_descr = pro_descr.drop('product_description',axis=1)\n",
    "All_df = pd.merge(All_df,pro_descr,how='left',on='product_uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['product_description_stemmed']=All_df['product_description_parsed'].map(lambda x:str_stemmer_wo_parser(x))\n",
    "All_df['product_description_stefxmmed_woBM']=All_df['product_description_parsed_woBM'].map(lambda x:str_stemmer_wo_parser(x))\n",
    "All_df['product_description_stemmed_woBrand']=All_df['product_description_parsed_woBrand'].map(lambda x:str_stemmer_wo_parser(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for var in All_df.keys():\n",
    "    All_df[var]=All_df[var].fillna(\"\") \n",
    "    \n",
    "    \n",
    "def extract_after_word(s,word):\n",
    "    output=\"\"\n",
    "    if word in s:\n",
    "        srch= re.search(r'(?<=\\b'+word+'\\ )[a-zA-Z0-9\\n\\ \\%\\$\\-\\#\\@\\&\\/\\.\\'\\*\\(\\)\\,]+',s)\n",
    "        if srch!=None:\n",
    "            output=srch.group(0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['search_term_for']=All_df['search_term_parsed'].map(lambda x: extract_after_word(x,'for'))\n",
    "All_df['search_term_for_stemmed']=All_df['search_term_for'].map(lambda x:str_stemmer_wo_parser(x,stoplist=stoplist_wo_can))\n",
    "\n",
    "All_df['search_term_with']=All_df['search_term_parsed'].map(lambda x: extract_after_word(x,'with'))\n",
    "All_df['search_term_with_stemmed']=All_df['search_term_with'].map(lambda x:str_stemmer_wo_parser(x,stoplist=stoplist_wo_can))\n",
    "\n",
    "All_df['product_title_parsed_without']=All_df['product_title_parsed'].map(lambda x: extract_after_word(x,'without'))\n",
    "All_df['product_title_without_stemmed']=All_df['product_title_parsed_without'].map(lambda x:str_stemmer_wo_parser(x,stoplist=stoplist_wo_can))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### save the list of string variables which are not features\n",
    "string_variables_list=list(All_df.keys())\n",
    "print(str(len(string_variables_list) ) + \" total variables...\")\n",
    "string_variables_list.remove('id')\n",
    "string_variables_list.remove('product_uid')\n",
    "string_variables_list.remove('relevance')\n",
    "string_variables_list.remove('is_query_misspelled')\n",
    "print(\"including \"+ str(len(string_variables_list) ) + \" text variables to drop later\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### STEP 1: Dummy for no attributes and empty attribute bullets\n",
    "#################################################################\n",
    "All_df['has_attributes_dummy']=1\n",
    "All_df['has_attributes_dummy']= All_df['has_attributes_dummy'].fillna(0)\n",
    "All_df['no_bullets_dummy'] = All_df['Bullet'].map(lambda x:int(len(x)==0))\n",
    "All_df['is_replaced_using_google_dict']=All_df['search_term'].map(lambda x: 1 if x in google_dict.keys() else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### STEP 2: Basic text features\n",
    "#################################################################\n",
    "def sentence_statistics(s):\n",
    "    s= re.sub('[^a-zA-Z0-9\\ \\%\\$\\-]', '', s)\n",
    "    word_list=s.split()\n",
    "    meaningful_word_list=[word for word in s.split() if len(re.findall(r'\\d+', word))==0 and len(wn.synsets(word))>0]\n",
    "    vowels=sum([len(re.sub('[^aeiou]', '', word)) for word in word_list])\n",
    "    letters = sum([len(word) for word in word_list])\n",
    "    \n",
    "    return len(word_list), len(meaningful_word_list), 1.0*sum([len(word) for word in word_list])/len(word_list), 1.0*vowels/letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['query_sentence_stats_tuple'] = All_df['search_term_parsed'].map(lambda x:  sentence_statistics(x) )\n",
    "All_df['len_of_meaningful_words_in_query'] = All_df['query_sentence_stats_tuple'].map(lambda x:  x[1] )\n",
    "All_df['ratio_of_meaningful_words_in_query'] = All_df['query_sentence_stats_tuple'].map(lambda x:  1.0*x[1]/x[0] )\n",
    "All_df['avg_wordlength_in_query'] = All_df['query_sentence_stats_tuple'].map(lambda x:  x[2] )\n",
    "All_df['ratio_vowels_in_query'] = All_df['query_sentence_stats_tuple'].map(lambda x:  x[3] )\n",
    "All_df=All_df.drop(['query_sentence_stats_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['initial_len_of_query'] = All_df['search_term_stemmed'].map(lambda x:len(x.split()))#.astype(np.int64)\n",
    "All_df['len_of_query_woBM'] = All_df['search_term_stemmed_woBM'].map(lambda x:len(x.split()))#.astype(np.int64)\n",
    "\n",
    "All_df['len_of_query_for'] = All_df['search_term_for_stemmed'].map(lambda x:len(words_wo_digits(x, minLength=1).split()))#.astype(np.int64)\n",
    "All_df['len_of_query_with'] = All_df['search_term_with_stemmed'].map(lambda x:len(words_wo_digits(x, minLength=1).split()))#.astype(np.int64)\n",
    "All_df['len_of_prtitle_without'] = All_df['product_title_without_stemmed'].map(lambda x:len(words_wo_digits(x, minLength=1).split()))#.astype(np.int64)\n",
    "\n",
    "All_df['len_of_query_string_only_woBM'] = All_df['search_term_stemmed_woBM'].map(lambda x:len(words_wo_digits(x, minLength=1).split()))#.astype(np.int64)\n",
    "\n",
    "All_df['len_of_query_w_dash_woBM'] = All_df['search_term_stemmed_woBM'].map(lambda x:len(words_w_dash(x).split()))#.astype(np.int64)\n",
    "All_df['len_of_title_w_dash_woBM'] = All_df['product_title_stemmed_woBM'].map(lambda x:len(words_w_dash(x).split()))#.astype(np.int64)\n",
    "\n",
    "All_df['len_of_product_title_woBM'] = All_df['product_title_stemmed_woBM'].map(lambda x:len(x.split()))#.astype(np.int64)\n",
    "################# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! ###############\n",
    "All_df['len_of_product_description_woBM'] = All_df['product_description_stefxmmed_woBM'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "#############################################################################\n",
    "All_df['len_of_attribute_bullets_woBM'] = All_df['attribute_bullets_stemmed_woBM'].map(lambda x:len(x.split()))#.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['len_of_brands_in_query'] = All_df['brands_in_search_term'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "All_df['size_of_brands_in_query'] = All_df['brands_in_search_term'].map(lambda x:len(x.split(\";\"))).astype(np.int64)\n",
    "All_df['len_of_materials_in_query'] = All_df['materials_in_search_term'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "All_df['size_of_materials_in_query'] = All_df['materials_in_search_term'].map(lambda x:len(x.split(\";\"))).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['len_of_brands_in_product_title'] = All_df['brands_in_product_title'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "All_df['size_of_brands_in_product_title'] = All_df['brands_in_product_title'].map(lambda x:len(x.split(\";\"))).astype(np.int64)\n",
    "All_df['len_of_materials_in_product_title'] = All_df['materials_in_product_title'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "All_df['size_of_materials_in_product_title'] = All_df['materials_in_product_title'].map(lambda x:len(x.split(\";\"))).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['len_of_brands_in_product_description'] = All_df['brands_in_product_description'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "All_df['size_of_brands_in_product_description'] = All_df['brands_in_product_description'].map(lambda x:len(x.split(\";\"))).astype(np.int64)\n",
    "All_df['len_of_materials_in_product_description'] = All_df['materials_in_product_description'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "All_df['size_of_materials_in_product_description'] = All_df['materials_in_product_description'].map(lambda x:len(x.split(\";\"))).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['len_of_brands_in_attribute_bullets'] = All_df['brands_in_attribute_bullets'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "All_df['size_of_brands_in_attribute_bullets'] = All_df['brands_in_attribute_bullets'].map(lambda x:len(x.split(\";\"))).astype(np.int64)\n",
    "All_df['len_of_materials_in_attribute_bullets'] = All_df['materials_in_attribute_bullets'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "All_df['size_of_materials_in_attribute_bullets'] = All_df['materials_in_attribute_bullets'].map(lambda x:len(x.split(\";\"))).astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['len_of_query']=All_df['len_of_query_woBM']+All_df['size_of_brands_in_query']+All_df['size_of_materials_in_query']\n",
    "All_df['len_of_product_title']=All_df['len_of_product_title_woBM']+All_df['size_of_brands_in_product_title']\n",
    "All_df['len_of_product_description']=All_df['len_of_product_title_woBM']+All_df['size_of_brands_in_product_description']+All_df['size_of_materials_in_product_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['len_of_attribute_bullets']=All_df['len_of_attribute_bullets_woBM']+All_df['size_of_brands_in_attribute_bullets']+All_df['size_of_materials_in_attribute_bullets']\n",
    "\n",
    "All_df['len_of_query_keys'] = All_df['search_term_keys_stemmed'].map(lambda x:len(x.split()))#.astype(np.int64)\n",
    "All_df['len_of_product_title_keys'] = All_df['product_title_keys_stemmed'].map(lambda x:len(x.split()))#.astype(np.int64)\n",
    "All_df['len_of_query_thekey'] = All_df['search_term_thekey'].map(lambda x:len(x.split()))#.astype(np.int64)\n",
    "All_df['len_of_product_title_thekey'] = All_df['product_title_thekey'].map(lambda x:len(x.split()))#.astype(np.int64)\n",
    "All_df['len_of_query_beforethekey'] = All_df['search_term_beforethekey'].map(lambda x:len(x.split()))#.astype(np.int64)\n",
    "All_df['len_of_product_title_beforethekey'] = All_df['product_title_beforethekey'].map(lambda x:len(x.split()))#.astype(np.int64)\n",
    "All_df['len_of_query_before2thekey'] = All_df['search_term_before2thekey'].map(lambda x:len(x.split()))#.astype(np.int64)\n",
    "All_df['len_of_product_title_before2thekey'] = All_df['product_title_before2thekey'].map(lambda x:len(x.split()))#.astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_ratio(a,b):\n",
    "    if b==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return min(1,1.0*a/b)\n",
    "\n",
    "\n",
    "def perc_digits_in_str(s):\n",
    "    if len(s.split())==0:\n",
    "        output=0\n",
    "    else:\n",
    "        output =1.0 -1.0*len(words_wo_digits(s, minLength=1).split()) / len(s.split())\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['attribute_bullets_parsed'] = All_df['attribute_bullets_parsed'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_of_something time: 91.7 minutes\n",
      "\n",
      "perc_digits time: 1.3 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column_name in ['search_term', 'product_title', 'product_description','attribute_bullets']:\n",
    "    All_df['ratio_of_nn_important_in_'+column_name]= All_df.apply(lambda x: \\\n",
    "        find_ratio(len(nn_important_words(x['search_term_tokens']).split()), len(x[column_name+'_parsed'])) ,axis=1)\n",
    "    All_df['ratio_of_nn_unimportant_in_'+column_name]= All_df.apply(lambda x: \\\n",
    "        find_ratio(len(nn_unimportant_words(x['search_term_tokens']).split()), len(x[column_name+'_parsed'])) ,axis=1)\n",
    "    All_df['ratio_of_vb_in_'+column_name]= All_df.apply(lambda x: \\\n",
    "        find_ratio(len(vb_words(x['search_term_tokens']).split()), len(x[column_name+'_parsed'])) ,axis=1)\n",
    "    All_df['ratio_of_vbg_in_'+column_name]= All_df.apply(lambda x: \\\n",
    "        find_ratio(len(vbg_words(x['search_term_tokens']).split()), len(x[column_name+'_parsed'])) ,axis=1)\n",
    "    All_df['ratio_of_jj_rb_in_'+column_name]= All_df.apply(lambda x: \\\n",
    "        find_ratio(len(nn_unimportant_words(x['search_term_tokens']).split()), len(x[column_name+'_parsed'])) ,axis=1)\n",
    "    \n",
    "        \n",
    "print('len_of_something time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()\n",
    "\n",
    "All_df['perc_digits_in_query'] = All_df['search_term_stemmed'].map(lambda x: perc_digits_in_str(x)  )\n",
    "All_df['perc_digits_in_title'] = All_df['product_title_stemmed'].map(lambda x: perc_digits_in_str(x)  )\n",
    "All_df['perc_digits_in_description'] = All_df['product_description_stemmed'].map(lambda x: perc_digits_in_str(x) )\n",
    "All_df['perc_digits_in_bullets'] = All_df['attribute_bullets_stemmed'].map(lambda x: perc_digits_in_str(x) )\n",
    "\n",
    "print('perc_digits time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_brand_material_in_attribute(str_query_brands,str_attribute_brands):\n",
    "    list_query_brands=list(set(str_query_brands.split(\";\")))\n",
    "    list_attribute_brands=list(set(str_attribute_brands.split(\";\")))\n",
    "    while '' in list_query_brands:\n",
    "        list_query_brands.remove('')\n",
    "    while '' in list_attribute_brands:\n",
    "        list_attribute_brands.remove('')\n",
    "    \n",
    "    str_attribute_brands=\" \".join(str_attribute_brands.split(\";\"))    \n",
    "    full_match=0\n",
    "    partial_match=0\n",
    "    assumed_match=0\n",
    "    no_match=0\n",
    "    num_of_query_brands=len(list_query_brands)\n",
    "    num_of_attribute_brands=len(list_attribute_brands)\n",
    "    if num_of_query_brands>0:\n",
    "        for brand in list_query_brands:\n",
    "            if brand in list_attribute_brands:\n",
    "                full_match+=1\n",
    "            elif ' '+brand+' ' in ' '+str_attribute_brands+' ':\n",
    "                partial_match+=1\n",
    "            elif (' '+brand.split()[0] in ' '+str_attribute_brands and brand.split()[0][0] not in \"0123456789\") or \\\n",
    "            (len(brand.split())>1 and (' '+brand.split()[0]+' '+brand.split()[1]) in ' '+str_attribute_brands):\n",
    "                assumed_match+=1\n",
    "            else:\n",
    "                no_match+=1\n",
    "                \n",
    "    convoluted_output=0 # no brand in query\n",
    "    if num_of_query_brands>0:\n",
    "        if num_of_attribute_brands==0:\n",
    "            convoluted_output = -1 # no brand in text, but there is brand in query\n",
    "        elif no_match==0:\n",
    "            if assumed_match==0:\n",
    "                convoluted_output=3 # all brands fully matched\n",
    "            else:\n",
    "                convoluted_output=2 # all brands matched at least partially\n",
    "        else:\n",
    "            if full_match+ partial_match+ assumed_match>0:\n",
    "                convoluted_output = 1 # one brand matched but the other is not\n",
    "            else:\n",
    "                convoluted_output= -2  #brand mismatched\n",
    "    return full_match, partial_match, assumed_match, no_match, convoluted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### brand \n",
    "All_df['brands_all']=All_df['brands_in_search_term']+\"\\t\"+All_df['brands_in_product_title']+\"\\t\"+All_df['brands_in_product_description']+\"\\t\"\\\n",
    "+All_df['brands_in_attribute_bullets']+\"\\t\"+All_df['brand_parsed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['query_brand_in_brand_tuple']=All_df['brands_all'].map(lambda x: query_brand_material_in_attribute(x.split(\"\\t\")[0],x.split(\"\\t\")[4]))\n",
    "All_df['query_brand_in_brand_fullmatch']=All_df['query_brand_in_brand_tuple'].map(lambda x: x[0])\n",
    "All_df['query_brand_in_brand_partialmatch']=All_df['query_brand_in_brand_tuple'].map(lambda x: x[1])\n",
    "All_df['query_brand_in_brand_assumedmatch']=All_df['query_brand_in_brand_tuple'].map(lambda x: x[2])\n",
    "All_df['query_brand_in_brand_nomatch']=All_df['query_brand_in_brand_tuple'].map(lambda x: x[3])\n",
    "All_df['query_brand_in_brand_convoluted']=All_df['query_brand_in_brand_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['query_brand_in_brand_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['query_brand_in_all_tuple']=All_df['brands_all'].map(lambda x: query_brand_material_in_attribute(x.split(\"\\t\")[0],\";\".join(x.split(\"\\t\")[1:])))\n",
    "All_df['query_brand_in_all_fullmatch']=All_df['query_brand_in_all_tuple'].map(lambda x: x[0])\n",
    "All_df['query_brand_in_all_partialmatch']=All_df['query_brand_in_all_tuple'].map(lambda x: x[1])\n",
    "All_df['query_brand_in_all_assumedmatch']=All_df['query_brand_in_all_tuple'].map(lambda x: x[2])\n",
    "All_df['query_brand_in_all_nomatch']=All_df['query_brand_in_all_tuple'].map(lambda x: x[3])\n",
    "All_df['query_brand_in_all_convoluted']=All_df['query_brand_in_all_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['query_brand_in_all_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['query_brand_in_title_tuple']=All_df['brands_all'].map(lambda x: query_brand_material_in_attribute(x.split(\"\\t\")[0],x.split(\"\\t\")[1]))\n",
    "All_df['query_brand_in_title_fullmatch']=All_df['query_brand_in_title_tuple'].map(lambda x: x[0])\n",
    "All_df['query_brand_in_title_partialmatch']=All_df['query_brand_in_title_tuple'].map(lambda x: x[1])\n",
    "All_df['query_brand_in_title_assumedmatch']=All_df['query_brand_in_title_tuple'].map(lambda x: x[2])\n",
    "All_df['query_brand_in_title_nomatch']=All_df['query_brand_in_title_tuple'].map(lambda x: x[3])\n",
    "All_df['query_brand_in_title_convoluted']=All_df['query_brand_in_title_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['query_brand_in_title_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create brand match variables time: 4.0 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "All_df['query_brand_in_description_tuple']=All_df['brands_all'].map(lambda x: query_brand_material_in_attribute(x.split(\"\\t\")[0],x.split(\"\\t\")[2]))\n",
    "All_df['query_brand_in_description_convoluted']=All_df['query_brand_in_description_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['query_brand_in_description_tuple'],axis=1)\n",
    "\n",
    "All_df['query_brand_in_bullets_tuple']=All_df['brands_all'].map(lambda x: query_brand_material_in_attribute(x.split(\"\\t\")[0],x.split(\"\\t\")[3]))\n",
    "All_df['query_brand_in_bullets_convoluted']=All_df['query_brand_in_bullets_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['query_brand_in_bullets_tuple'],axis=1)\n",
    "\n",
    "All_df=All_df.drop(['brands_all'],axis=1)\n",
    "\n",
    "print('create brand match variables time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### material\n",
    "All_df['materials_all']=All_df['materials_in_search_term']+\"\\t\"+All_df['materials_in_product_title']+\"\\t\"+\\\n",
    "All_df['materials_in_product_description']+\"\\t\"+All_df['materials_in_attribute_bullets']+\"\\t\"+All_df['material_parsed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['query_material_in_material_tuple']=All_df['materials_all'].map(lambda x: query_brand_material_in_attribute(x.split(\"\\t\")[0],x.split(\"\\t\")[4]))\n",
    "All_df['query_material_in_material_fullmatch']=All_df['query_material_in_material_tuple'].map(lambda x: x[0])\n",
    "All_df['query_material_in_material_partialmatch']=All_df['query_material_in_material_tuple'].map(lambda x: x[1])\n",
    "All_df['query_material_in_material_assumedmatch']=All_df['query_material_in_material_tuple'].map(lambda x: x[2])\n",
    "All_df['query_material_in_material_nomatch']=All_df['query_material_in_material_tuple'].map(lambda x: x[3])\n",
    "All_df['query_material_in_material_convoluted']=All_df['query_material_in_material_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['query_material_in_material_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['query_material_in_all_tuple']=All_df['materials_all'].map(lambda x: query_brand_material_in_attribute(x.split(\"\\t\")[0],\";\".join(x.split(\"\\t\")[1:])))\n",
    "All_df['query_material_in_all_fullmatch']=All_df['query_material_in_all_tuple'].map(lambda x: x[0])\n",
    "All_df['query_material_in_all_partialmatch']=All_df['query_material_in_all_tuple'].map(lambda x: x[1])\n",
    "All_df['query_material_in_all_assumedmatch']=All_df['query_material_in_all_tuple'].map(lambda x: x[2])\n",
    "All_df['query_material_in_all_nomatch']=All_df['query_material_in_all_tuple'].map(lambda x: x[3])\n",
    "All_df['query_material_in_all_convoluted']=All_df['query_material_in_all_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['query_material_in_all_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create material match variables time: 0.2 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "All_df['query_material_in_title_tuple']=All_df['materials_all'].map(lambda x: query_brand_material_in_attribute(x.split(\"\\t\")[0],x.split(\"\\t\")[1]))\n",
    "All_df['query_material_in_title_convoluted']=All_df['query_material_in_title_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['query_material_in_title_tuple'],axis=1)\n",
    "\n",
    "All_df['query_material_in_description_tuple']=All_df['materials_all'].map(lambda x: query_brand_material_in_attribute(x.split(\"\\t\")[0],x.split(\"\\t\")[2]))\n",
    "All_df['query_material_in_description_convoluted']=All_df['query_material_in_description_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['query_material_in_description_tuple'],axis=1)\n",
    "\n",
    "All_df['query_material_in_bullets_tuple']=All_df['materials_all'].map(lambda x: query_brand_material_in_attribute(x.split(\"\\t\")[0],x.split(\"\\t\")[3]))\n",
    "All_df['query_material_in_bullets_convoluted']=All_df['query_material_in_bullets_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['query_material_in_bullets_tuple'],axis=1)\n",
    "\n",
    "All_df=All_df.drop(['materials_all'],axis=1)\n",
    "\n",
    "print('create material match variables time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Count number of similar words/letters/bigrams etc\n",
    "### query vs product title\n",
    "All_df['wordFor_in_title_string_only_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(x['search_term_for_stemmed'],x['product_title_stemmed'],string_only=True),axis=1)\n",
    "All_df['wordFor_in_title_string_only_num'] = All_df['wordFor_in_title_string_only_tuple'].map(lambda x: x[0])\n",
    "All_df['wordFor_in_title_string_only_let'] = All_df['wordFor_in_title_string_only_tuple'].map(lambda x: x[2])\n",
    "All_df['wordFor_in_title_string_only_letratio'] = All_df['wordFor_in_title_string_only_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['wordFor_in_title_string_only_tuple'],axis=1)\n",
    "\n",
    "All_df['wordWith_in_title_string_only_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(x['search_term_with_stemmed'],x['product_title_stemmed'],string_only=True),axis=1)\n",
    "All_df['wordWith_in_title_string_only_num'] = All_df['wordWith_in_title_string_only_tuple'].map(lambda x: x[0])\n",
    "All_df['wordWith_in_title_string_only_let'] = All_df['wordWith_in_title_string_only_tuple'].map(lambda x: x[2])\n",
    "All_df['wordWith_in_title_string_only_letratio'] = All_df['wordWith_in_title_string_only_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['wordWith_in_title_string_only_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['prtitleWithout_in_query_string_only_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(x['product_title_without_stemmed'],x['search_term_stemmed'],string_only=True),axis=1)\n",
    "All_df['prtitleWithout_in_query_string_only_num'] = All_df['prtitleWithout_in_query_string_only_tuple'].map(lambda x: x[0])\n",
    "All_df['prtitleWithout_in_query_string_only_let'] = All_df['prtitleWithout_in_query_string_only_tuple'].map(lambda x: x[2])\n",
    "All_df['prtitleWithout_in_query_string_only_letratio'] = All_df['prtitleWithout_in_query_string_only_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['prtitleWithout_in_query_string_only_tuple'],axis=1)\n",
    "\n",
    "All_df['query_in_title']=All_df.apply(lambda x: \\\n",
    "            query_in_text(x['search_term_stemmed'],x['product_title_stemmed']),axis=1)\n",
    "\n",
    "All_df['word_in_title_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(x['search_term_stemmed'],x['product_title_stemmed']),axis=1)\n",
    "All_df['word_in_title_num'] = All_df['word_in_title_tuple'].map(lambda x: x[0])\n",
    "All_df['word_in_title_sum'] = All_df['word_in_title_tuple'].map(lambda x: x[1])\n",
    "All_df['word_in_title_let'] = All_df['word_in_title_tuple'].map(lambda x: x[2])\n",
    "All_df['word_in_title_numratio'] = All_df['word_in_title_tuple'].map(lambda x: x[3])\n",
    "All_df['word_in_title_letratio'] = All_df['word_in_title_tuple'].map(lambda x: x[4])\n",
    "All_df['word_in_title_string'] = All_df['word_in_title_tuple'].map(lambda x: x[5])\n",
    "All_df=All_df.drop(['word_in_title_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['word_in_title_string_only_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(x['search_term_stemmed'],x['product_title_stemmed'],string_only=True),axis=1)\n",
    "All_df['word_in_title_string_only_num'] = All_df['word_in_title_string_only_tuple'].map(lambda x: x[0])\n",
    "All_df['word_in_title_string_only_sum'] = All_df['word_in_title_string_only_tuple'].map(lambda x: x[1])\n",
    "All_df['word_in_title_string_only_let'] = All_df['word_in_title_string_only_tuple'].map(lambda x: x[2])\n",
    "All_df['word_in_title_string_only_numratio'] = All_df['word_in_title_string_only_tuple'].map(lambda x: x[3])\n",
    "All_df['word_in_title_string_only_letratio'] = All_df['word_in_title_string_only_tuple'].map(lambda x: x[4])\n",
    "All_df['word_in_title_string_only_string'] = All_df['word_in_title_string_only_tuple'].map(lambda x: x[5])\n",
    "All_df=All_df.drop(['word_in_title_string_only_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['word_in_title_w_dash']=All_df.apply(lambda x: \\\n",
    "            str_common_word(words_w_dash(x['search_term_stemmed']),words_w_dash(x['product_title_stemmed']))[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str_2common_words_111(str1, str2, string_only=False):\n",
    "    num=0\n",
    "    total_entries=0\n",
    "    cnt_letters=0\n",
    "    cnt_unique_letters=0\n",
    "    words_in_query=str1.split()\n",
    "    for cnt in range(0,len(words_in_query)-1):\n",
    "        two_words=words_in_query[cnt]+' '+words_in_query[cnt+1]\n",
    "        if string_only==False or len(re.findall(r'\\d+', two_words))==0:\n",
    "            if (' '+two_words+' ') in (' '+str2+' '):\n",
    "                num+=1\n",
    "                total_entries+=(' '+str2+' ').count(' '+two_words+' ')\n",
    "                cnt_letters+=(' '+str2+' ').count(' '+two_words+' ') * (len(two_words)-1)\n",
    "                cnt_unique_letters+=(len(two_words)-1)\n",
    "    return num, total_entries, cnt_unique_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['two_words_in_title_tuple']=All_df.apply(lambda x: \\\n",
    "            str_2common_words_111(x['search_term_stemmed'],x['product_title_stemmed']),axis=1)\n",
    "All_df['two_words_in_title_num'] = All_df['two_words_in_title_tuple'].map(lambda x: x[0])\n",
    "All_df['two_words_in_title_sum'] = All_df['two_words_in_title_tuple'].map(lambda x: x[1])\n",
    "All_df['two_words_in_title_let'] = All_df['two_words_in_title_tuple'].map(lambda x: x[2])\n",
    "All_df=All_df.drop(['two_words_in_title_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['two_words_in_title_string_only_tuple']=All_df.apply(lambda x: \\\n",
    "            str_2common_words_111(x['search_term_stemmed'],x['product_title_stemmed'],string_only=True),axis=1)\n",
    "All_df['two_words_in_title_string_only_num'] = All_df['two_words_in_title_string_only_tuple'].map(lambda x: x[0])\n",
    "All_df['two_words_in_title_string_only_sum'] = All_df['two_words_in_title_string_only_tuple'].map(lambda x: x[1])\n",
    "All_df['two_words_in_title_string_only_let'] = All_df['two_words_in_title_string_only_tuple'].map(lambda x: x[2])\n",
    "All_df=All_df.drop(['two_words_in_title_string_only_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['common_digits_in_title_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_digits(x['search_term_stemmed'],x['product_title_stemmed']),axis=1)\n",
    "All_df['len_of_digits_in_query'] = All_df['common_digits_in_title_tuple'].map(lambda x: x[0])\n",
    "All_df['len_of_digits_in_title'] = All_df['common_digits_in_title_tuple'].map(lambda x: x[1])\n",
    "All_df['common_digits_in_title_num'] = All_df['common_digits_in_title_tuple'].map(lambda x: x[2])\n",
    "All_df['common_digits_in_title_ratio'] = All_df['common_digits_in_title_tuple'].map(lambda x: x[3])\n",
    "All_df['common_digits_in_title_jaccard'] = All_df['common_digits_in_title_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['common_digits_in_title_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['nn_important_in_title_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(str_stemmer_wo_parser(nn_important_words(x['search_term_tokens']),stoplist=stoplist_wo_can),\\\n",
    "                            x['product_title_stemmed']),axis=1)\n",
    "All_df['nn_important_in_title_num'] = All_df['nn_important_in_title_tuple'].map(lambda x: x[0])\n",
    "All_df['nn_important_in_title_sum'] = All_df['nn_important_in_title_tuple'].map(lambda x: x[1])\n",
    "All_df['nn_important_in_title_let'] = All_df['nn_important_in_title_tuple'].map(lambda x: x[2])\n",
    "All_df['nn_important_in_title_numratio'] = All_df['nn_important_in_title_tuple'].map(lambda x: x[3])\n",
    "All_df['nn_important_in_title_letratio'] = All_df['nn_important_in_title_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['nn_important_in_title_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['nn_unimportant_in_title_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(str_stemmer_wo_parser(nn_unimportant_words(x['search_term_tokens']),stoplist=stoplist_wo_can),\\\n",
    "                            x['product_title_stemmed']),axis=1)\n",
    "All_df['nn_unimportant_in_title_num'] = All_df['nn_unimportant_in_title_tuple'].map(lambda x: x[0])\n",
    "All_df['nn_unimportant_in_title_let'] = All_df['nn_unimportant_in_title_tuple'].map(lambda x: x[2])\n",
    "All_df['nn_unimportant_in_title_letratio'] = All_df['nn_unimportant_in_title_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['nn_unimportant_in_title_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['nn_important_in_nn_important_in_title_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(str_stemmer_wo_parser(nn_important_words(x['search_term_tokens']),stoplist=stoplist_wo_can),\\\n",
    "                            str_stemmer_wo_parser(nn_important_words(x['product_title_tokens']),stoplist=stoplist_wo_can)),axis=1)\n",
    "All_df['nn_important_in_nn_important_in_title_num'] = All_df['nn_important_in_nn_important_in_title_tuple'].map(lambda x: x[0])\n",
    "All_df['nn_important_in_nn_important_in_title_sum'] = All_df['nn_important_in_nn_important_in_title_tuple'].map(lambda x: x[1])\n",
    "All_df['nn_important_in_nn_important_in_title_let'] = All_df['nn_important_in_nn_important_in_title_tuple'].map(lambda x: x[2])\n",
    "All_df['nn_important_in_nn_important_in_title_letratio'] = All_df['nn_important_in_nn_important_in_title_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['nn_important_in_nn_important_in_title_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['nn_important_in_nn_unimportant_in_title_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(str_stemmer_wo_parser(nn_important_words(x['search_term_tokens']),stoplist=stoplist_wo_can),\\\n",
    "                            str_stemmer_wo_parser(nn_unimportant_words(x['product_title_tokens']),stoplist=stoplist_wo_can)),axis=1)\n",
    "All_df['nn_important_in_nn_unimportant_in_title_num'] = All_df['nn_important_in_nn_unimportant_in_title_tuple'].map(lambda x: x[0])\n",
    "All_df['nn_important_in_nn_unimportant_in_title_sum'] = All_df['nn_important_in_nn_unimportant_in_title_tuple'].map(lambda x: x[1])\n",
    "All_df['nn_important_in_nn_unimportant_in_title_let'] = All_df['nn_important_in_nn_unimportant_in_title_tuple'].map(lambda x: x[2])\n",
    "All_df['nn_important_in_nn_unimportant_in_title_letratio'] = All_df['nn_important_in_nn_unimportant_in_title_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['nn_important_in_nn_unimportant_in_title_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['nn_unimportant_in_nn_important_in_title_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(str_stemmer_wo_parser(nn_unimportant_words(x['search_term_tokens']),stoplist=stoplist_wo_can),\\\n",
    "                            str_stemmer_wo_parser(nn_important_words(x['product_title_tokens']),stoplist=stoplist_wo_can)),axis=1)\n",
    "All_df['nn_unimportant_in_nn_important_in_title_num'] = All_df['nn_unimportant_in_nn_important_in_title_tuple'].map(lambda x: x[0])\n",
    "All_df['nn_unimportant_in_nn_important_in_title_sum'] = All_df['nn_unimportant_in_nn_important_in_title_tuple'].map(lambda x: x[1])\n",
    "All_df['nn_unimportant_in_nn_important_in_title_let'] = All_df['nn_unimportant_in_nn_important_in_title_tuple'].map(lambda x: x[2])\n",
    "All_df['nn_unimportant_in_nn_important_in_title_letratio'] = All_df['nn_unimportant_in_nn_important_in_title_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['nn_unimportant_in_nn_important_in_title_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['jj_rb_in_jj_rb_in_title_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(str_stemmer_wo_parser(jj_rb_words(x['search_term_tokens']),stoplist=stoplist_wo_can),\\\n",
    "                            str_stemmer_wo_parser(jj_rb_words(x['product_title_tokens']),stoplist=stoplist_wo_can)),axis=1)\n",
    "All_df['jj_rb_in_jj_rb_in_title_num'] = All_df['jj_rb_in_jj_rb_in_title_tuple'].map(lambda x: x[0])\n",
    "All_df['jj_rb_in_jj_rb_in_title_sum'] = All_df['jj_rb_in_jj_rb_in_title_tuple'].map(lambda x: x[1])\n",
    "All_df['jj_rb_in_jj_rb_in_title_let'] = All_df['jj_rb_in_jj_rb_in_title_tuple'].map(lambda x: x[2])\n",
    "All_df=All_df.drop(['jj_rb_in_jj_rb_in_title_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words_in_title time: 98.9 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "All_df['vbg_in_vbg_in_title_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(str_stemmer_wo_parser(vbg_words(x['search_term_tokens']),stoplist=stoplist_wo_can),\\\n",
    "                            str_stemmer_wo_parser(vbg_words(x['product_title_tokens']),stoplist=stoplist_wo_can)),axis=1)\n",
    "All_df['vbg_in_vbg_in_title_num'] = All_df['vbg_in_vbg_in_title_tuple'].map(lambda x: x[0])\n",
    "All_df['vbg_in_vbg_in_title_sum'] = All_df['vbg_in_vbg_in_title_tuple'].map(lambda x: x[1])\n",
    "All_df['vbg_in_vbg_in_title_let'] = All_df['vbg_in_vbg_in_title_tuple'].map(lambda x: x[2])\n",
    "All_df=All_df.drop(['vbg_in_vbg_in_title_tuple'],axis=1)\n",
    "print('words_in_title time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### query vs product description\n",
    "All_df['wordFor_in_description_string_only_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(x['search_term_for_stemmed'],x['product_description_stemmed'],string_only=True),axis=1)\n",
    "All_df['wordFor_in_description_string_only_num'] = All_df['wordFor_in_description_string_only_tuple'].map(lambda x: x[0])\n",
    "All_df['wordFor_in_description_string_only_let'] = All_df['wordFor_in_description_string_only_tuple'].map(lambda x: x[2])\n",
    "All_df['wordFor_in_description_string_only_letratio'] = All_df['wordFor_in_description_string_only_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['wordFor_in_description_string_only_tuple'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['wordWith_in_description_string_only_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(x['search_term_with_stemmed'],x['product_description_stemmed'],string_only=True),axis=1)\n",
    "All_df['wordWith_in_description_string_only_num'] = All_df['wordWith_in_description_string_only_tuple'].map(lambda x: x[0])\n",
    "All_df['wordWith_in_description_string_only_let'] = All_df['wordWith_in_description_string_only_tuple'].map(lambda x: x[2])\n",
    "All_df['wordWith_in_description_string_only_letratio'] = All_df['wordWith_in_description_string_only_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['wordWith_in_description_string_only_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['query_in_description']=All_df.apply(lambda x: \\\n",
    "            query_in_text(x['search_term_stemmed'],x['product_description_stemmed']),axis=1)\n",
    "\n",
    "All_df['word_in_description_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(x['search_term_stemmed'],x['product_description_stemmed']),axis=1)\n",
    "All_df['word_in_description_num'] = All_df['word_in_description_tuple'].map(lambda x: x[0])\n",
    "All_df['word_in_description_sum'] = All_df['word_in_description_tuple'].map(lambda x: x[1])\n",
    "All_df['word_in_description_let'] = All_df['word_in_description_tuple'].map(lambda x: x[2])\n",
    "All_df['word_in_description_numratio'] = All_df['word_in_description_tuple'].map(lambda x: x[3])\n",
    "All_df['word_in_description_letratio'] = All_df['word_in_description_tuple'].map(lambda x: x[4])\n",
    "All_df['word_in_description_string'] = All_df['word_in_description_tuple'].map(lambda x: x[5])\n",
    "All_df=All_df.drop(['word_in_description_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['word_in_description_string_only_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(x['search_term_stemmed'],x['product_description_stemmed'],string_only=True),axis=1)\n",
    "All_df['word_in_description_string_only_num'] = All_df['word_in_description_string_only_tuple'].map(lambda x: x[0])\n",
    "All_df['word_in_description_string_only_sum'] = All_df['word_in_description_string_only_tuple'].map(lambda x: x[1])\n",
    "All_df['word_in_description_string_only_let'] = All_df['word_in_description_string_only_tuple'].map(lambda x: x[2])\n",
    "All_df['word_in_description_string_only_numratio'] = All_df['word_in_description_string_only_tuple'].map(lambda x: x[3])\n",
    "All_df['word_in_description_string_only_letratio'] = All_df['word_in_description_string_only_tuple'].map(lambda x: x[4])\n",
    "All_df['word_in_description_string_only_string'] = All_df['word_in_description_string_only_tuple'].map(lambda x: x[5])\n",
    "All_df=All_df.drop(['word_in_description_string_only_tuple'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['word_in_description_w_dash']=All_df.apply(lambda x: \\\n",
    "            str_common_word(words_w_dash(x['search_term_stemmed']),words_w_dash(x['product_description_stemmed']))[0],axis=1)\n",
    "\n",
    "All_df['two_words_in_description_tuple']=All_df.apply(lambda x: \\\n",
    "            str_2common_words_111(x['search_term_stemmed'],x['product_description_stemmed']),axis=1)\n",
    "All_df['two_words_in_description_num'] = All_df['two_words_in_description_tuple'].map(lambda x: x[0])\n",
    "All_df['two_words_in_description_sum'] = All_df['two_words_in_description_tuple'].map(lambda x: x[1])\n",
    "All_df['two_words_in_description_let'] = All_df['two_words_in_description_tuple'].map(lambda x: x[2])\n",
    "All_df=All_df.drop(['two_words_in_description_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['two_words_in_description_string_only_tuple']=All_df.apply(lambda x: \\\n",
    "            str_2common_words_111(x['search_term_stemmed'],x['product_description_stemmed'],string_only=True),axis=1)\n",
    "All_df['two_words_in_description_string_only_num'] = All_df['two_words_in_description_string_only_tuple'].map(lambda x: x[0])\n",
    "All_df['two_words_in_description_string_only_sum'] = All_df['two_words_in_description_string_only_tuple'].map(lambda x: x[1])\n",
    "All_df['two_words_in_description_string_only_let'] = All_df['two_words_in_description_string_only_tuple'].map(lambda x: x[2])\n",
    "All_df=All_df.drop(['two_words_in_description_string_only_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['common_digits_in_description_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_digits(x['search_term_stemmed'],x['product_description_stemmed']),axis=1)\n",
    "All_df['len_of_digits_in_query'] = All_df['common_digits_in_description_tuple'].map(lambda x: x[0])\n",
    "All_df['len_of_digits_in_description'] = All_df['common_digits_in_description_tuple'].map(lambda x: x[1])\n",
    "All_df['common_digits_in_description_num'] = All_df['common_digits_in_description_tuple'].map(lambda x: x[2])\n",
    "All_df['common_digits_in_description_ratio'] = All_df['common_digits_in_description_tuple'].map(lambda x: x[3])\n",
    "All_df['common_digits_in_description_jaccard'] = All_df['common_digits_in_description_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['common_digits_in_description_tuple'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['nn_important_in_description_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(str_stemmer_wo_parser(nn_important_words(x['search_term_tokens']),stoplist=stoplist_wo_can),\\\n",
    "                            x['product_description_stemmed']),axis=1)\n",
    "All_df['nn_important_in_description_num'] = All_df['nn_important_in_description_tuple'].map(lambda x: x[0])\n",
    "All_df['nn_important_in_description_sum'] = All_df['nn_important_in_description_tuple'].map(lambda x: x[1])\n",
    "All_df['nn_important_in_description_let'] = All_df['nn_important_in_description_tuple'].map(lambda x: x[2])\n",
    "All_df['nn_important_in_description_numratio'] = All_df['nn_important_in_description_tuple'].map(lambda x: x[3])\n",
    "All_df['nn_important_in_description_letratio'] = All_df['nn_important_in_description_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['nn_important_in_description_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['nn_unimportant_in_description_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(str_stemmer_wo_parser(nn_unimportant_words(x['search_term_tokens']),stoplist=stoplist_wo_can),\\\n",
    "                            x['product_description_stemmed']),axis=1)\n",
    "All_df['nn_unimportant_in_description_num'] = All_df['nn_unimportant_in_description_tuple'].map(lambda x: x[0])\n",
    "All_df['nn_unimportant_in_description_let'] = All_df['nn_unimportant_in_description_tuple'].map(lambda x: x[2])\n",
    "All_df['nn_unimportant_in_description_letratio'] = All_df['nn_unimportant_in_description_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['nn_unimportant_in_description_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['nn_important_in_nn_important_in_description_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(str_stemmer_wo_parser(nn_important_words(x['search_term_tokens']),stoplist=stoplist_wo_can),\\\n",
    "                            str_stemmer_wo_parser(nn_important_words(str(x['product_description_tokens'])))),axis=1)\n",
    "All_df['nn_important_in_nn_important_in_description_num'] = All_df['nn_important_in_nn_important_in_description_tuple'].map(lambda x: x[0])\n",
    "All_df['nn_important_in_nn_important_in_description_sum'] = All_df['nn_important_in_nn_important_in_description_tuple'].map(lambda x: x[1])\n",
    "All_df['nn_important_in_nn_important_in_description_let'] = All_df['nn_important_in_nn_important_in_description_tuple'].map(lambda x: x[2])\n",
    "All_df['nn_important_in_nn_important_in_description_letratio'] = All_df['nn_important_in_nn_important_in_description_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['nn_important_in_nn_important_in_description_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['nn_important_in_nn_unimportant_in_description_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(str_stemmer_wo_parser(nn_important_words(x['search_term_tokens']),stoplist=stoplist_wo_can),\\\n",
    "                            str_stemmer_wo_parser(nn_unimportant_words(str(x['product_description_tokens'])))),axis=1)\n",
    "All_df['nn_important_in_nn_unimportant_in_description_num'] = All_df['nn_important_in_nn_unimportant_in_description_tuple'].map(lambda x: x[0])\n",
    "All_df['nn_important_in_nn_unimportant_in_description_sum'] = All_df['nn_important_in_nn_unimportant_in_description_tuple'].map(lambda x: x[1])\n",
    "All_df['nn_important_in_nn_unimportant_in_description_let'] = All_df['nn_important_in_nn_unimportant_in_description_tuple'].map(lambda x: x[2])\n",
    "All_df['nn_important_in_nn_unimportant_in_description_letratio'] = All_df['nn_important_in_nn_unimportant_in_description_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['nn_important_in_nn_unimportant_in_description_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['nn_unimportant_in_nn_important_in_description_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(str_stemmer_wo_parser(nn_unimportant_words(x['search_term_tokens']),stoplist=stoplist_wo_can),\\\n",
    "                            str_stemmer_wo_parser(nn_important_words(str(x['product_description_tokens'])))),axis=1)\n",
    "All_df['nn_unimportant_in_nn_important_in_description_num'] = All_df['nn_unimportant_in_nn_important_in_description_tuple'].map(lambda x: x[0])\n",
    "All_df['nn_unimportant_in_nn_important_in_description_sum'] = All_df['nn_unimportant_in_nn_important_in_description_tuple'].map(lambda x: x[1])\n",
    "All_df['nn_unimportant_in_nn_important_in_description_let'] = All_df['nn_unimportant_in_nn_important_in_description_tuple'].map(lambda x: x[2])\n",
    "All_df['nn_unimportant_in_nn_important_in_description_letratio'] = All_df['nn_unimportant_in_nn_important_in_description_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['nn_unimportant_in_nn_important_in_description_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['jj_rb_in_jj_rb_in_description_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(str_stemmer_wo_parser(jj_rb_words(x['search_term_tokens']),stoplist=stoplist_wo_can),\\\n",
    "                            str_stemmer_wo_parser(jj_rb_words(str(x['product_description_tokens'])))),axis=1)\n",
    "All_df['jj_rb_in_jj_rb_in_description_num'] = All_df['jj_rb_in_jj_rb_in_description_tuple'].map(lambda x: x[0])\n",
    "All_df['jj_rb_in_jj_rb_in_description_sum'] = All_df['jj_rb_in_jj_rb_in_description_tuple'].map(lambda x: x[1])\n",
    "All_df['jj_rb_in_jj_rb_in_description_let'] = All_df['jj_rb_in_jj_rb_in_description_tuple'].map(lambda x: x[2])\n",
    "All_df=All_df.drop(['jj_rb_in_jj_rb_in_description_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words_in_description time: 43.9 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "All_df['vbg_in_vbg_in_description_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(str_stemmer_wo_parser(vbg_words(x['search_term_tokens']),stoplist=stoplist_wo_can),\\\n",
    "                            str_stemmer_wo_parser(vbg_words(str(x['product_description_tokens'])))),axis=1)\n",
    "All_df['vbg_in_vbg_in_description_num'] = All_df['vbg_in_vbg_in_description_tuple'].map(lambda x: x[0])\n",
    "All_df['vbg_in_vbg_in_description_sum'] = All_df['vbg_in_vbg_in_description_tuple'].map(lambda x: x[1])\n",
    "All_df['vbg_in_vbg_in_description_let'] = All_df['vbg_in_vbg_in_description_tuple'].map(lambda x: x[2])\n",
    "All_df=All_df.drop(['vbg_in_vbg_in_description_tuple'],axis=1)\n",
    "print('words_in_description time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### query vs attribute bullets\n",
    "All_df['wordFor_in_bullets_string_only_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(x['search_term_for_stemmed'],x['attribute_bullets_stemmed'],string_only=True),axis=1)\n",
    "All_df['wordFor_in_bullets_string_only_num'] = All_df['wordFor_in_bullets_string_only_tuple'].map(lambda x: x[0])\n",
    "All_df['wordFor_in_bullets_string_only_let'] = All_df['wordFor_in_bullets_string_only_tuple'].map(lambda x: x[2])\n",
    "All_df['wordFor_in_bullets_string_only_letratio'] = All_df['wordFor_in_bullets_string_only_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['wordFor_in_bullets_string_only_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['wordWith_in_bullets_string_only_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(x['search_term_with_stemmed'],x['attribute_bullets_stemmed'],string_only=True),axis=1)\n",
    "All_df['wordWith_in_bullets_string_only_num'] = All_df['wordWith_in_bullets_string_only_tuple'].map(lambda x: x[0])\n",
    "All_df['wordWith_in_bullets_string_only_let'] = All_df['wordWith_in_bullets_string_only_tuple'].map(lambda x: x[2])\n",
    "All_df['wordWith_in_bullets_string_only_letratio'] = All_df['wordWith_in_bullets_string_only_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['wordWith_in_bullets_string_only_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['query_in_bullets']=All_df.apply(lambda x: \\\n",
    "            query_in_text(x['search_term_stemmed'],x['attribute_bullets_stemmed']),axis=1)\n",
    "\n",
    "All_df['word_in_bullets_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(x['search_term_stemmed'],x['attribute_bullets_stemmed']),axis=1)\n",
    "All_df['word_in_bullets_num'] = All_df['word_in_bullets_tuple'].map(lambda x: x[0])\n",
    "All_df['word_in_bullets_sum'] = All_df['word_in_bullets_tuple'].map(lambda x: x[1])\n",
    "All_df['word_in_bullets_let'] = All_df['word_in_bullets_tuple'].map(lambda x: x[2])\n",
    "All_df['word_in_bullets_numratio'] = All_df['word_in_bullets_tuple'].map(lambda x: x[3])\n",
    "All_df['word_in_bullets_letratio'] = All_df['word_in_bullets_tuple'].map(lambda x: x[4])\n",
    "All_df['word_in_bullets_string'] = All_df['word_in_bullets_tuple'].map(lambda x: x[5])\n",
    "All_df=All_df.drop(['word_in_bullets_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['word_in_bullets_string_only_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(x['search_term_stemmed'],x['attribute_bullets_stemmed'],string_only=True),axis=1)\n",
    "All_df['word_in_bullets_string_only_num'] = All_df['word_in_bullets_string_only_tuple'].map(lambda x: x[0])\n",
    "All_df['word_in_bullets_string_only_sum'] = All_df['word_in_bullets_string_only_tuple'].map(lambda x: x[1])\n",
    "All_df['word_in_bullets_string_only_let'] = All_df['word_in_bullets_string_only_tuple'].map(lambda x: x[2])\n",
    "All_df['word_in_bullets_string_only_numratio'] = All_df['word_in_bullets_string_only_tuple'].map(lambda x: x[3])\n",
    "All_df['word_in_bullets_string_only_letratio'] = All_df['word_in_bullets_string_only_tuple'].map(lambda x: x[4])\n",
    "All_df['word_in_bullets_string_only_string'] = All_df['word_in_bullets_string_only_tuple'].map(lambda x: x[5])\n",
    "All_df=All_df.drop(['word_in_bullets_string_only_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['word_in_bullets_w_dash']=All_df.apply(lambda x: \\\n",
    "            str_common_word(words_w_dash(x['search_term_stemmed']),words_w_dash(x['attribute_bullets_stemmed']))[0],axis=1)\n",
    "\n",
    "All_df['two_words_in_bullets_tuple']=All_df.apply(lambda x: \\\n",
    "            str_2common_words_111(x['search_term_stemmed'],x['attribute_bullets_stemmed']),axis=1)\n",
    "All_df['two_words_in_bullets_num'] = All_df['two_words_in_bullets_tuple'].map(lambda x: x[0])\n",
    "All_df['two_words_in_bullets_sum'] = All_df['two_words_in_bullets_tuple'].map(lambda x: x[1])\n",
    "All_df['two_words_in_bullets_let'] = All_df['two_words_in_bullets_tuple'].map(lambda x: x[2])\n",
    "All_df=All_df.drop(['two_words_in_bullets_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['two_words_in_bullets_string_only_tuple']=All_df.apply(lambda x: \\\n",
    "            str_2common_words_111(x['search_term_stemmed'],x['attribute_bullets_stemmed'],string_only=True),axis=1)\n",
    "All_df['two_words_in_bullets_string_only_num'] = All_df['two_words_in_bullets_string_only_tuple'].map(lambda x: x[0])\n",
    "All_df['two_words_in_bullets_string_only_sum'] = All_df['two_words_in_bullets_string_only_tuple'].map(lambda x: x[1])\n",
    "All_df['two_words_in_bullets_string_only_let'] = All_df['two_words_in_bullets_string_only_tuple'].map(lambda x: x[2])\n",
    "All_df=All_df.drop(['two_words_in_bullets_string_only_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['common_digits_in_bullets_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_digits(x['search_term_stemmed'],x['attribute_bullets_stemmed']),axis=1)\n",
    "All_df['len_of_digits_in_query'] = All_df['common_digits_in_bullets_tuple'].map(lambda x: x[0])\n",
    "All_df['len_of_digits_in_bullets'] = All_df['common_digits_in_bullets_tuple'].map(lambda x: x[1])\n",
    "All_df['common_digits_in_bullets_num'] = All_df['common_digits_in_bullets_tuple'].map(lambda x: x[2])\n",
    "All_df['common_digits_in_bullets_ratio'] = All_df['common_digits_in_bullets_tuple'].map(lambda x: x[3])\n",
    "All_df['common_digits_in_bullets_jaccard'] = All_df['common_digits_in_bullets_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['common_digits_in_bullets_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['nn_important_in_bullets_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(str_stemmer_wo_parser(nn_important_words(x['search_term_tokens']),stoplist=stoplist_wo_can),\\\n",
    "                            x['attribute_bullets_stemmed']),axis=1)\n",
    "All_df['nn_important_in_bullets_num'] = All_df['nn_important_in_bullets_tuple'].map(lambda x: x[0])\n",
    "All_df['nn_important_in_bullets_sum'] = All_df['nn_important_in_bullets_tuple'].map(lambda x: x[1])\n",
    "All_df['nn_important_in_bullets_let'] = All_df['nn_important_in_bullets_tuple'].map(lambda x: x[2])\n",
    "All_df['nn_important_in_bullets_numratio'] = All_df['nn_important_in_bullets_tuple'].map(lambda x: x[3])\n",
    "All_df['nn_important_in_bullets_letratio'] = All_df['nn_important_in_bullets_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['nn_important_in_bullets_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['nn_unimportant_in_bullets_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(str_stemmer_wo_parser(nn_unimportant_words(x['search_term_tokens']),stoplist=stoplist_wo_can),\\\n",
    "                            x['attribute_bullets_stemmed']),axis=1)\n",
    "All_df['nn_unimportant_in_bullets_num'] = All_df['nn_unimportant_in_bullets_tuple'].map(lambda x: x[0])\n",
    "All_df['nn_unimportant_in_bullets_let'] = All_df['nn_unimportant_in_bullets_tuple'].map(lambda x: x[2])\n",
    "All_df['nn_unimportant_in_bullets_letratio'] = All_df['nn_unimportant_in_bullets_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['nn_unimportant_in_bullets_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['nn_important_in_nn_important_in_bullets_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(str_stemmer_wo_parser(nn_important_words(x['search_term_tokens']),stoplist=stoplist_wo_can),\\\n",
    "                            str_stemmer_wo_parser(nn_important_words(x['attribute_bullets_tokens']))),axis=1)\n",
    "All_df['nn_important_in_nn_important_in_bullets_num'] = All_df['nn_important_in_nn_important_in_bullets_tuple'].map(lambda x: x[0])\n",
    "All_df['nn_important_in_nn_important_in_bullets_sum'] = All_df['nn_important_in_nn_important_in_bullets_tuple'].map(lambda x: x[1])\n",
    "All_df['nn_important_in_nn_important_in_bullets_let'] = All_df['nn_important_in_nn_important_in_bullets_tuple'].map(lambda x: x[2])\n",
    "All_df['nn_important_in_nn_important_in_bullets_letratio'] = All_df['nn_important_in_nn_important_in_bullets_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['nn_important_in_nn_important_in_bullets_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['nn_important_in_nn_unimportant_in_bullets_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(str_stemmer_wo_parser(nn_important_words(x['search_term_tokens']),stoplist=stoplist_wo_can),\\\n",
    "                            str_stemmer_wo_parser(nn_unimportant_words(x['attribute_bullets_tokens']))),axis=1)\n",
    "All_df['nn_important_in_nn_unimportant_in_bullets_num'] = All_df['nn_important_in_nn_unimportant_in_bullets_tuple'].map(lambda x: x[0])\n",
    "All_df['nn_important_in_nn_unimportant_in_bullets_sum'] = All_df['nn_important_in_nn_unimportant_in_bullets_tuple'].map(lambda x: x[1])\n",
    "All_df['nn_important_in_nn_unimportant_in_bullets_let'] = All_df['nn_important_in_nn_unimportant_in_bullets_tuple'].map(lambda x: x[2])\n",
    "All_df['nn_important_in_nn_unimportant_in_bullets_letratio'] = All_df['nn_important_in_nn_unimportant_in_bullets_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['nn_important_in_nn_unimportant_in_bullets_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['nn_unimportant_in_nn_important_in_bullets_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(str_stemmer_wo_parser(nn_unimportant_words(x['search_term_tokens']),stoplist=stoplist_wo_can),\\\n",
    "                            str_stemmer_wo_parser(nn_important_words(x['attribute_bullets_tokens']))),axis=1)\n",
    "All_df['nn_unimportant_in_nn_important_in_bullets_num'] = All_df['nn_unimportant_in_nn_important_in_bullets_tuple'].map(lambda x: x[0])\n",
    "All_df['nn_unimportant_in_nn_important_in_bullets_sum'] = All_df['nn_unimportant_in_nn_important_in_bullets_tuple'].map(lambda x: x[1])\n",
    "All_df['nn_unimportant_in_nn_important_in_bullets_let'] = All_df['nn_unimportant_in_nn_important_in_bullets_tuple'].map(lambda x: x[2])\n",
    "All_df['nn_unimportant_in_nn_important_in_bullets_letratio'] = All_df['nn_unimportant_in_nn_important_in_bullets_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['nn_unimportant_in_nn_important_in_bullets_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['jj_rb_in_jj_rb_in_bullets_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(str_stemmer_wo_parser(jj_rb_words(x['search_term_tokens']),stoplist=stoplist_wo_can),\\\n",
    "                            str_stemmer_wo_parser(jj_rb_words(x['attribute_bullets_tokens']))),axis=1)\n",
    "All_df['jj_rb_in_jj_rb_in_bullets_num'] = All_df['jj_rb_in_jj_rb_in_bullets_tuple'].map(lambda x: x[0])\n",
    "All_df['jj_rb_in_jj_rb_in_bullets_sum'] = All_df['jj_rb_in_jj_rb_in_bullets_tuple'].map(lambda x: x[1])\n",
    "All_df['jj_rb_in_jj_rb_in_bullets_let'] = All_df['jj_rb_in_jj_rb_in_bullets_tuple'].map(lambda x: x[2])\n",
    "All_df=All_df.drop(['jj_rb_in_jj_rb_in_bullets_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words_in_bullets time: 16.7 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "All_df['vbg_in_vbg_in_bullets_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(str_stemmer_wo_parser(vbg_words(x['search_term_tokens']),stoplist=stoplist_wo_can),\\\n",
    "                            str_stemmer_wo_parser(vbg_words(x['attribute_bullets_tokens']))),axis=1)\n",
    "All_df['vbg_in_vbg_in_bullets_num'] = All_df['vbg_in_vbg_in_bullets_tuple'].map(lambda x: x[0])\n",
    "All_df['vbg_in_vbg_in_bullets_sum'] = All_df['vbg_in_vbg_in_bullets_tuple'].map(lambda x: x[1])\n",
    "All_df['vbg_in_vbg_in_bullets_let'] = All_df['vbg_in_vbg_in_bullets_tuple'].map(lambda x: x[2])\n",
    "All_df=All_df.drop(['vbg_in_vbg_in_bullets_tuple'],axis=1)\n",
    "print('words_in_bullets time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "\n",
    "All_df['keyword_in_titlekeys_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(x['search_term_keys_stemmed'],x['product_title_keys_stemmed']),axis=1)\n",
    "All_df['keyword_in_titlekeys_num'] = All_df['keyword_in_titlekeys_tuple'].map(lambda x: x[0])\n",
    "All_df['keyword_in_titlekeys_sum'] = All_df['keyword_in_titlekeys_tuple'].map(lambda x: x[1])\n",
    "All_df['keyword_in_titlekeys_let'] = All_df['keyword_in_titlekeys_tuple'].map(lambda x: x[2])\n",
    "All_df['keyword_in_titlekeys_numratio'] = All_df['keyword_in_titlekeys_tuple'].map(lambda x: x[3])\n",
    "All_df['keyword_in_titlekeys_letratio'] = All_df['keyword_in_titlekeys_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['keyword_in_titlekeys_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['keyword_in_description_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(x['search_term_keys_stemmed'],x['product_description_stemmed']),axis=1)\n",
    "All_df['keyword_in_description_num'] = All_df['keyword_in_description_tuple'].map(lambda x: x[0])\n",
    "All_df['keyword_in_description_sum'] = All_df['keyword_in_description_tuple'].map(lambda x: x[1])\n",
    "All_df['keyword_in_description_let'] = All_df['keyword_in_description_tuple'].map(lambda x: x[2])\n",
    "All_df['keyword_in_description_numratio'] = All_df['keyword_in_description_tuple'].map(lambda x: x[3])\n",
    "All_df['keyword_in_description_letratio'] = All_df['keyword_in_description_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['keyword_in_description_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['keyword_in_bullets_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(x['search_term_keys_stemmed'],x['attribute_bullets_stemmed']),axis=1)\n",
    "All_df['keyword_in_bullets_num'] = All_df['keyword_in_bullets_tuple'].map(lambda x: x[0])\n",
    "All_df['keyword_in_bullets_sum'] = All_df['keyword_in_bullets_tuple'].map(lambda x: x[1])\n",
    "All_df['keyword_in_bullets_let'] = All_df['keyword_in_bullets_tuple'].map(lambda x: x[2])\n",
    "All_df['keyword_in_bullets_numratio'] = All_df['keyword_in_bullets_tuple'].map(lambda x: x[3])\n",
    "All_df['keyword_in_bullets_letratio'] = All_df['keyword_in_bullets_tuple'].map(lambda x: x[4])\n",
    "All_df=All_df.drop(['keyword_in_bullets_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['jaccard_keyword_in_titlekeys_tuple']=All_df.apply(lambda x: \\\n",
    "            str_jaccard(x['search_term_keys_stemmed'],x['product_title_keys_stemmed']),axis=1)\n",
    "All_df['keyword_in_titlekeys_jacnum'] = All_df['jaccard_keyword_in_titlekeys_tuple'].map(lambda x: x[0])\n",
    "All_df['keyword_in_titlekeys_jaclet'] = All_df['jaccard_keyword_in_titlekeys_tuple'].map(lambda x: x[1])\n",
    "All_df=All_df.drop(['jaccard_keyword_in_titlekeys_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concatenate_keys(beforethekey,thekey):\n",
    "    l=[beforethekey,thekey]\n",
    "    if \"\" in l:\n",
    "        l.remove(\"\")\n",
    "    if \"\" in l:\n",
    "        l.remove(\"\")\n",
    "    return \" \".join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['thekeys_in_title']=All_df.apply(lambda x: \\\n",
    "            query_in_text(concatenate_keys(x['search_term_beforethekey_stemmed'],x['search_term_thekey_stemmed']),\\\n",
    "                          x['product_title_stemmed']), axis=1)  \n",
    "All_df['thekeys_in_description']=All_df.apply(lambda x: \\\n",
    "            query_in_text(concatenate_keys(x['search_term_beforethekey_stemmed'],x['search_term_thekey_stemmed']),\\\n",
    "                          x['product_description_stemmed']), axis=1)  \n",
    "All_df['thekeys_in_bullets']=All_df.apply(lambda x: \\\n",
    "            query_in_text(concatenate_keys(x['search_term_beforethekey_stemmed'],x['search_term_thekey_stemmed']),\\\n",
    "                          x['attribute_bullets_stemmed']), axis=1)                            \n",
    "\n",
    "All_df['thekey_in_description_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(x['search_term_thekey_stemmed'],x['product_description_stemmed']), axis=1) \n",
    "All_df['thekey_in_description_sum'] = All_df['thekey_in_description_tuple'].map(lambda x: x[1])\n",
    "All_df['thekey_in_description_let'] = All_df['thekey_in_description_tuple'].map(lambda x: x[2])\n",
    "All_df=All_df.drop(['thekey_in_description_tuple'],axis=1)\n",
    "\n",
    "All_df['thekey_in_bullets_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(x['search_term_thekey_stemmed'],x['attribute_bullets_stemmed']), axis=1) \n",
    "All_df['thekey_in_bullets_sum'] = All_df['thekey_in_bullets_tuple'].map(lambda x: x[1])\n",
    "All_df['thekey_in_bullets_let'] = All_df['thekey_in_bullets_tuple'].map(lambda x: x[2])\n",
    "All_df=All_df.drop(['thekey_in_bullets_tuple'],axis=1)\n",
    "\n",
    "\n",
    "All_df['beforethekey_in_description_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(x['search_term_beforethekey_stemmed'],x['product_description_stemmed']), axis=1) \n",
    "All_df['beforethekey_in_description_sum'] = All_df['beforethekey_in_description_tuple'].map(lambda x: x[1])\n",
    "All_df['beforethekey_in_description_let'] = All_df['beforethekey_in_description_tuple'].map(lambda x: x[2])\n",
    "All_df=All_df.drop(['beforethekey_in_description_tuple'],axis=1)\n",
    "\n",
    "All_df['beforethekey_in_bullets_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(x['search_term_beforethekey_stemmed'],x['attribute_bullets_stemmed']), axis=1) \n",
    "All_df['beforethekey_in_bullets_sum'] = All_df['beforethekey_in_bullets_tuple'].map(lambda x: x[1])\n",
    "All_df['beforethekey_in_bullets_let'] = All_df['beforethekey_in_bullets_tuple'].map(lambda x: x[2])\n",
    "All_df=All_df.drop(['beforethekey_in_bullets_tuple'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df['thekey_in_nn_important_in_description_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(x['search_term_thekey_stemmed'],\n",
    "                            str_stemmer_wo_parser(nn_important_words(str(x['product_description_tokens'])))),axis=1)\n",
    "All_df['thekey_in_nn_important_in_description_sum'] = All_df['thekey_in_nn_important_in_description_tuple'].map(lambda x: x[1])\n",
    "All_df['thekey_in_nn_important_in_description_let'] = All_df['thekey_in_nn_important_in_description_tuple'].map(lambda x: x[2])\n",
    "All_df=All_df.drop(['thekey_in_nn_important_in_description_tuple'],axis=1)\n",
    "\n",
    "All_df['thekey_in_nn_important_in_bullets_tuple']=All_df.apply(lambda x: \\\n",
    "            str_common_word(x['search_term_thekey_stemmed'],\\\n",
    "                            str_stemmer_wo_parser(nn_important_words(x['attribute_bullets_tokens']))),axis=1)\n",
    "All_df['thekey_in_nn_important_in_bullets_sum'] = All_df['thekey_in_nn_important_in_bullets_tuple'].map(lambda x: x[1])\n",
    "All_df['thekey_in_nn_important_in_bullets_let'] = All_df['thekey_in_nn_important_in_bullets_tuple'].map(lambda x: x[2])\n",
    "All_df=All_df.drop(['thekey_in_nn_important_in_bullets_tuple'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def match_thekeywords(word1,word2):\n",
    "    return int(len(word1)>0 and len(word2)>0 and (word1 in word2 or word2 in word1))\n",
    "\n",
    "All_df['thekey_in_thekey']=All_df.apply(lambda x: \\\n",
    "            match_thekeywords(x['search_term_thekey_stemmed'],x['product_title_thekey_stemmed']),axis=1)\n",
    "All_df['beforethekey_in_beforethekey']=All_df.apply(lambda x: \\\n",
    "            match_thekeywords(x['search_term_beforethekey_stemmed'],x['product_title_beforethekey_stemmed']),axis=1) \n",
    "All_df['beforethekeys_in_beforethekeys']=All_df.apply(lambda x: \\\n",
    "            max(match_thekeywords(x['search_term_beforethekey_stemmed'],x['product_title_beforethekey_stemmed']),\\\n",
    "                match_thekeywords(x['search_term_beforethekey_stemmed'],x['product_title_before2thekey_stemmed']),\\\n",
    "                match_thekeywords(x['search_term_before2thekey_stemmed'],x['product_title_beforethekey_stemmed']),\\\n",
    "                match_thekeywords(x['search_term_before2thekey_stemmed'],x['product_title_before2thekey_stemmed'])),axis=1)\n",
    "All_df['thekey_in_beforethekeys']=All_df.apply(lambda x: \\\n",
    "            max(match_thekeywords(x['search_term_thekey_stemmed'],x['product_title_beforethekey_stemmed']),\\\n",
    "                match_thekeywords(x['search_term_thekey_stemmed'],x['product_title_before2thekey_stemmed'])),axis=1)\n",
    "All_df['beforethekeys_in_thekey']=All_df.apply(lambda x: \\\n",
    "            max(match_thekeywords(x['search_term_thekey_stemmed'],x['product_title_beforethekey_stemmed']),\\\n",
    "                match_thekeywords(x['search_term_thekey_stemmed'],x['product_title_before2thekey_stemmed'])),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 39278 unique combinations; 0.3 minutes\n",
      "5000 out of 39278 unique combinations; 1.2 minutes\n",
      "10000 out of 39278 unique combinations; 2.2 minutes\n",
      "15000 out of 39278 unique combinations; 3.1 minutes\n",
      "20000 out of 39278 unique combinations; 4.1 minutes\n",
      "25000 out of 39278 unique combinations; 5.0 minutes\n",
      "30000 out of 39278 unique combinations; 6.0 minutes\n",
      "35000 out of 39278 unique combinations; 6.9 minutes\n",
      "thekeys similarity time: 7.8 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Use NLTK WordNet to calculate similarities between keywords\n",
    "t2 = time()\n",
    "\n",
    "##############\n",
    "All_df['key_for_dict']=All_df.apply(lambda x: x['search_term_thekey']+\"\\t\"+x['product_title_thekey'],axis=1) \n",
    "aa=list(set(list(All_df['key_for_dict'])))\n",
    "my_dict={}\n",
    "for i in range(0,len(aa)):\n",
    "    my_dict[aa[i]]=find_similarity(aa[i].split(\"\\t\")[0],aa[i].split(\"\\t\")[1])\n",
    "    if (i % 5000)==0:\n",
    "        print(\"\"+str(i)+\" out of \"+str(len(aa))+\" unique combinations; \"+str(round((time()-t2)/60,1))+\" minutes\")\n",
    "    \n",
    "All_df['thekeys_similarity_tuple']=All_df['key_for_dict'].map(lambda x: my_dict[x] )\n",
    "All_df['thekeys_pathsimilarity_max'] = All_df['thekeys_similarity_tuple'].map(lambda x: x[0])\n",
    "All_df['thekeys_pathsimilarity_mean'] = All_df['thekeys_similarity_tuple'].map(lambda x: x[1])\n",
    "All_df['thekeys_lchsimilarity_max'] = All_df['thekeys_similarity_tuple'].map(lambda x: x[2])\n",
    "All_df['thekeys_lchsimilarity_mean'] = All_df['thekeys_similarity_tuple'].map(lambda x: x[3])\n",
    "All_df['thekeys_ressimilarity_max'] = All_df['thekeys_similarity_tuple'].map(lambda x: x[4])\n",
    "All_df['thekeys_ressimilarity_mean'] = All_df['thekeys_similarity_tuple'].map(lambda x: x[5])\n",
    "All_df=All_df.drop(['thekeys_similarity_tuple'],axis=1)\n",
    "print('thekeys similarity time:',round((time()-t2)/60,1) ,'minutes\\n')\n",
    "t2 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 43176 unique combinations; 0.3 minutes\n",
      "5000 out of 43176 unique combinations; 1.6 minutes\n",
      "10000 out of 43176 unique combinations; 2.8 minutes\n",
      "15000 out of 43176 unique combinations; 4.1 minutes\n",
      "20000 out of 43176 unique combinations; 5.2 minutes\n",
      "25000 out of 43176 unique combinations; 6.4 minutes\n",
      "30000 out of 43176 unique combinations; 7.6 minutes\n",
      "35000 out of 43176 unique combinations; 8.8 minutes\n",
      "40000 out of 43176 unique combinations; 10.0 minutes\n",
      "beforethekeys similarity time: 10.8 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "All_df['key_for_dict']=All_df.apply(lambda x: x['search_term_beforethekey']+\"\\t\"+x['product_title_beforethekey'],axis=1) \n",
    "aa=list(set(list(All_df['key_for_dict'])))\n",
    "my_dict={}\n",
    "for i in range(0,len(aa)):\n",
    "    my_dict[aa[i]]=find_similarity(aa[i].split(\"\\t\")[0],aa[i].split(\"\\t\")[1],nouns=False)\n",
    "    if (i % 5000)==0:\n",
    "        print(\"\"+str(i)+\" out of \"+str(len(aa))+\" unique combinations; \"+str(round((time()-t2)/60,1))+\" minutes\")\n",
    "    \n",
    "All_df['beforethekeys_similarity_tuple']=All_df['key_for_dict'].map(lambda x: my_dict[x] )\n",
    "All_df['beforethekeys_pathsimilarity_max'] = All_df['beforethekeys_similarity_tuple'].map(lambda x: x[0])\n",
    "All_df['beforethekeys_pathsimilarity_mean'] = All_df['beforethekeys_similarity_tuple'].map(lambda x: x[1])\n",
    "All_df['beforethekeys_lchsimilarity_max'] = All_df['beforethekeys_similarity_tuple'].map(lambda x: x[2])\n",
    "All_df['beforethekeys_lchsimilarity_mean'] = All_df['beforethekeys_similarity_tuple'].map(lambda x: x[3])\n",
    "All_df['beforethekeys_ressimilarity_max'] = All_df['beforethekeys_similarity_tuple'].map(lambda x: x[4])\n",
    "All_df['beforethekeys_ressimilarity_mean'] = All_df['beforethekeys_similarity_tuple'].map(lambda x: x[5])\n",
    "All_df=All_df.drop(['beforethekeys_similarity_tuple'],axis=1)\n",
    "print('beforethekeys similarity time:',round((time()-t2)/60,1) ,'minutes\\n')\n",
    "t2 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 52374 unique combinations; 0.3 minutes\n",
      "5000 out of 52374 unique combinations; 1.7 minutes\n",
      "10000 out of 52374 unique combinations; 3.0 minutes\n",
      "15000 out of 52374 unique combinations; 4.4 minutes\n",
      "20000 out of 52374 unique combinations; 5.8 minutes\n",
      "25000 out of 52374 unique combinations; 7.1 minutes\n",
      "30000 out of 52374 unique combinations; 8.5 minutes\n",
      "35000 out of 52374 unique combinations; 9.9 minutes\n",
      "40000 out of 52374 unique combinations; 11.2 minutes\n",
      "45000 out of 52374 unique combinations; 12.6 minutes\n",
      "50000 out of 52374 unique combinations; 13.9 minutes\n",
      "thekey_beforethekey similarity time: 14.6 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "All_df['key_for_dict']=All_df.apply(lambda x: x['search_term_thekey']+\"\\t\"+x['product_title_beforethekey'],axis=1) \n",
    "aa=list(set(list(All_df['key_for_dict'])))\n",
    "my_dict={}\n",
    "for i in range(0,len(aa)):\n",
    "    my_dict[aa[i]]=find_similarity(aa[i].split(\"\\t\")[0],aa[i].split(\"\\t\")[1],nouns=False)\n",
    "    if (i % 5000)==0:\n",
    "        print(\"\"+str(i)+\" out of \"+str(len(aa))+\" unique combinations; \"+str(round((time()-t2)/60,1))+\" minutes\")\n",
    "All_df['thekey_beforethekey_similarity_tuple']=All_df['key_for_dict'].map(lambda x: my_dict[x] )\n",
    "All_df['thekey_beforethekey_pathsimilarity_max'] = All_df['thekey_beforethekey_similarity_tuple'].map(lambda x: x[0])\n",
    "All_df['thekey_beforethekey_pathsimilarity_mean'] = All_df['thekey_beforethekey_similarity_tuple'].map(lambda x: x[1])\n",
    "All_df['thekey_beforethekey_lchsimilarity_max'] = All_df['thekey_beforethekey_similarity_tuple'].map(lambda x: x[2])\n",
    "All_df['thekey_beforethekey_lchsimilarity_mean'] = All_df['thekey_beforethekey_similarity_tuple'].map(lambda x: x[3])\n",
    "All_df['thekey_beforethekey_ressimilarity_max'] = All_df['thekey_beforethekey_similarity_tuple'].map(lambda x: x[4])\n",
    "All_df['thekey_beforethekey_ressimilarity_mean'] = All_df['thekey_beforethekey_similarity_tuple'].map(lambda x: x[5])\n",
    "All_df=All_df.drop(['thekey_beforethekey_similarity_tuple'],axis=1)\n",
    "print('thekey_beforethekey similarity time:',round((time()-t2)/60,1) ,'minutes\\n')\n",
    "t2 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 42371 unique combinations; 0.3 minutes\n",
      "5000 out of 42371 unique combinations; 1.5 minutes\n",
      "10000 out of 42371 unique combinations; 2.8 minutes\n",
      "15000 out of 42371 unique combinations; 4.0 minutes\n",
      "20000 out of 42371 unique combinations; 5.3 minutes\n",
      "25000 out of 42371 unique combinations; 6.6 minutes\n",
      "30000 out of 42371 unique combinations; 7.8 minutes\n",
      "35000 out of 42371 unique combinations; 9.1 minutes\n",
      "40000 out of 42371 unique combinations; 10.4 minutes\n",
      "thekey_before2thekey similarity time: 11.0 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "All_df['key_for_dict']=All_df.apply(lambda x: x['search_term_thekey']+\"\\t\"+x['product_title_before2thekey'],axis=1) \n",
    "aa=list(set(list(All_df['key_for_dict'])))\n",
    "my_dict={}\n",
    "for i in range(0,len(aa)):\n",
    "    my_dict[aa[i]]=find_similarity(aa[i].split(\"\\t\")[0],aa[i].split(\"\\t\")[1],nouns=False)\n",
    "    if (i % 5000)==0:\n",
    "        print(\"\"+str(i)+\" out of \"+str(len(aa))+\" unique combinations; \"+str(round((time()-t2)/60,1))+\" minutes\")\n",
    "All_df['thekey_before2thekey_similarity_tuple']=All_df['key_for_dict'].map(lambda x: my_dict[x] )\n",
    "All_df['thekey_before2thekey_pathsimilarity_max'] = All_df['thekey_before2thekey_similarity_tuple'].map(lambda x: x[0])\n",
    "All_df['thekey_before2thekey_pathsimilarity_mean'] = All_df['thekey_before2thekey_similarity_tuple'].map(lambda x: x[1])\n",
    "All_df['thekey_before2thekey_lchsimilarity_max'] = All_df['thekey_before2thekey_similarity_tuple'].map(lambda x: x[2])\n",
    "All_df['thekey_before2thekey_lchsimilarity_mean'] = All_df['thekey_before2thekey_similarity_tuple'].map(lambda x: x[3])\n",
    "All_df['thekey_before2thekey_ressimilarity_max'] = All_df['thekey_before2thekey_similarity_tuple'].map(lambda x: x[4])\n",
    "All_df['thekey_before2thekey_ressimilarity_mean'] = All_df['thekey_before2thekey_similarity_tuple'].map(lambda x: x[5])\n",
    "All_df=All_df.drop(['thekey_before2thekey_similarity_tuple'],axis=1)\n",
    "print('thekey_before2thekey similarity time:',round((time()-t2)/60,1) ,'minutes\\n')\n",
    "t2 = time()\n",
    "\n",
    "for var_name in ['pathsimilarity_max','pathsimilarity_mean','lchsimilarity_max','lchsimilarity_mean',\n",
    "                 'ressimilarity_max', 'ressimilarity_mean']:\n",
    "    All_df['thekey_beforethekeys_'+var_name] = All_df.apply(lambda x: \\\n",
    "        max(x['thekey_beforethekey_'+var_name], x['thekey_before2thekey_'+var_name]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 33754 unique combinations; 2.1 minutes\n",
      "5000 out of 33754 unique combinations; 3.5 minutes\n",
      "10000 out of 33754 unique combinations; 4.9 minutes\n",
      "15000 out of 33754 unique combinations; 6.3 minutes\n",
      "20000 out of 33754 unique combinations; 7.6 minutes\n",
      "25000 out of 33754 unique combinations; 9.0 minutes\n",
      "30000 out of 33754 unique combinations; 10.4 minutes\n",
      "beforethekey_thekey similarity time: 11.5 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "All_df['key_for_dict']=All_df.apply(lambda x: x['search_term_beforethekey']+\"\\t\"+x['product_title_thekey'],axis=1) \n",
    "aa=list(set(list(All_df['key_for_dict'])))\n",
    "my_dict={}\n",
    "for i in range(0,len(aa)):\n",
    "    my_dict[aa[i]]=find_similarity(aa[i].split(\"\\t\")[0],aa[i].split(\"\\t\")[1],nouns=False)\n",
    "    if (i % 5000)==0:\n",
    "        print(\"\"+str(i)+\" out of \"+str(len(aa))+\" unique combinations; \"+str(round((time()-t2)/60,1))+\" minutes\")\n",
    "    \n",
    "All_df['beforethekey_thekey_similarity_tuple']=All_df['key_for_dict'].map(lambda x: my_dict[x] )\n",
    "All_df['beforethekey_thekey_pathsimilarity_max'] = All_df['beforethekey_thekey_similarity_tuple'].map(lambda x: x[0])\n",
    "All_df['beforethekey_thekey_pathsimilarity_mean'] = All_df['beforethekey_thekey_similarity_tuple'].map(lambda x: x[1])\n",
    "All_df['beforethekey_thekey_lchsimilarity_max'] = All_df['beforethekey_thekey_similarity_tuple'].map(lambda x: x[2])\n",
    "All_df['beforethekey_thekey_lchsimilarity_mean'] = All_df['beforethekey_thekey_similarity_tuple'].map(lambda x: x[3])\n",
    "All_df['beforethekey_thekey_ressimilarity_max'] = All_df['beforethekey_thekey_similarity_tuple'].map(lambda x: x[4])\n",
    "All_df['beforethekey_thekey_ressimilarity_mean'] = All_df['beforethekey_thekey_similarity_tuple'].map(lambda x: x[5])\n",
    "All_df=All_df.drop(['beforethekey_thekey_similarity_tuple'],axis=1)\n",
    "print('beforethekey_thekey similarity time:',round((time()-t2)/60,1) ,'minutes\\n')\n",
    "t2 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df.to_csv('maybe_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 13945 unique combinations; 809.7 minutes\n",
      "5000 out of 13945 unique combinations; 811.0 minutes\n",
      "10000 out of 13945 unique combinations; 812.3 minutes\n",
      "query similarity time: 813.3 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "All_df['key_for_dict']=All_df.apply(lambda x: x['search_term_thekey']+\"\\t\"+x['search_term_beforethekey'],axis=1) \n",
    "aa=list(set(list(All_df['key_for_dict'])))\n",
    "my_dict={}\n",
    "for i in range(0,len(aa)):\n",
    "    my_dict[aa[i]]=find_similarity(aa[i].split(\"\\t\")[0],aa[i].split(\"\\t\")[1],nouns=False)\n",
    "    if (i % 5000)==0:\n",
    "        print(\"\"+str(i)+\" out of \"+str(len(aa))+\" unique combinations; \"+str(round((time()-t2)/60,1))+\" minutes\")\n",
    "    \n",
    "All_df['query_similarity_tuple']=All_df['key_for_dict'].map(lambda x: my_dict[x] )\n",
    "All_df['query_pathsimilarity_max'] = All_df['query_similarity_tuple'].map(lambda x: x[0])\n",
    "All_df['query_pathsimilarity_mean'] = All_df['query_similarity_tuple'].map(lambda x: x[1])\n",
    "All_df['query_lchhsimilarity_max'] = All_df['query_similarity_tuple'].map(lambda x: x[2])\n",
    "All_df['query_lchsimilarity_mean'] = All_df['query_similarity_tuple'].map(lambda x: x[3])\n",
    "All_df['query_ressimilarity_max'] = All_df['query_similarity_tuple'].map(lambda x: x[4])\n",
    "All_df['query_ressimilarity_mean'] = All_df['query_similarity_tuple'].map(lambda x: x[5])\n",
    "All_df=All_df.drop(['query_similarity_tuple'],axis=1)\n",
    "print('query similarity time:',round((time()-t2)/60,1) ,'minutes\\n')\n",
    "t2 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 22526 unique combinations; 1.3 minutes\n",
      "5000 out of 22526 unique combinations; 2.6 minutes\n",
      "10000 out of 22526 unique combinations; 4.0 minutes\n",
      "15000 out of 22526 unique combinations; 5.4 minutes\n",
      "20000 out of 22526 unique combinations; 6.7 minutes\n",
      "title similarity time: 7.4 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "All_df['key_for_dict']=All_df.apply(lambda x: x['product_title_thekey']+\"\\t\"+x['product_title_beforethekey'],axis=1) \n",
    "aa=list(set(list(All_df['key_for_dict'])))\n",
    "my_dict={}\n",
    "for i in range(0,len(aa)):\n",
    "    my_dict[aa[i]]=find_similarity(aa[i].split(\"\\t\")[0],aa[i].split(\"\\t\")[1],nouns=False)\n",
    "    if (i % 5000)==0:\n",
    "        print(\"\"+str(i)+\" out of \"+str(len(aa))+\" unique combinations; \"+str(round((time()-t2)/60,1))+\" minutes\")\n",
    "    \n",
    "All_df['title_similarity_tuple']=All_df['key_for_dict'].map(lambda x: my_dict[x] )\n",
    "All_df['title_pathsimilarity_max'] = All_df['title_similarity_tuple'].map(lambda x: x[0])\n",
    "All_df['title_pathsimilarity_mean'] = All_df['title_similarity_tuple'].map(lambda x: x[1])\n",
    "All_df['title_lchsimilarity_max'] = All_df['title_similarity_tuple'].map(lambda x: x[2])\n",
    "All_df['title_lchsimilarity_mean'] = All_df['title_similarity_tuple'].map(lambda x: x[3])\n",
    "All_df['title_ressimilarity_max'] = All_df['title_similarity_tuple'].map(lambda x: x[4])\n",
    "All_df['title_ressimilarity_mean'] = All_df['title_similarity_tuple'].map(lambda x: x[5])\n",
    "All_df=All_df.drop(['title_similarity_tuple'],axis=1)\n",
    "print('title similarity time:',round((time()-t2)/60,1) ,'minutes\\n')\n",
    "t2 = time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence match time: 892.6 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "All_df['seqmatch_title_tuple']=All_df.apply(lambda x: \\\n",
    "            seq_matcher(x['search_term_stemmed'],x['product_title_stemmed']),axis=1)\n",
    "All_df['seqmatch_title_ratio'] = All_df['seqmatch_title_tuple'].map(lambda x: x[0])\n",
    "All_df['seqmatch_title_ratioscaled'] = All_df['seqmatch_title_tuple'].map(lambda x: x[1])\n",
    "All_df=All_df.drop(['seqmatch_title_tuple'],axis=1)\n",
    "\n",
    "\n",
    "All_df['seqmatch_description_tuple']=All_df.apply(lambda x: \\\n",
    "            seq_matcher(x['search_term_stemmed'],x['product_description_stemmed']),axis=1)\n",
    "All_df['seqmatch_description_ratio'] = All_df['seqmatch_description_tuple'].map(lambda x: x[0])\n",
    "All_df['seqmatch_description_ratioscaled'] = All_df['seqmatch_description_tuple'].map(lambda x: x[1])\n",
    "All_df=All_df.drop(['seqmatch_description_tuple'],axis=1)\n",
    "\n",
    "All_df['seqmatch_bullets_tuple']=All_df.apply(lambda x: \\\n",
    "            seq_matcher(x['search_term_stemmed'],x['attribute_bullets_stemmed']),axis=1)\n",
    "All_df['seqmatch_bullets_ratio'] = All_df['seqmatch_bullets_tuple'].map(lambda x: x[0])\n",
    "All_df['seqmatch_bullets_ratioscaled'] = All_df['seqmatch_bullets_tuple'].map(lambda x: x[1])\n",
    "All_df=All_df.drop(['seqmatch_bullets_tuple'],axis=1)\n",
    "\n",
    "All_df['seqmatch_desc&bullets_tuple']=All_df.apply(lambda x: \\\n",
    "            seq_matcher(x['search_term_stemmed'],x['product_description_stemmed']+\" \"+x['attribute_bullets_stemmed']),axis=1)\n",
    "All_df['seqmatch_desc&bullets_ratio'] = All_df['seqmatch_desc&bullets_tuple'].map(lambda x: x[0])\n",
    "All_df['seqmatch_desc&bullets_ratioscaled'] = All_df['seqmatch_desc&bullets_tuple'].map(lambda x: x[1])\n",
    "All_df=All_df.drop(['seqmatch_desc&bullets_tuple'],axis=1)\n",
    "\n",
    "print('sequence match time:',round((time()-t0)/60,1) ,'minutes\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### STEP 7: Some TFIDF features\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "vectorizer_title =  TfidfVectorizer(stop_words='english',max_df=0.5) \n",
    "vectorizer_description =  TfidfVectorizer(stop_words='english',max_df=0.5) \n",
    "vectorizer_bullets =  TfidfVectorizer(stop_words='english',max_df=0.5) \n",
    "\n",
    "features_title = vectorizer_title.fit_transform(list(set(list(All_df['product_title_stemmed'])))) \n",
    "features_description = vectorizer_description.fit_transform(list(set(list(All_df['product_description_stemmed'])))) \n",
    "features_bullets = vectorizer_bullets.fit_transform(list(set(list(All_df['attribute_bullets_stemmed'])))) \n",
    "\n",
    "tfidf_title = vectorizer_title.transform(All_df['search_term_stemmed']) \n",
    "tfidf_description = vectorizer_description.transform(All_df['search_term_stemmed']) \n",
    "tfidf_bullets = vectorizer_bullets.transform(All_df['search_term_stemmed']) \n",
    "\n",
    "tfidf_title_querythekey = vectorizer_title.transform(All_df['search_term_thekey_stemmed'])\n",
    "tfidf_title_querybeforethekey = vectorizer_title.transform(All_df['search_term_beforethekey_stemmed'])\n",
    "\n",
    "\n",
    "tfidf_matchtitle = vectorizer_title.transform(All_df['word_in_title_string'])\n",
    "tfidf_matchtitle_stringonly = vectorizer_title.transform(All_df['word_in_title_string_only_string'])\n",
    "tfidf_matchdescription = vectorizer_description.transform(All_df['word_in_description_string']) \n",
    "tfidf_matchdescription_stringonly = vectorizer_description.transform(All_df['word_in_description_string_only_string']) \n",
    "tfidf_matchbullets = vectorizer_bullets.transform(All_df['word_in_bullets_string']) \n",
    "tfidf_matchbullets_stringonly = vectorizer_bullets.transform(All_df['word_in_bullets_string_only_string']) \n",
    "\n",
    "len(vectorizer_title.get_feature_names())\n",
    "len(vectorizer_description.get_feature_names())\n",
    "len(vectorizer_bullets.get_feature_names())\n",
    "\n",
    "uno_title=np.ones((len(vectorizer_title.get_feature_names()),1))\n",
    "uno_description=np.ones((len(vectorizer_description.get_feature_names()),1))\n",
    "uno_bullets=np.ones((len(vectorizer_bullets.get_feature_names()),1))\n",
    "\n",
    "let_title=np.asarray([[len(word)] for word in vectorizer_title.get_feature_names()])\n",
    "let_description=np.asarray([[len(word)] for word in vectorizer_description.get_feature_names()])\n",
    "let_bullets=np.asarray([[len(word)] for word in vectorizer_bullets.get_feature_names()])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "All_df['tfidf_title_num']=tfidf_title.tocsr().dot(uno_title)\n",
    "All_df['tfidf_description_num']=tfidf_description.tocsr().dot(uno_description)\n",
    "All_df['tfidf_bullets_num']=tfidf_bullets.tocsr().dot(uno_bullets)\n",
    "\n",
    "All_df['tfidf_title_let']=tfidf_title.tocsr().dot(let_title)\n",
    "All_df['tfidf_description_let']=tfidf_description.tocsr().dot(let_description)\n",
    "All_df['tfidf_bullets_let']=tfidf_bullets.tocsr().dot(let_bullets)\n",
    "\n",
    "All_df['tfidf_matchtitle_num']=tfidf_matchtitle.tocsr().dot(uno_title)\n",
    "All_df['tfidf_matchdescription_num']=tfidf_matchdescription.tocsr().dot(uno_description)\n",
    "All_df['tfidf_matchbullets_num']=tfidf_matchbullets.tocsr().dot(uno_bullets)\n",
    "\n",
    "All_df['tfidf_matchtitle_stringonly_num']=tfidf_matchtitle_stringonly.tocsr().dot(uno_title)\n",
    "All_df['tfidf_matchdescription_stringonly_num']=tfidf_matchdescription_stringonly.tocsr().dot(uno_description)\n",
    "All_df['tfidf_matchbullets_stringonly_num']=tfidf_matchbullets_stringonly.tocsr().dot(uno_bullets)\n",
    "\n",
    "\n",
    "All_df['tfidf_title_querythekey_num']=tfidf_title_querythekey.tocsr().dot(uno_title)\n",
    "All_df['tfidf_title_querybeforethekey_num']=tfidf_title_querybeforethekey.tocsr().dot(uno_title)\n",
    "All_df['tfidf_title_querythekey_let']=tfidf_title_querythekey.tocsr().dot(let_title)\n",
    "All_df['tfidf_title_querybeforethekey_let']=tfidf_title_querybeforethekey.tocsr().dot(let_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf basic features time: 894.3 minutes\n",
      "\n",
      "tfidf advanced features time: 1.9 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('tfidf basic features time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()\n",
    "\n",
    "All_df['tfidf_nn_important_in_title_num']=vectorizer_title.transform(All_df['search_term_tokens'].map(lambda x:nn_important_words(x)) ).tocsr().dot(uno_title)\n",
    "All_df['tfidf_nn_important_in_description_num']=vectorizer_description.transform(All_df['search_term_tokens'].map(lambda x:nn_important_words(x)) ).tocsr().dot(uno_description)\n",
    "All_df['tfidf_nn_important_in_bullets_num']=vectorizer_bullets.transform(All_df['search_term_tokens'].map(lambda x:nn_important_words(x)) ).tocsr().dot(uno_bullets)\n",
    "All_df['tfidf_nn_important_in_title_let']=vectorizer_title.transform(All_df['search_term_tokens'].map(lambda x:nn_important_words(x)) ).tocsr().dot(let_title)\n",
    "All_df['tfidf_nn_important_in_description_let']=vectorizer_description.transform(All_df['search_term_tokens'].map(lambda x:nn_important_words(x)) ).tocsr().dot(let_description)\n",
    "All_df['tfidf_nn_important_in_bullets_let']=vectorizer_bullets.transform(All_df['search_term_tokens'].map(lambda x:nn_important_words(x)) ).tocsr().dot(let_bullets)\n",
    "\n",
    "All_df['tfidf_nn_unimportant_in_title_num']=vectorizer_title.transform(All_df['search_term_tokens'].map(lambda x:nn_unimportant_words(x)) ).tocsr().dot(uno_title)\n",
    "All_df['tfidf_nn_unimportant_in_description_num']=vectorizer_description.transform(All_df['search_term_tokens'].map(lambda x:nn_unimportant_words(x)) ).tocsr().dot(uno_description)\n",
    "All_df['tfidf_nn_unimportant_in_bullets_num']=vectorizer_bullets.transform(All_df['search_term_tokens'].map(lambda x:nn_unimportant_words(x)) ).tocsr().dot(uno_bullets)\n",
    "All_df['tfidf_nn_unimportant_in_title_let']=vectorizer_title.transform(All_df['search_term_tokens'].map(lambda x:nn_unimportant_words(x)) ).tocsr().dot(let_title)\n",
    "All_df['tfidf_nn_unimportant_in_description_let']=vectorizer_description.transform(All_df['search_term_tokens'].map(lambda x:nn_unimportant_words(x)) ).tocsr().dot(let_description)\n",
    "All_df['tfidf_nn_unimportant_in_bullets_let']=vectorizer_bullets.transform(All_df['search_term_tokens'].map(lambda x:nn_unimportant_words(x)) ).tocsr().dot(let_bullets)\n",
    "\n",
    "All_df['tfidf_vbg_in_title_num']=vectorizer_title.transform(All_df['search_term_tokens'].map(lambda x:vbg_words(x)) ).tocsr().dot(uno_title)\n",
    "All_df['tfidf_vbg_in_description_num']=vectorizer_description.transform(All_df['search_term_tokens'].map(lambda x:vbg_words(x)) ).tocsr().dot(uno_description)\n",
    "All_df['tfidf_vbg_in_bullets_num']=vectorizer_bullets.transform(All_df['search_term_tokens'].map(lambda x:vbg_words(x)) ).tocsr().dot(uno_bullets)\n",
    "All_df['tfidf_vbg_in_title_let']=vectorizer_title.transform(All_df['search_term_tokens'].map(lambda x:vbg_words(x)) ).tocsr().dot(let_title)\n",
    "All_df['tfidf_vbg_in_description_let']=vectorizer_description.transform(All_df['search_term_tokens'].map(lambda x:vbg_words(x)) ).tocsr().dot(let_description)\n",
    "All_df['tfidf_vbg_in_bullets_let']=vectorizer_bullets.transform(All_df['search_term_tokens'].map(lambda x:vbg_words(x)) ).tocsr().dot(let_bullets)\n",
    "\n",
    "\n",
    "All_df['tfidf_jj_rb_in_title_num']=vectorizer_title.transform(All_df['search_term_tokens'].map(lambda x:jj_rb_words(x)) ).tocsr().dot(uno_title)\n",
    "All_df['tfidf_jj_rb_in_description_num']=vectorizer_description.transform(All_df['search_term_tokens'].map(lambda x:jj_rb_words(x)) ).tocsr().dot(uno_description)\n",
    "All_df['tfidf_jj_rb_in_bullets_num']=vectorizer_bullets.transform(All_df['search_term_tokens'].map(lambda x:jj_rb_words(x)) ).tocsr().dot(uno_bullets)\n",
    "All_df['tfidf_jj_rb_in_title_let']=vectorizer_title.transform(All_df['search_term_tokens'].map(lambda x:jj_rb_words(x)) ).tocsr().dot(let_title)\n",
    "All_df['tfidf_jj_rb_in_description_let']=vectorizer_description.transform(All_df['search_term_tokens'].map(lambda x:jj_rb_words(x)) ).tocsr().dot(let_description)\n",
    "All_df['tfidf_jj_rb_in_bullets_let']=vectorizer_bullets.transform(All_df['search_term_tokens'].map(lambda x:jj_rb_words(x)) ).tocsr().dot(let_bullets)\n",
    "\n",
    "\n",
    "\n",
    "All_df=All_df.drop(['word_in_title_string','word_in_title_string_only_string','word_in_description_string',\\\n",
    "'word_in_description_string_only_string','word_in_bullets_string','word_in_bullets_string_only_string'],axis=1)\n",
    "\n",
    "print('tfidf advanced features time:',round((time()-t0)/60,1) ,'minutes\\n')\n",
    "t0 = time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "All_df.to_csv('cleaning_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'product_title',\n",
       " 'product_uid',\n",
       " 'relevance',\n",
       " 'search_term',\n",
       " 'product_description',\n",
       " 'Brand',\n",
       " 'Certification',\n",
       " 'Color',\n",
       " 'Bullet',\n",
       " 'Product Height (in.)',\n",
       " 'Product Width (in.)',\n",
       " 'Product Depth (in.)',\n",
       " 'Product Weight (lb.)',\n",
       " 'Product Length (in.)',\n",
       " 'Indoor/Outdoor',\n",
       " 'Commercial / Residential',\n",
       " 'ENERGY STAR Certified',\n",
       " 'Package Quantity',\n",
       " 'Hardware Included',\n",
       " 'product_title_simpleparsed',\n",
       " 'search_term_simpleparsed',\n",
       " 'search_term_parsed',\n",
       " 'search_term_parsed_wospellcheck',\n",
       " 'product_title_parsed',\n",
       " 'brand_parsed',\n",
       " 'Material',\n",
       " 'material_parsed',\n",
       " 'search_term_parsed_woBrand',\n",
       " 'brands_in_search_term',\n",
       " 'search_term_parsed_woBM',\n",
       " 'materials_in_search_term',\n",
       " 'product_title_parsed_woBrand',\n",
       " 'brands_in_product_title',\n",
       " 'product_title_parsed_woBM',\n",
       " 'materials_in_product_title',\n",
       " 'search_term_tokens',\n",
       " 'product_title_tokens',\n",
       " 'attribute_bullets_parsed',\n",
       " 'search_term_for',\n",
       " 'search_term_for_stemmed',\n",
       " 'search_term_with',\n",
       " 'search_term_with_stemmed',\n",
       " 'product_title_parsed_without',\n",
       " 'product_title_without_stemmed',\n",
       " 'attribute_bullets_parsed_woBrand',\n",
       " 'brands_in_attribute_bullets',\n",
       " 'attribute_bullets_parsed_woBM',\n",
       " 'materials_in_attribute_bullets',\n",
       " 'attribute_bullets_tokens',\n",
       " 'product_title_cut',\n",
       " 'product_title_cut_woBrand',\n",
       " 'product_title_cut_tokens',\n",
       " 'product_title_cut_wordtokens',\n",
       " 'search_term_cut_woBrand',\n",
       " 'search_term_cut_tokens',\n",
       " 'search_term_cut_wordtokens',\n",
       " 'search_term_keys',\n",
       " 'product_title_keys',\n",
       " 'product_title_thekey',\n",
       " 'product_title_beforethekey',\n",
       " 'product_title_before2thekey',\n",
       " 'search_term_thekey',\n",
       " 'search_term_beforethekey',\n",
       " 'search_term_before2thekey',\n",
       " 'attribute_bullets_stemmed',\n",
       " 'attribute_bullets_stemmed_woBM',\n",
       " 'attribute_bullets_stemmed_woBrand',\n",
       " 'search_term_keys_stemmed',\n",
       " 'product_title_keys_stemmed',\n",
       " 'search_term_stemmed',\n",
       " 'search_term_stemmed_woBM',\n",
       " 'search_term_stemmed_woBrand',\n",
       " 'product_title_stemmed',\n",
       " 'product_title_stemmed_woBM',\n",
       " 'product_title_stemmed_woBrand',\n",
       " 'search_term_thekey_stemmed',\n",
       " 'product_title_thekey_stemmed',\n",
       " 'search_term_beforethekey_stemmed',\n",
       " 'product_title_beforethekey_stemmed',\n",
       " 'search_term_before2thekey_stemmed',\n",
       " 'product_title_before2thekey_stemmed',\n",
       " 'has_attributes_dummy',\n",
       " 'no_bullets_dummy',\n",
       " 'is_replaced_using_google_dict',\n",
       " 'len_of_meaningful_words_in_query',\n",
       " 'ratio_of_meaningful_words_in_query',\n",
       " 'avg_wordlength_in_query',\n",
       " 'ratio_vowels_in_query',\n",
       " 'initial_len_of_query',\n",
       " 'len_of_query_woBM',\n",
       " 'len_of_query_for',\n",
       " 'len_of_query_with',\n",
       " 'len_of_prtitle_without',\n",
       " 'len_of_query_string_only_woBM',\n",
       " 'len_of_query_w_dash_woBM',\n",
       " 'len_of_title_w_dash_woBM',\n",
       " 'len_of_product_title_woBM',\n",
       " 'len_of_attribute_bullets_woBM',\n",
       " 'len_of_brands_in_query',\n",
       " 'size_of_brands_in_query',\n",
       " 'len_of_materials_in_query',\n",
       " 'size_of_materials_in_query',\n",
       " 'len_of_brands_in_product_title',\n",
       " 'size_of_brands_in_product_title',\n",
       " 'len_of_materials_in_product_title',\n",
       " 'size_of_materials_in_product_title',\n",
       " 'len_of_brands_in_attribute_bullets',\n",
       " 'size_of_brands_in_attribute_bullets',\n",
       " 'len_of_materials_in_attribute_bullets',\n",
       " 'size_of_materials_in_attribute_bullets',\n",
       " 'len_of_query',\n",
       " 'len_of_product_title',\n",
       " 'len_of_attribute_bullets',\n",
       " 'len_of_query_keys',\n",
       " 'len_of_product_title_keys',\n",
       " 'len_of_query_thekey',\n",
       " 'len_of_product_title_thekey',\n",
       " 'len_of_query_beforethekey',\n",
       " 'len_of_product_title_beforethekey',\n",
       " 'len_of_query_before2thekey',\n",
       " 'len_of_product_title_before2thekey',\n",
       " 'product_description_parsed',\n",
       " 'product_description_parsed_woBrand',\n",
       " 'brands_in_product_description',\n",
       " 'product_description_parsed_woBM',\n",
       " 'materials_in_product_description',\n",
       " 'product_description_tokens',\n",
       " 'product_description_stemmed',\n",
       " 'product_description_stefxmmed_woBM',\n",
       " 'product_description_stemmed_woBrand',\n",
       " 'len_of_product_description_woBM',\n",
       " 'len_of_brands_in_product_description',\n",
       " 'size_of_brands_in_product_description',\n",
       " 'len_of_materials_in_product_description',\n",
       " 'size_of_materials_in_product_description',\n",
       " 'len_of_product_description',\n",
       " 'ratio_of_nn_important_in_search_term',\n",
       " 'ratio_of_nn_unimportant_in_search_term',\n",
       " 'ratio_of_vb_in_search_term',\n",
       " 'ratio_of_vbg_in_search_term',\n",
       " 'ratio_of_jj_rb_in_search_term',\n",
       " 'ratio_of_nn_important_in_product_title',\n",
       " 'ratio_of_nn_unimportant_in_product_title',\n",
       " 'ratio_of_vb_in_product_title',\n",
       " 'ratio_of_vbg_in_product_title',\n",
       " 'ratio_of_jj_rb_in_product_title',\n",
       " 'ratio_of_nn_important_in_product_description',\n",
       " 'ratio_of_nn_unimportant_in_product_description',\n",
       " 'ratio_of_vb_in_product_description',\n",
       " 'ratio_of_vbg_in_product_description',\n",
       " 'ratio_of_jj_rb_in_product_description',\n",
       " 'ratio_of_nn_important_in_attribute_bullets',\n",
       " 'ratio_of_nn_unimportant_in_attribute_bullets',\n",
       " 'ratio_of_vb_in_attribute_bullets',\n",
       " 'ratio_of_vbg_in_attribute_bullets',\n",
       " 'ratio_of_jj_rb_in_attribute_bullets',\n",
       " 'perc_digits_in_query',\n",
       " 'perc_digits_in_title',\n",
       " 'perc_digits_in_description',\n",
       " 'perc_digits_in_bullets',\n",
       " 'query_brand_in_brand_fullmatch',\n",
       " 'query_brand_in_brand_partialmatch',\n",
       " 'query_brand_in_brand_assumedmatch',\n",
       " 'query_brand_in_brand_nomatch',\n",
       " 'query_brand_in_brand_convoluted',\n",
       " 'query_brand_in_all_fullmatch',\n",
       " 'query_brand_in_all_partialmatch',\n",
       " 'query_brand_in_all_assumedmatch',\n",
       " 'query_brand_in_all_nomatch',\n",
       " 'query_brand_in_all_convoluted',\n",
       " 'query_brand_in_title_fullmatch',\n",
       " 'query_brand_in_title_partialmatch',\n",
       " 'query_brand_in_title_assumedmatch',\n",
       " 'query_brand_in_title_nomatch',\n",
       " 'query_brand_in_title_convoluted',\n",
       " 'query_brand_in_description_convoluted',\n",
       " 'query_brand_in_bullets_convoluted',\n",
       " 'query_material_in_material_fullmatch',\n",
       " 'query_material_in_material_partialmatch',\n",
       " 'query_material_in_material_assumedmatch',\n",
       " 'query_material_in_material_nomatch',\n",
       " 'query_material_in_material_convoluted',\n",
       " 'query_material_in_all_fullmatch',\n",
       " 'query_material_in_all_partialmatch',\n",
       " 'query_material_in_all_assumedmatch',\n",
       " 'query_material_in_all_nomatch',\n",
       " 'query_material_in_all_convoluted',\n",
       " 'query_material_in_title_convoluted',\n",
       " 'query_material_in_description_convoluted',\n",
       " 'query_material_in_bullets_convoluted',\n",
       " 'wordFor_in_title_string_only_num',\n",
       " 'wordFor_in_title_string_only_let',\n",
       " 'wordFor_in_title_string_only_letratio',\n",
       " 'wordWith_in_title_string_only_num',\n",
       " 'wordWith_in_title_string_only_let',\n",
       " 'wordWith_in_title_string_only_letratio',\n",
       " 'prtitleWithout_in_query_string_only_num',\n",
       " 'prtitleWithout_in_query_string_only_let',\n",
       " 'prtitleWithout_in_query_string_only_letratio',\n",
       " 'query_in_title',\n",
       " 'word_in_title_num',\n",
       " 'word_in_title_sum',\n",
       " 'word_in_title_let',\n",
       " 'word_in_title_numratio',\n",
       " 'word_in_title_letratio',\n",
       " 'word_in_title_string_only_num',\n",
       " 'word_in_title_string_only_sum',\n",
       " 'word_in_title_string_only_let',\n",
       " 'word_in_title_string_only_numratio',\n",
       " 'word_in_title_string_only_letratio',\n",
       " 'word_in_title_w_dash',\n",
       " 'two_words_in_title_num',\n",
       " 'two_words_in_title_sum',\n",
       " 'two_words_in_title_let',\n",
       " 'two_words_in_title_string_only_num',\n",
       " 'two_words_in_title_string_only_sum',\n",
       " 'two_words_in_title_string_only_let',\n",
       " 'len_of_digits_in_query',\n",
       " 'len_of_digits_in_title',\n",
       " 'common_digits_in_title_num',\n",
       " 'common_digits_in_title_ratio',\n",
       " 'common_digits_in_title_jaccard',\n",
       " 'nn_important_in_title_num',\n",
       " 'nn_important_in_title_sum',\n",
       " 'nn_important_in_title_let',\n",
       " 'nn_important_in_title_numratio',\n",
       " 'nn_important_in_title_letratio',\n",
       " 'nn_unimportant_in_title_num',\n",
       " 'nn_unimportant_in_title_let',\n",
       " 'nn_unimportant_in_title_letratio',\n",
       " 'nn_important_in_nn_important_in_title_num',\n",
       " 'nn_important_in_nn_important_in_title_sum',\n",
       " 'nn_important_in_nn_important_in_title_let',\n",
       " 'nn_important_in_nn_important_in_title_letratio',\n",
       " 'nn_important_in_nn_unimportant_in_title_num',\n",
       " 'nn_important_in_nn_unimportant_in_title_sum',\n",
       " 'nn_important_in_nn_unimportant_in_title_let',\n",
       " 'nn_important_in_nn_unimportant_in_title_letratio',\n",
       " 'nn_unimportant_in_nn_important_in_title_num',\n",
       " 'nn_unimportant_in_nn_important_in_title_sum',\n",
       " 'nn_unimportant_in_nn_important_in_title_let',\n",
       " 'nn_unimportant_in_nn_important_in_title_letratio',\n",
       " 'jj_rb_in_jj_rb_in_title_num',\n",
       " 'jj_rb_in_jj_rb_in_title_sum',\n",
       " 'jj_rb_in_jj_rb_in_title_let',\n",
       " 'vbg_in_vbg_in_title_num',\n",
       " 'vbg_in_vbg_in_title_sum',\n",
       " 'vbg_in_vbg_in_title_let',\n",
       " 'wordFor_in_description_string_only_num',\n",
       " 'wordFor_in_description_string_only_let',\n",
       " 'wordFor_in_description_string_only_letratio',\n",
       " 'wordWith_in_description_string_only_num',\n",
       " 'wordWith_in_description_string_only_let',\n",
       " 'wordWith_in_description_string_only_letratio',\n",
       " 'query_in_description',\n",
       " 'word_in_description_num',\n",
       " 'word_in_description_sum',\n",
       " 'word_in_description_let',\n",
       " 'word_in_description_numratio',\n",
       " 'word_in_description_letratio',\n",
       " 'word_in_description_string_only_num',\n",
       " 'word_in_description_string_only_sum',\n",
       " 'word_in_description_string_only_let',\n",
       " 'word_in_description_string_only_numratio',\n",
       " 'word_in_description_string_only_letratio',\n",
       " 'word_in_description_w_dash',\n",
       " 'two_words_in_description_num',\n",
       " 'two_words_in_description_sum',\n",
       " 'two_words_in_description_let',\n",
       " 'two_words_in_description_string_only_num',\n",
       " 'two_words_in_description_string_only_sum',\n",
       " 'two_words_in_description_string_only_let',\n",
       " 'len_of_digits_in_description',\n",
       " 'common_digits_in_description_num',\n",
       " 'common_digits_in_description_ratio',\n",
       " 'common_digits_in_description_jaccard',\n",
       " 'nn_important_in_description_num',\n",
       " 'nn_important_in_description_sum',\n",
       " 'nn_important_in_description_let',\n",
       " 'nn_important_in_description_numratio',\n",
       " 'nn_important_in_description_letratio',\n",
       " 'nn_unimportant_in_description_num',\n",
       " 'nn_unimportant_in_description_let',\n",
       " 'nn_unimportant_in_description_letratio',\n",
       " 'nn_important_in_nn_important_in_description_num',\n",
       " 'nn_important_in_nn_important_in_description_sum',\n",
       " 'nn_important_in_nn_important_in_description_let',\n",
       " 'nn_important_in_nn_important_in_description_letratio',\n",
       " 'nn_important_in_nn_unimportant_in_description_num',\n",
       " 'nn_important_in_nn_unimportant_in_description_sum',\n",
       " 'nn_important_in_nn_unimportant_in_description_let',\n",
       " 'nn_important_in_nn_unimportant_in_description_letratio',\n",
       " 'nn_unimportant_in_nn_important_in_description_num',\n",
       " 'nn_unimportant_in_nn_important_in_description_sum',\n",
       " 'nn_unimportant_in_nn_important_in_description_let',\n",
       " 'nn_unimportant_in_nn_important_in_description_letratio',\n",
       " 'jj_rb_in_jj_rb_in_description_num',\n",
       " 'jj_rb_in_jj_rb_in_description_sum',\n",
       " 'jj_rb_in_jj_rb_in_description_let',\n",
       " 'vbg_in_vbg_in_description_num',\n",
       " 'vbg_in_vbg_in_description_sum',\n",
       " 'vbg_in_vbg_in_description_let',\n",
       " 'wordFor_in_bullets_string_only_num',\n",
       " 'wordFor_in_bullets_string_only_let',\n",
       " 'wordFor_in_bullets_string_only_letratio',\n",
       " 'wordWith_in_bullets_string_only_num',\n",
       " 'wordWith_in_bullets_string_only_let',\n",
       " 'wordWith_in_bullets_string_only_letratio',\n",
       " 'query_in_bullets',\n",
       " 'word_in_bullets_num',\n",
       " 'word_in_bullets_sum',\n",
       " 'word_in_bullets_let',\n",
       " 'word_in_bullets_numratio',\n",
       " 'word_in_bullets_letratio',\n",
       " 'word_in_bullets_string_only_num',\n",
       " 'word_in_bullets_string_only_sum',\n",
       " 'word_in_bullets_string_only_let',\n",
       " 'word_in_bullets_string_only_numratio',\n",
       " 'word_in_bullets_string_only_letratio',\n",
       " 'word_in_bullets_w_dash',\n",
       " 'two_words_in_bullets_num',\n",
       " 'two_words_in_bullets_sum',\n",
       " 'two_words_in_bullets_let',\n",
       " 'two_words_in_bullets_string_only_num',\n",
       " 'two_words_in_bullets_string_only_sum',\n",
       " 'two_words_in_bullets_string_only_let',\n",
       " 'len_of_digits_in_bullets',\n",
       " 'common_digits_in_bullets_num',\n",
       " 'common_digits_in_bullets_ratio',\n",
       " 'common_digits_in_bullets_jaccard',\n",
       " 'nn_important_in_bullets_num',\n",
       " 'nn_important_in_bullets_sum',\n",
       " 'nn_important_in_bullets_let',\n",
       " 'nn_important_in_bullets_numratio',\n",
       " 'nn_important_in_bullets_letratio',\n",
       " 'nn_unimportant_in_bullets_num',\n",
       " 'nn_unimportant_in_bullets_let',\n",
       " 'nn_unimportant_in_bullets_letratio',\n",
       " 'nn_important_in_nn_important_in_bullets_num',\n",
       " 'nn_important_in_nn_important_in_bullets_sum',\n",
       " 'nn_important_in_nn_important_in_bullets_let',\n",
       " 'nn_important_in_nn_important_in_bullets_letratio',\n",
       " 'nn_important_in_nn_unimportant_in_bullets_num',\n",
       " 'nn_important_in_nn_unimportant_in_bullets_sum',\n",
       " 'nn_important_in_nn_unimportant_in_bullets_let',\n",
       " 'nn_important_in_nn_unimportant_in_bullets_letratio',\n",
       " 'nn_unimportant_in_nn_important_in_bullets_num',\n",
       " 'nn_unimportant_in_nn_important_in_bullets_sum',\n",
       " 'nn_unimportant_in_nn_important_in_bullets_let',\n",
       " 'nn_unimportant_in_nn_important_in_bullets_letratio',\n",
       " 'jj_rb_in_jj_rb_in_bullets_num',\n",
       " 'jj_rb_in_jj_rb_in_bullets_sum',\n",
       " 'jj_rb_in_jj_rb_in_bullets_let',\n",
       " 'vbg_in_vbg_in_bullets_num',\n",
       " 'vbg_in_vbg_in_bullets_sum',\n",
       " 'vbg_in_vbg_in_bullets_let',\n",
       " 'keyword_in_titlekeys_num',\n",
       " 'keyword_in_titlekeys_sum',\n",
       " 'keyword_in_titlekeys_let',\n",
       " 'keyword_in_titlekeys_numratio',\n",
       " 'keyword_in_titlekeys_letratio',\n",
       " 'keyword_in_description_num',\n",
       " 'keyword_in_description_sum',\n",
       " 'keyword_in_description_let',\n",
       " 'keyword_in_description_numratio',\n",
       " 'keyword_in_description_letratio',\n",
       " 'keyword_in_bullets_num',\n",
       " 'keyword_in_bullets_sum',\n",
       " 'keyword_in_bullets_let',\n",
       " 'keyword_in_bullets_numratio',\n",
       " 'keyword_in_bullets_letratio',\n",
       " 'keyword_in_titlekeys_jacnum',\n",
       " 'keyword_in_titlekeys_jaclet',\n",
       " 'thekeys_in_title',\n",
       " 'thekeys_in_description',\n",
       " 'thekeys_in_bullets',\n",
       " 'thekey_in_description_sum',\n",
       " 'thekey_in_description_let',\n",
       " 'thekey_in_bullets_sum',\n",
       " 'thekey_in_bullets_let',\n",
       " 'beforethekey_in_description_sum',\n",
       " 'beforethekey_in_description_let',\n",
       " 'beforethekey_in_bullets_sum',\n",
       " 'beforethekey_in_bullets_let',\n",
       " 'thekey_in_nn_important_in_description_sum',\n",
       " 'thekey_in_nn_important_in_description_let',\n",
       " 'thekey_in_nn_important_in_bullets_sum',\n",
       " 'thekey_in_nn_important_in_bullets_let',\n",
       " 'thekey_in_thekey',\n",
       " 'beforethekey_in_beforethekey',\n",
       " 'beforethekeys_in_beforethekeys',\n",
       " 'thekey_in_beforethekeys',\n",
       " 'beforethekeys_in_thekey',\n",
       " 'key_for_dict',\n",
       " 'thekeys_pathsimilarity_max',\n",
       " 'thekeys_pathsimilarity_mean',\n",
       " 'thekeys_lchsimilarity_max',\n",
       " 'thekeys_lchsimilarity_mean',\n",
       " 'thekeys_ressimilarity_max',\n",
       " 'thekeys_ressimilarity_mean',\n",
       " 'beforethekeys_pathsimilarity_max',\n",
       " 'beforethekeys_pathsimilarity_mean',\n",
       " 'beforethekeys_lchsimilarity_max',\n",
       " 'beforethekeys_lchsimilarity_mean',\n",
       " 'beforethekeys_ressimilarity_max',\n",
       " 'beforethekeys_ressimilarity_mean',\n",
       " 'thekey_beforethekey_pathsimilarity_max',\n",
       " 'thekey_beforethekey_pathsimilarity_mean',\n",
       " 'thekey_beforethekey_lchsimilarity_max',\n",
       " 'thekey_beforethekey_lchsimilarity_mean',\n",
       " 'thekey_beforethekey_ressimilarity_max',\n",
       " 'thekey_beforethekey_ressimilarity_mean',\n",
       " 'thekey_before2thekey_pathsimilarity_max',\n",
       " 'thekey_before2thekey_pathsimilarity_mean',\n",
       " 'thekey_before2thekey_lchsimilarity_max',\n",
       " 'thekey_before2thekey_lchsimilarity_mean',\n",
       " 'thekey_before2thekey_ressimilarity_max',\n",
       " 'thekey_before2thekey_ressimilarity_mean',\n",
       " 'thekey_beforethekeys_pathsimilarity_max',\n",
       " 'thekey_beforethekeys_pathsimilarity_mean',\n",
       " 'thekey_beforethekeys_lchsimilarity_max',\n",
       " 'thekey_beforethekeys_lchsimilarity_mean',\n",
       " 'thekey_beforethekeys_ressimilarity_max',\n",
       " 'thekey_beforethekeys_ressimilarity_mean',\n",
       " 'beforethekey_thekey_pathsimilarity_max',\n",
       " 'beforethekey_thekey_pathsimilarity_mean',\n",
       " 'beforethekey_thekey_lchsimilarity_max',\n",
       " 'beforethekey_thekey_lchsimilarity_mean',\n",
       " 'beforethekey_thekey_ressimilarity_max',\n",
       " 'beforethekey_thekey_ressimilarity_mean',\n",
       " 'query_pathsimilarity_max',\n",
       " 'query_pathsimilarity_mean',\n",
       " 'query_lchhsimilarity_max',\n",
       " 'query_lchsimilarity_mean',\n",
       " 'query_ressimilarity_max',\n",
       " 'query_ressimilarity_mean',\n",
       " 'title_pathsimilarity_max',\n",
       " 'title_pathsimilarity_mean',\n",
       " 'title_lchsimilarity_max',\n",
       " 'title_lchsimilarity_mean',\n",
       " 'title_ressimilarity_max',\n",
       " 'title_ressimilarity_mean',\n",
       " 'seqmatch_title_ratio',\n",
       " 'seqmatch_title_ratioscaled',\n",
       " 'seqmatch_description_ratio',\n",
       " 'seqmatch_description_ratioscaled',\n",
       " 'seqmatch_bullets_ratio',\n",
       " 'seqmatch_bullets_ratioscaled',\n",
       " 'seqmatch_desc&bullets_ratio',\n",
       " 'seqmatch_desc&bullets_ratioscaled',\n",
       " 'tfidf_title_num',\n",
       " 'tfidf_description_num',\n",
       " 'tfidf_bullets_num',\n",
       " 'tfidf_title_let',\n",
       " 'tfidf_description_let',\n",
       " 'tfidf_bullets_let',\n",
       " 'tfidf_matchtitle_num',\n",
       " 'tfidf_matchdescription_num',\n",
       " 'tfidf_matchbullets_num',\n",
       " 'tfidf_matchtitle_stringonly_num',\n",
       " 'tfidf_matchdescription_stringonly_num',\n",
       " 'tfidf_matchbullets_stringonly_num',\n",
       " 'tfidf_title_querythekey_num',\n",
       " 'tfidf_title_querybeforethekey_num',\n",
       " 'tfidf_title_querythekey_let',\n",
       " 'tfidf_title_querybeforethekey_let',\n",
       " 'tfidf_nn_important_in_title_num',\n",
       " 'tfidf_nn_important_in_description_num',\n",
       " 'tfidf_nn_important_in_bullets_num',\n",
       " 'tfidf_nn_important_in_title_let',\n",
       " 'tfidf_nn_important_in_description_let',\n",
       " 'tfidf_nn_important_in_bullets_let',\n",
       " 'tfidf_nn_unimportant_in_title_num',\n",
       " 'tfidf_nn_unimportant_in_description_num',\n",
       " 'tfidf_nn_unimportant_in_bullets_num',\n",
       " 'tfidf_nn_unimportant_in_title_let',\n",
       " 'tfidf_nn_unimportant_in_description_let',\n",
       " 'tfidf_nn_unimportant_in_bullets_let',\n",
       " 'tfidf_vbg_in_title_num',\n",
       " 'tfidf_vbg_in_description_num',\n",
       " 'tfidf_vbg_in_bullets_num',\n",
       " 'tfidf_vbg_in_title_let',\n",
       " 'tfidf_vbg_in_description_let',\n",
       " 'tfidf_vbg_in_bullets_let',\n",
       " 'tfidf_jj_rb_in_title_num',\n",
       " 'tfidf_jj_rb_in_description_num',\n",
       " 'tfidf_jj_rb_in_bullets_num',\n",
       " 'tfidf_jj_rb_in_title_let',\n",
       " 'tfidf_jj_rb_in_description_let',\n",
       " 'tfidf_jj_rb_in_bullets_let']"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(All_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
